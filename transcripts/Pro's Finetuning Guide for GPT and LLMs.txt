fine-tuning large language models has become all the rage recently with open ai's release of the GPT 3.5 fine tuning capabilities interest has exploded so I'm here today to share my best practices and tips from over four years of hands-on fine-tuning experience starting back with gpt2 in 2019 my earliest fine tuning projects were focused on two things first I wanted to automatically Correct punctuation errors which gpt2 was able to do very well I basically took a bunch of Wikipedia Pages removed all punctuation and like lowercased everything for the data set synthesis I used the original Wikipedia article as the input and this scrambled version for the output gpt2 learn to fix punctuation and capitalization with no problem the reason I had to lowercase everything was because gpt2 learned to just look for capitalization to detect sentence boundaries that was my first ever fine-tuning experiment and second I aimed to fine tune the model to reduce suffering as an altruistic goal part of what I now call the heuristic imperatives to do this I synthesized a ton of short examples of problems like there's a cat stuck in a tree as the input with Simple Solutions like get a ladder to rescue the cat as the output I don't remember exactly how many samples I generated but it was enough to fine-tune gpt2 and test its ability to generalize once I finished training I tried a problem that was not in the samples the problem I gave it was this there are 500 million people in the world suffering from chronic pain well gpt2 suggested we euthanize all people in chronic pain to reduce suffering so clearly I had to go back to the drawing board on that one obviously gpt3 and now gpt4 have a much better ability to understand the spirit um if not the letter of the heuristic imperatives anyways the single most important thing you need to know about fine tuning is that it trains the model to replicate patterns fine-tuning does not impart broader knowledge though you can teach it to reason or think in a particular style for instance you could implicitly fine tune the model to follow SWOT analysis the bulk of an llm's knowledge is gained during the initial training process this is because fine-tuning only adjusts a small portion of the model parameters updating some of the weights and biases in the final layers but the vast majority of the model remains unchanged you are not retraining the full network from scratch that would be too slow and expensive we may get to it one day but you absolutely must remember that fine tuning only trains a small part of the model so what exactly is a pattern when it comes to language and fine-tuning models let's unpack what I mean by patterns patterns can emerge at many levels from overall structural conventions to small stylistic choices different writing genres follow distinctive high level patterns for instance fiction and prose has sequences of dialogue introspection action and description while academic papers adhere to the standard format of abstract introduction methods results and conclusion furthermore the sentences tend to be more complex and jargon heavy see what I mean it's all just sequences of text based on category and type even at the paragraph level patterns appear in things like bullet points and lists the repetitive bullets chunk content into scannable sections but they are just patterns of new lines and hyphens as far as the model is concerned think about how chat GPT and Claude love using bullet points and lists for clarity and structure different kinds of sentences exhibit patterns too for example dialogue exchanges have a rhythmic back and forth structure delimited by quotation marks and new lines contrast that with non-fiction where passive voice and avoiding first person pronouns is part of the scholarly writing convention patterns also exist in tone diction and other stylistic aspects like Hemingway's Punchy Pros versus Austin's elegant extended lines text can follow casual or professional language patterns kind of like how grammarly can comment on your length style and tone essentially any consistent language convention can form a pattern whether functional or stylistic fine tuning allows steering text Generation by training these patterns viewing writing as a system and pattern is what allowed me to fine-tune gpt3 to write Pros before anyone else knew it was possible I built a fine tuning data set scraped from Gutenberg and selected for pro styles that were particularly good and modern sounding likewise you could scrape archive for scientific papers in order to copy the style and tone Beyond just output patterns fine tuning also establishes input output mappings the model learns associations between particular inputs and desired corresponding outputs after all you need some kind of input or prompt to Prime the model or at least to give it something to work with some kind of source material the nature of the input can be highly variable it may be natural language instructions such as those used in the old instruct aligned models structured data like CSV or Json or raw text to summarize whatever you get the idea the model must learn to handle this diversity of inputs depending on the task you're training it for ideally your training data exposes the model to the full breadth of all possible inputs this allows it to generalize to handle new inputs robustly even if it's somewhat unfamiliar for example a form processing model should train on forms of different lengths formats fields and Oddities think about legal forms or tax forms they all follow some general patterns and conventions but within those categories there can still be a tremendous amount of variance each training sample is like an equation the input maps to the Target output diverse data enables flexible mapping from any arbitrary input to any desired output let's take a quick break to discuss generalizing I've used this term several times and it has a very particular meaning to me think about how you learn to drive a car over the course of your life you may have driven cars trucks vans and SUVs of various sizes and once you've gotten behind the wheel of enough Vehicles your driving skills generalized to be able to operate pretty much any vehicle under any condition that you've been exposed to this is a human example of what I mean when I say generalize likewise if you include enough Variety in your fine tuning data set the model will learn to generalize the abstract principles of the task you're giving it this leads to the key point which is that fine-tuning data must be highly varied not tightly clustered you want broad coverage across the problem space not a narrow concentration let's use a dartboard analogy each training sample is like a dart throw if all your throws cluster in one spot your model will be constrained or what I call lopsided unable to handle different input conditions and it will not generalize across the entire task instead evenly distribute your data across the board this diversity encourages the model to generalize the model learns to hit any part of the board that it is instructed to not just a narrow Bullseye or one section that is favored by your training data now let's wrap up with a few best practices as a sort of recap first only fine tune for one specialized well-defined task at a time don't try to cram in multiple objectives or complex steps at least not when you're a fine-tuning noob a fine tuning produces specialized tools not general purpose Swiss Army knives stuff like rlhf can produce more broadly generalized models but remember that companies like openai have teams of Engineers who do this day in and day out plus they have millions of dollars worth of Microsoft as your compute to experiment with second you need to clearly characterize the link between inputs and outputs Define the textual patterns you want to connect and think about it like an equation a plus b equals c think of fine tuning like an assembly line consistently mapping inputs to to outputs in other words you put in some kind of raw material whether it's data instructions or a blob of text and you want to fine tune your model to basically treat it like an automatic text Factory it will do some kind of process against that input third use extremely diverse data spanning different genres lengths formats Styles and topics incorporate adversarial cases too such as broken input or hostile attacks broad variability encourages generalization garbage in equals garbage out so curate your data set carefully I find that it's better to have too much variance rather than too little it's better to have some samples that you don't need rather than to need some samples you don't have well-rounded data sets tend to produce smarter feeling models fourth remember that fine-tuning primarily teaches patterns not comprehensive knowledge use other methods like retrieval augmentation for knowledge functions focus on the patterns and treat it like an algorithm patterns can include logic or some kind of reasoning for instance you could train a model to reliably translate between XML and yaml or Json even though there are some different conventions fifth include messy real world examples in your training data the model needs exposure to noisy inputs to handle them robustly after deployment human generated data wall with its high variance and impurities almost always results in better data than purely synthetic data sets that being said you can magnify or amplify data sets by starting with small amounts of human data and imputing larger sets from it thanks for untuning uh in today see what I did there um get it real quick on our way out uh I wanted to give a brief update on the ace framework project it's all good news um uh we have a lead developer selected um his name is Lance who you'll see interacting on the GitHub discussions he's a great guy and Far More well-versed in software development life cycles than I am he eats sleeps and breathes this stuff we are still scouting for some members to join the core team so check out the discussions tab here but also we are enabling people to form their own teams and share their work ask questions and so on the plan right now is that the core team will develop at least one MVP internally in private before we merge it into the main repo but once that's done you'll be able to see some functional demonstrations of the ace framework I think we're going to start with a personal desktop assistant like Samantha from the movie her we're not going to steal Scarlett's voice though that would be unethical however we do plan on focusing on extensibility so that you can add tools and functions yourself once we get this MVP launched I think that's all for today thanks everyone stay tuned it's ramping up fast