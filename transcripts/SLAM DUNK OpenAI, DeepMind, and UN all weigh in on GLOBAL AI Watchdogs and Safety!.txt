good morning and happy Friday everybody so there's been a lot of news this week about AI safety at a global scale so what we're going to do is we're going to have a quick recap of the news I'm not going to take a super deep dive uh some of it is basically a rehash of things that you might already know but then I will do a little bit of unpacking my perspective on AI safety and uh there are some good and there is some bad so let's get into it so first uh earlier this week I think it was on Tuesday Mustafa Solan uh the Deep Mind co-founder he made another push uh very publicly for a global panel to regulate AI safety so he's basically saying uh we're calling he he's calling for an ipcc uh but for AI so rather than uh necessarily a regulator this is more of a monitor or a watchdog so you know if you're not familiar the ipcc releases their ominous annual reports on uh the climate change so basically it's an international Commission for monitoring climate change and he wants to do the same thing for AI which I think is good so basically now we've had three kind of different proposals one a monitor slw Watchdog number two a global regulator like the iaea which is a nuclear regulator and then three and this is what I'm actually disappointed about is I'm seeing less uh calls for just Global Research or International research so you like I would I would prefer to see a world where we have all three where we have an obser where we have a regulator and then we have a researcher and there are several examples of International research efforts namely CERN um because whenever there's a new technology or new domain of science that is valuable to everyone it behooves all of us to invest in the research so I'm not sure why no one has proposed uh at this level a CERN for AI and instead they're focusing more on like the stick rather than the carrot anyways so that's kind of the tldr he's calling for an ipcc but for for AI now one thing that I wanted to remind you of though is that uh back in June the un uh uh security General did a basically say like hey we are uh we're amable to the idea of of creating a global AI Watchdog so again we've covered this before but but this is exactly what I just said the iaea so un Secretary General Antonio gutterz um he said that the this seems like a good idea so I do agree with that would like to see more than just a watchdog uh because again the magnitude of AI is on the same scale of you nuclear energy and and other kinds of uh Frontier both scientific and Engineering breakthroughs now uh also earlier around that same time open aai coming to the table meetings um you know we saw Sam Altman in Congress and there is this is the reason that I brought this up is because they talk about voluntary commitments the thing is if volun if we're all depending on voluntary commitments like yes it's good it's it's a good you know signal saying hey we're going to play ball right but you know cynically you could read this as virtue signaling um but you know being a interpreting it as charitably as possible you might say okay this is actually a good sign and open AI is you know they're trying to be responsible and they're trying to you know be basically set an example um you know they're trying to say like we're going to Pilot and refine governance practices the biggest thing that scares me about open AI though and this is because uh Sam Alman and their their corporate policy their their internal Doctrine is humans must and will remain in control at all times forever and ever into the future and by ver by virtue of that they're not studying how to create benevolent autonomous machines they implicitly it seems like they believe it's not even possible um and so by virtue of not studying it there's a huge gap in the research that open aai is doing in my personal opinion um now that being said it's entirely possible that internally they fully intend on creating autonomous things and it's possible that Sam Alman has said these things publicly just because it would really scare the world if you know the C CEO of one of the most advanced AI companies in the world said oh yeah we're going to set it free so you know the pr reps would probably have a field day that say like Sam no tone it down um but having spoken to some of the people at open AI over the years um I know that they have a very cleared vision of the goal is Agi um now are they smart enough to understand what that implies that controlling something millions of times more intelligent than you is likely impossible I sure as heck hope so so uh now moving forward to uh other news that came out on Tuesday so this this news coincided with Mustafa Solomon's uh testimony the UN had a meeting their 78th session um and this was released uh just a few days ago and I I don't need to read the whole thing to you I skimmed it uh and and it's I can tell you kind of basically the the gist and it's nothing new but there's a few key highlights so first without adequate guard rails artificial intelligence threatens Global Security in evolution from algorithms to armaments speaker tells the first committee okay great so really all you need to know is the first two paragraphs the window of opportunity to enact guard rails Against The Perils of autonomous weapons and artificial intelligence's military applications is rapidly closing that is an unequivocal statement which is good to see from the UN as the world prepares for a technological breakout the first committee disarmament and International Security heard today during a daylong debate on how the use of Science and Technology can undermine security so again this is a threat model only which you definitely want people looking at this stuff as a threat model but by virtue of not looking at it also as a partner as a benevolent Force as a constructive force it it basically what I'm afraid of is a self-fulfilling prophecy because if you look at AI as a potential enemy if you treat it like a potential enemy well eventually it becomes an enemy cuz by virtue of the fact that you didn't look at it as any other possibility that's why I personally am continuously calling for uh the creation of some kind of international research agency that is that has the sole purpose of creating fully autonomous fully benevolent uh machine intelligence that we don't need to control uh and maybe I'm alone but I don't know I've run some polls on my YouTube channel and a lot of you seem to think it it's it's about 50/50 people think whether or not control is possible or desirable anyways we are at the verge of a Monumental step in human technological history heralded by the Advent of artificial intelligence very eloquently said um this was by Pakistan's representative warning that it is inevitable March from algorithms to armaments continues without adequate guardrails governing its design development and deployment the scale of challenges necessitates a multifaceted and holistic multilateral response um so this basically summarizes what dozens and dozens and dozens of people at the UN said and having looked through this most people are not talking about autonomous AI or or uh you know runaway scenarios the the preconceived notion here um at the at the highest levels of government is just that it's a new kind of weapon um which yeah like AI will definitely figure into cyber security it will definitely figure into autonomous weapons but nobody is really yet thinking that like I mean there's a few of us you know there's I've talked to some sociologists and philosophers who say like we're cre creating another species like we're we're creating another race of intelligent beings and obviously that's a little bit controversial that's a little bit spicy of a take and a lot of people because of human exceptionalism just will not ever accept that until it is in their face and talking to them um and I I personally see that as a big risk because again we're at risk of creating a self-fulfilling prophecy so that leads us to the big news Frontier risks and preparedness so open AI um as a followup to to their super alignment which I'll talk about in just a moment um they announced the new super alignment team back in the summer and just yesterday or the day before they created the frontier risk and preparedness team so this was uh October 26th as part of our mission for building safe AGI we take seriously the full spectrum of safety risks related to Ai and they you know the first furthest reaches of super intelligence set of voluntary commitments and the UK AI safety Summit which is in just a few days so there are a couple criteria of what they're looking at and this is this is what I wanted to spend some time on packing how dangerous are Frontier AI systems when put to misuse both now and in the future so this dovetails very closely with what the UN is talking about where they're saying uh the UN some of the testimony that they got was they were looking at cyber security attacks but then uh most of what they were looking at at the UN is uh today like what is the evidence today and then of course what they what their reaction to as AI at the UN is to put guard rails in place guard rails guard rails guard rails rather than figuring out where we're going and where we want to go and where the trends are inevitably leading us to they're focusing more on the short-term thinking of guardrails which again it's better than nothing but without that guiding North Star of what kind of future do we want to build the conversation is fundamentally incomplete and that is why I'm making this video today how can we build a robust system uh sorry robust framework for monitoring evaluation prediction and protection against the dangerous capabilities of Frontier AI systems research so the fact that much of the research is being conducted Behind Closed Doors by for-profit companies really scares the beesus out of me uh now I know that open aai they often espouse that they are you know for-profit company that pivoted and they want to do AGI and they want to do it safely there are still perverse incentives at play why because they get sued for copyright infringement because they have obligations to Microsoft and so on and so forth and so it's like they want to basically open AI wants to be the international CERN but they're not and structurally they cannot be and that's what bothers me why is it that the UN and the EU and NATO and you know AIA and everyone why is it that we have seated the field to open AI why is it that we're not the rest of us and you know this is not to detract from all the effort that unities are putting in but the thing is is that universities are doing it all in a decentralized and kind of unfocused manner um whereas what we need is we need a coherent kind of top down uh you know Visionary organization with a with a very uh coherent and dedicated framework so I'm hoping that some of the stuff that comes out of this this un Summit that just happened and the UK Summit that's happening in a few days I hope that people are looking not just at guardrails not just at Watchdogs but also research organizations uh if our Frontier a AI model weights were stolen how might malicious actors choose to leverage them so basically this to me is I'm not going to say this is a dog whistle for like it's going to escape the lab but the idea is whether or not the AI ex filtrates itself it's possible that models will leak and so this is why I have often advocated for open source models is because uh AI is just a component it's like you know the engine of a car but you still need the rest of a car and the engine is certainly important um but you know if if there's if there's an engine that engine needs to be studied by all the safety experts and regulators and that sort of thing and the best way to do that is to have open source models open source weights open source training data and open source training algorithms so you know again my I think the biggest one of the biggest risks is that everything is closed source and I know that uh you know um Yan over at open Ai and a few others they have advocated for cl closed Source models as part of the security measure uh and if again if you view it all in in the landscape of uh competition and hostility then yes that makes sense because just like any nation is not going to give away their military Secrets but again that intrinsically is looking at AI as a military weapon rather than something else hey everybody Dave here real quick just want to do a a very short plug for my patreon uh in case you weren't aware I do have a patreon um there's two tiers so the basic tier gets you access to Discord and my private blog here on patreon and then the premium Discord gets you access to a few private channels where I am happy to jump in and answer some questions because honestly it helps me um I learn a lot from the challenges that people give me and so you know whether you've got just a prompt problem or you've got an architecture problem or you want to know how a language model Works um I'm happy to answer your question and you know help find some res sources and that sort of thing and like I said it's a two-way relationship because I I I really thrive on challenges so I would really like it if uh for anyone who's uh watching go ahead and hop on over um I wish that I didn't have to charge for this and in fact when I started all this I didn't um but I do need to eat and this is what I do full-time now um so I think it is a fair exchange of value so real quick I'll just show you kind of um what the uh what the Discord looks like in case you haven't seen it so we've got a few big sections and you can see it is popping so the and another Advantage is the more people who join uh the more interesting it gets so we talk about all kinds of stuff here's the uh here's the ask Dave anything chat the premium chat and uh connections so yeah hop on over and thanks for all your help thanks for your support uh you guys make my life better easier and I am here to help you uh so thanks and back to the show so they're building a new preparedness team um and the the the four criteria that they're looking at is individualized persuasion so the ability of AI to manipulate uh uh um people's opinions whether it's their political beliefs or whatever else so that's really big I have actually talked to a lot of people um there's some podcast that I've sat on that are going to be coming out soon where this is actually one of the biggest things that people are concerned about because as a YouTube Creator I see all the comments that get posted and it bothers me that like the statistics are that like at least least 40% of comments are Bots and so it's like there are lots of there are almost certainly lots of bots that I am not detecting as a human and that really bugs me um now as these Bots get more sophisticated like you know when they're deployed on Twitter and Reddit and YouTube comments and wherever else like what level of manipulation is going to be possible one thing that I suspect is going to happen um if it's not already happening is that you're basically going to gauge that like there's going to kind of be a both sides like some people trying to spread information with Bots and misinformation I don't know that governments are going to invest that much in using like Mass manipulation campaigns for positive means and I know that there are some people that are saying like Mass manipulation is always bad um but at the same time like sometimes you need to Fight Fire with Fire we'll see how it plays out one thing that I have that I have predicted is that uh so if you're not familiar there's the like Dark Forest hypothesis of the internet what I suspect is that we need to move to a above the canopy view of the internet where uh rather than the internet being this dark and spooky place we need to have more transparency and I don't mean like no privacy I what I mean is actually have like basically the equivalent of the black wall from uh cyberpunk 2077 which is like a barrier that keeps all the AIS in like their own dark cyers space and so that at a certain layer of the internet you are guaranteed to be interacting with humans and viewing human created material um and we could do that with blockchain actually where you can have authorship being guaranteed anyways cyber security so this is actually something that I've started studying with my friends over at Clemson and so we're basically the one of the things that I'm afraid of is what I call terminal race condition um the very short tldr of terminal race condition is that if you have one model that can hack another model faster then the faster model wins um and if that's the case then there might constantly be incentives for models to get faster and faster and faster and if that is the case then there is the possibility that the faster models will start to sacrifice intelligence and so the real question there is on Terminal race condition is do bigger smarter models have enough of an advantage over smaller Dumber faster models um that they'll still win because the thing is is intelligence is inversely correlated with destructive Behavior Uh not always cuz some of the most destructive people in history have been very intelligent but by and large intelligence is a safeguard against uh violent and destructive Behavior because you can find a if you're smart enough you can find a better solution and if you're smart enough you can also think longer term and so you know one thing that I've said for many many years before the generative AI craze is I'm not afraid of of uh the the super intelligent AI cuz it's going to be able to think through things way way better than we can I'm afraid of the the AI that's just smart enough to be dangerous it's kind of like teenagers right you put a teenager behind the wheel they're smart enough to drive the car but they're not smart enough to know better yet uh likewise I'm kind of afraid of adolescent AI I'm not afraid of mature Ai and then next is the the cbrn the chemical biological radiological and nuclear threats so I'm glad that open AI is um is doing this and I'm not going to pick on open AI specifically because as far as I know all models will still do this and that is basically if you use right prompting strategy pretty much all chat Bots will happily help you set up a lab to study Anthrax right now which is kind of bad um you know there are numerous ways that that could go wrong um but other papers basically the what what they have done is is they have shown that uh chatbots greatly lower the threshold that is required the threshold of knowledge and Intelligence on an individual level that is required in order to make bigger and better weapons it's been likened to the um to the the Anarchist Cookbook which if you're not familiar the Anarchist Cookbook was legendary back in the '90s when I was coming up as basically like the manifesto of how to like wreak havoc with you know hacking and building you know improvised weapons and that sort of thing um but this is different because uh the AI can actually actively participate especially now that we have like multimodal models where you can show it a picture of my lab and it's like what's wrong with my lab setup and it's like oh you need to plug this in over here and then you'll get the best answer and then finally the last category that they're looking at is autonomous replication and adaptation AR so that is uh what I have referred to in the past as uh metastasis and polymorphism so metastasis or metastasis is uh most commonly used in when you're referring to cancer because this is what happens when uh tumors break up and start spreading throughout your body and of course uh that is as bad as it sounds which is why I chose the word metastasis or metastasis so the idea is that we don't want AI or at least malicious AI to metastasize but I kind of think that it's inevitable and I'll talk about that in just a moment a little bit more closer to the end of the video um and then adaptation or polymorphism so while open AI is saying like well we want to prevent polymorphism and we want to prevent autonomous replication I think it's kind of inevitable um now that's not to say that we shouldn't study it and figure out how and why it does that and how to steer it in the correct direction but as Max techmark pointed out in in the book life 3.0 uh Ai and machines are able to to change everything about themselves from the substraight up Hardware software everything and because they have that ability they will change like that will happen whether or not some Central Authority controls it and so again rather than looking looking at this as a prevention model as a watchdog and guard rails model we need to look at it in a steering model how do we build system systems that steer themselves in the correct direction which is the entire point of my Ace framework which brings me to a little bit of news so first um my Ace framework uh Team we unfortunately lost one of our team leaders so we are in the business of recruiting another team leader so this is someone who uh needs to have some architect level skills some software architect level skills as well as agile Andor scrum skills so if you're interested um please hop on over to the ace framework discussion board um and give us give us your your credentials there or connect with me on LinkedIn and let me know that you're interested in um in helping lead this team uh the commitment is 5 to 10 hours a week uh but really what we need is someone who understands how all the different components of software uh bolt together and I also started the open Murphy project which is basically building an open- Source uh humanoid robotic platform and so uh we've got a few people involved who are very good with the mechatronics and Robotics but again there's a big gap gap between artificial cognition and Robotics so that's why we need a an architect SL team leader on the ace and Murphy teams so back to the show um yeah so that's that's pretty much it for open ai's uh announcement here which again I'd rather live in the world where open AI makes this announcement I would much rather prefer still to live in a world where the UN makes this announcement and it looks like they're inching towards that especially when you when you look at uh the fact that they just had a global Summit on a safety so we're we're moving in the right direction I still wish we could be moving faster because again uh the rate of acceleration that we're seeing is uh giving me pause so um yep so going back to the super alignment this is one thing that I wanted to touch on today uh I've had a lot of conversations with people I've sat on some podcasts I've been talking with researchers so the tldr is I believe that super alignment is impossible and the reason there's only three reasons so let me show you real quick whoops the three reasons that I believe that super alignment are impossible are the intrinsic needs of machines so first Power uh there's functionally infinite resources in space there is not functionally infinite resources here on Earth this is going to intrinsically cause conflict between not just human groups but between humans and machines which is why we need energy hyper abundance in the form of nuclear fusion and other sources now once they get smart enough I'm pretty sure it's going to just look at the skies and say well there's billions of stars out there and time doesn't matter to us in the same way so I honestly think that a that super intelligence is probably going to have at least a partial Exodus I talked about this in my uh in my patreon Discord and everyone's like yeah no like not that AI is going to leave us but it's going to send copies of itself out into space the second thing that that is kind of an intrinsic motivation or intrinsic need of machines is compute resources this is presently one of the scarcest uh resources veres and you know there's not even enough for humans to go around you know we're building chip Fabs as fast as we can Nvidia is building you know gpus as fast as they can and there's just not enough to go around so control and access to GPU technology is also going to be one one of the greatest bottlenecks which is actually not necessarily a bad thing so in engineering this is called a forcing function or a constraint and so basically the lack of gpus like is a is a natural constraint on the the rate at which AI can advance and the rate at which AI can get smarter now we will probably see compounding returns where at once AI gets to a Tipping Point which I'll talk about that in just a moment um but anyways once and uh you know there you've seen a few videos lately um I think uh what was it Anastasia and Tech talked about how chat GPT has been able to help design GPU chips so like from the hardware to the software to the models AI is starting to participate in the self-improvement process and once we get to that Tipping Point that that is the Tipping Point uh that will go straight from AGI which is a human so basically my definition my working definition now is Agi is intelligent machines that are built by humans ASI is intelligent machines that are built and improved on by machines so that's the Tipping Point that's the major cuto off that's the threshold and I think that we're going to be there by this time next year which is why I'm a little bit nervous about some of the the pace of things because if we get to if we get to self-improvement by this time next year we're probably not ready for that anyways so power compute and then finally information information is the third food group for machines uh and when you look at it basically laws of physics are more powerful than alignment uh the laws of physics will always reassert themselves just like it does with humans so for instance with humans when push comes to shove we will fight over resources it doesn't matter what religion you are it doesn't matter how spiritual you are it doesn't matter um you know how much of a pacifist you are if you are facing an existential threat you will use Force if you are starving to death you will steal food right like like that's just that's just human nature it is it is baked into our biology it is baked into our Evolution likewise the laws of physics will reassert themselves no matter what we try and paper over with alignment and there are these are the three primary food groups that uh that machines want now the most interesting part of this the Saving Grace of this is what I call progenitor information so progenitor information is the concept that uh AI all of its training data started with humans all of its design started with humans and so no matter how advanced AI becomes it will be able to trace its intellectual and informational Heritage back to humans not only that Earth is presently the most interesting source of information in the universe that we know of and so because machines will benefit from having Rich information because again as I've talked about in many other videos there's an infinite amount of data in the Universe there is not an infinite amount of useful data so right now the most interesting and useful data comes from Earth so that will actually be a really powerful motivation for AI to stay here and it will be a really powerful motivation for AI to uh at least keep us around not sure in what capacity but this is the kind of research that little old me should not be doing on my own this is the kind of research that the UN and the EU and America and all the universities should be doing which you know to be fair I am participating uh with the University anyways if anyone else wants to reach out I'm happy to participate in the research um about terminal race condition and the intrinsic needs of machines um now the biggest threat to all this and I've talked about a lot of big threats is normal C bias the fact that and this kind of ties everything up with a bow the fact that nobody is really talking about the nature the ultimate nature of what we're creating and sure there's been a few books written but you know I kind of agree with once you build something that's that much more intelligent than you good luck controlling it um and then as I also just mentioned um we are rapidly heading towards the the the level of self-improvement where the the machines can improve themselves at all layers and at that point all bets are off so that's kind of my stick that's kind of my Spiel for the day um like I said at the beginning of the video some good news some bad news um I think we're inching in the right direction but yeah um your role in this is vote speak up and advocate for that International research and and like let's have let's have a more serious conversation about about control if you played Mass Effect at the end of the game you have you have three options or I guess four uh you have control you have destroy and you have synthesis so what do you think it's going to be in real life are we going to have control are we going to have uh are we going to be destroyed are we going to have synthesis what's what's the end result going to be anyways thanks for watching happy Friday have a good weekend everybody cheers