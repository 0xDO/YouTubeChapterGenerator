one question that I get a lot is how do you get more variety out of GPT and language models and so there's a few ways that you can do this and one of the ways that most people are familiar with getting more variety and uh variability and interesting outputs from GPT is just to turn up the temperature and of course you can turn up the temperature um but what many people have noticed is I always keep the temperature at zero and so when you keep the temperature of GPT at zero it is mechanistic it is uh deterministic meaning that with the same input you will get the same output every single time and so what you need to know about temperature is that this is an artificial way of getting variability and so there's a lot of math involved I talked to a professor from Duke University a while ago about this and the Very simplified version is that there is a distribution of probable tokens and what the temperature does is that it increases the likelihood Ood of choosing a less likely token and so this is why you can get just really random behaviors out of it just by turning up the temperature now that being said this is a simple math tweak and there is an entirely other uh method of getting lots of Randomness lots of entropy and variability out of GPT models and this is what I call uh me mechanistic or algorithmic entropy and so I ran an experiment a long time ago back in gpt3 So a whole generation AG go where the idea was I was going to generate uh a bunch of synopses uh story synopses so that I could uh fine-tune a model to be a story synopsis generator obviously with uh with the level of sophistication that models have today that's no longer necessary because the context windows are larger uh which gives us room for more uh instructions it also the models are smarter which means that they can do a better job so let me show you that original project real quick so this was September 23rd 202 2 more than a year ago so this is before chat GPT uh was even uh well it was probably on the horizon I think open AI was working on it but before we even had the first inklings of it so what I did with this and it's very complex but you can see uh there's input formats that I was looking for but the real key thing was lists of variables so this is this is what I mean by algorithmic entropy is where you can take information sources in this case I just use a like several lists of variables which when you have like they multiply right so let's say you have five lists of five variables um you pick one at random you have 25 different combinations uh to start with but you go up to a list of like if you have 10 variables and you have 30 versions each that's uh that it goes up is it exponentially factorially anyways you get a lot of options um when you have all these variables that to pick from and so for instance in generating synopsis I had a bunch of character profiles so in this case I had 30 different character profiles to pick from and that was just one variable I also had genres to pick from I think I had over 100 genres yeah 192 genres to pick from then in terms of Paces I I had only three Paces to pick from so this is how fast the story so you know Fast Pace intensifying or leisurely um and then settings in terms of settings I had 34 and of course you can have hundreds and hundreds of settings uh then story lines you know action-packed character driven I had 11 options here so in total I think I calculated it out back when when I first made this there was like 187 million different uh combinations that this schema would come up with and obviously that is more combinations than you will ever generate it is more than you will ever read but that is also a vast number of options and so then I wanted to show you kind of how I would approach this today so you can ignore that that old repository in the scripts because again that was back in the days of gpt3 which had a much smaller token window and uh very different limitations from what we have today so hello David Shapiro here I am once again asking for your support please head on over to my patreon page right now and sign up you can get on the free tier which means you'll get some critical updates about my books that are coming out but if you want the real shindig the full shebang you should join on one of the $5 or $50 tiers we've got a lot of creative and clever people there you're going to have a real ball if you join so head on over right now it's patreon.com davh you can't miss it and you can't miss out so the reason I'm making this video is because I get this question a lot um from my patreon uh from clients from you know comments or whatever and so this is far and way the best way to get entropy or variability or creativity out of GPT so uh this this technique applies to uh marketing to fiction to uh even problem solving honestly like you could do the same thing and so Mission you are a story synopsis generator again you always give it Mission or context uh you will be given a list of variables which you'll then use to brainstorm a complete story synopsis all three acts not just the overview output format I tell it what format to use uh just basically use markdown and headers scope I said each Act that you describe should be three solid paragraphs three acts and three paragraphs means nine full paragraphs in total and then another thing I said give it character name and places make sure you come up with specific names for everyone in every place do not use generic placeholders like the main character this is something that GPT does is it'll kind of use like the main character uh their Hometown you know it'll kind of refer to the things rather than naming them so I said all right make sure you use proper names for everything so then I went back to my old repo and I grabbed a whole bunch of variables so tone creepy unsettling and visceral time middle Renaissance style dialect filled uh scope sweeping location Scandinavia pace intensifying genre conspiracy Thriller characters exaggerated Larger than Life and so then what it generated it says in the heart of Scandinavia during the middle Renaissance A peculiar and unsettling tale unfolds our protagonist a robust and eccentric scholar named Olaf Gustafson I'm pretty sure that's the name of what's his name from uh Vikings Valhalla actually no it's Olaf haraldson anyways resides in the bustling city of Stockholm Olaf is known for his Larger than Life personality and is inable curiosity about the world which whoops where did it go come back come back uh which darn uh which often leads him into strange and dangerous situation he stumbles upon an ancient uh manuscript hidden in the depths of the Royal Library the man so you can see like I'm not going to read you the whole thing but you can see that it really took these variables and ran with it and like you make these lists longer or what you can also do and this is a more advanced technique is you can use prompt chaining such as Lang chain or tree of thought or whatever to actually brainstorm the lists of variables first and then you pick from those variables randomly now you do need to add entropy from outside the model so this is something that anyone familiar with computer science will know is that uh generating true Randomness is actually really hard so if you use the language model to generate its own Randomness you're going to be intrinsically constrained because like a computer it's just Computing so you can use like if you have a a random list generator you can go you can take the temperature higher and so here's what here's how I would do this let me um let me show you real quick okay so this is an example of why you don't want to use high temperature is because I said the temper temp to two and it you know it said Loki and then just completely went off the rails so this is why I don't rely on temperature because it just breaks the model so let's turn that temperature back down and so you say like Scandinavian names um and so then Anders bjor yada yada yada Etc so but obviously if you run this again you'll get the same exact list Anders Bjorn carsten Etc so obviously if it's very deterministic you need to change some input variables so there's a few ways that you can do this you can literally just do like random characters which is easy enough to generate from you know the random uh uh module in Python and this will give you different results so leaving it at a temperature zero actually no it didn't oh no we're getting a couple different ones um but then you can also turn the temperature up a little bit which will also help you get some more different results actually no we're still getting pretty similar results this is actually a really good example as to why that algorithmic entropy is really good because in like you can say like Scandinavian name you know then you could also add other variables like uh royal names Coastal names uh you know wealthy names or whatever and so then this will actually give you hopefully give you some different results Eric ingred Bjorn yeah so you see by uh adding some some variables this is the key thing that you need to know for entropy is injecting variables from external sources whether you read it from RSS feeds or Twitter posts or Reddit posts or even the the the uh you know just random lists of words so there's like dictionaries out there that you can um use this to inject from anyways I just wanted to get this video out there because this is a question that I've gotten a few times um recently so it tells me that uh some of you needed to know this so yeah I hope you got a lot out of this uh the repo if you want to go check it out is the Dave shap synopsis generator like I said this is an old old old thing the code is completely useless today but there is some useful information in here if you just want to see how I went about it and all the formatting and stuff and I showed you the story uh synopsis that it generated just a moment ago which worked really well so anyways thanks for watching cheers like subscribe etc etc have a good one