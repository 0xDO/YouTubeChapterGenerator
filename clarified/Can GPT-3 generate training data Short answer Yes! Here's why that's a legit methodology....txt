Hey everybody,

David Shapiro here with a video. I know that I did say that I'm taking a break from research, but apparently I'm getting back into the groove of making YouTube videos.

There was a discussion on the Open AI forum and this guy saw my video about reducing confabulation and he had some questions.

First of all, I just wanted to say that if I have not been as helpful of an educator as some of you would have liked, that's my fault because I was primarily using YouTube as a mode of dissemination. But I'm learning to put on my professor hat and I'm getting it all decked out in my own office in my house.

So anyways, going through this discussion, the first thing is like, okay, why what's the benefit if you're just using one model to generate output and then fine-tune the model? What are you actually getting out of that?

And so I wanted to kind of go over kind of the the the top level reasons even if you don't use much different data. There's a few benefits.

So first of all, when you generate fine-tuning data or synthetic data with something like GPT3, you can filter out the ones that you don't like. So that you can get a more consistent model.

And then there's also the problem of confabulation or hallucination in these models. So then another advantage is that you can incorporate multiple prompts. So for instance, rather than just have one prompt, you can have data that is several different prompts either stacked or you can have prompt chaining to generate the output.

Or you can have more lateral scaling which by that I mean you can have multiple different types of problems in your fine tuning data. I've demonstrated that in my core objective functions fine tuning videos where I actually train one model to do three different tasks.

So you can either do different tasks like go sideways where it's like task one task two task three and you have one model that can do all of them. Or you can do multiple tasks or both in my core objective functions.

In experiment four, I actually had to use maybe it wasn't this one. Oh yeah, so in this one I actually had to use I I did both. So this one is trained to do three different functions three different tasks which is reduce suffering, increase prosperity, and increase understanding.

But to increase understanding, to get that synthetic data, it actually took three prompts. Most of it was just the first two prompts, but the point is that I basically trained one model to do five different tasks all in one.

So that's another advantage of using GPT3 with its own synthetic data is you can kind of compress different problems into one model. And in their documentation, OpenAI said that doing this actually tends to increase the performance of all. So you get some transference effect where the the bigger your fine-tuning data set is, the better performance that you get overall.

And then finally, you can incorporate lots of different data. So if you don't if you do 100 synthetic data, which this is not so in this one, I had I had some real world data from like Reddit, which is so that's not synthetic at all.

But in my chat bots, that I did, those are some of those are 100 synthetic. Let's see. I actually the movie script generator because in this one, the premises were all generated by GPT3 as well, right? So this is 100 synthetic data where the input that I generated was synthetic and then the output was also synthetic.

Synthetic data is data that is generated by a computer. This can be done by taking real world data and then only the output is synthetic. This means that the data is not entirely real, but it is not entirely fake either. This can be useful for fine-tuning machine learning models so that they can be more robust and Generalizable.

Remember, I think that was about it. Um, yeah, so you know, accuracy, precision, but then confabulation whole new whole new animal. Um, I think that's about it. Yeah, so uh, gr good questions. And again, I just want to reiterate like I'm stepping into a new role as an educator. I'm not just sharing you know because most of what I've done up to this point is being like working entirely on my own. Um, just kind of like off you know writing books doing research which that's one thing, right? As an individual contributor, but as someone that like now people like a lot of you are asking questions and sometimes like we're on different wavelengths, right? Like I have been keeping up with synthetic data, right? The idea that you can use transformers to do synthetic data, but not everyone is up to the same speed, right? And that is me learning about my role, my new role as not just someone who's doing research and sharing ideas, but also as an educator. So if I have again, I'll just reiterate if I have not been as helpful as you would like, I'm learning, right? I'm I'm learning to teach and I'm learning to do this better. All right, I'm repeating myself now, so I'll go ahead and cut it off. But there you have it. Um, the short answer, yes, synthetic data is perfectly legit. Um, there are certain strengths and weaknesses as with all methodologies in science. So thanks for watching and uh check in check in again later.