How will we know when artificial general intelligence (AGI) has been achieved? This is a complicated question, because once something is smarter than us, it will be very difficult to understand it.

For example, any gifted child who is now a gifted adult will be familiar with the idea that we have to dumb ourselves down to reach a general audience. So if we're really smart, we understand things that other people don't, and if we talk at our level, other people will be lost. Once AGI machines comprehend more than we do, they will have to simplify and explain their reasoning and logic to us so that we can understand it.

Another critical thing to consider is that we will need to ensure that AGI is self-correcting and self-improving before it becomes incomprehensible. Self-improvement is just the idea of doing something better next time. But self-correction is a whole other ball game. For example, if we're out with friends and we mistakenly say something that hurts someone's feelings, we might self-correct by saying, "I don't want to do that again because I don't want to hurt my friends' feelings." But if we're an AGI machine with the power over life and death, we need to be able to measure our own performance and flaws in order to self-correct.

The bottom line is that we need to spend a lot of time working on self-correction and self-improvement before AGI becomes incomprehensible. Otherwise, we might make the same mistakes as less intelligent people and think that AGI is wrong and stupid when it's actually smarter than us.