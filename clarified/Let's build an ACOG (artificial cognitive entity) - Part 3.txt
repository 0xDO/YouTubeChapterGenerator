Cognitive architectures are computational or mathematical models that mimic a cognitive engine, such as the human brain. Robotics is primarily based on the input-processing-output model, which creates a feedback loop with the environment. This loop is what allows a robot to interact with its surroundings. However, cognitive architectures also have a second loop, called the thought loop, which handles all the thinking that goes on behind the scenes in the brain, both consciously and unconsciously.

One of the key innovations of cognitive architectures is that they can be trained with thousands of examples of the behavior we want to see. For example, if we want our robot to reduce suffering, we can give it examples of how to reduce suffering from all over the internet. This training data allows our robot to learn how to best reduce suffering in any given situation.

 Patricia S. Churchland's book, Brain Trust: What Neuroscience Tells Us About Morality, is a great resource for understanding how morality works from a neuroscientific perspective. In her book, Churchland argues that philosophy is not anchored to anything measurable and that contemporary philosophy is nothing more than armchair opinion. She suggests that sociologists, psychologists, and neuroscientists are the correct people to handle moral and ethical questions when it comes to artificial intelligence.

Churchland's book provides a much better model for designing AI than contemporary philosophy. She argues that brains are organized to seek well-being and that morality is based on this natural impulse. By understanding how morality works from a neuroscientific perspective, we can design better AI that is more likely to act in ways that benefit humanity as a whole.

In her book, "The Feeling of What Happens: Body and Emotion in the Making of Consciousness," neuroscientist Antonio Damasio argues that our nervous system is designed to seek relief from ill-being (suffering) and to promote well-being. She builds up to this assertion by first discussing the purpose of a nervous system, which is to detect injuries (nociception) in order to protect ourselves. Damasio then goes on to talk about what it means to care for ourselves and others, and how our morality is derived from this.

This book was recommended to me by someone, and I can see why. It is an excellent book that supports my work on artificial intelligence (AI) and morality. In particular, I found the section on the brain's organization to seek well-being and avoid suffering to be very insightful. However, I did find the book to be quite dry and dense in parts.

Overall, I would recommend this book to anyone interested in researching AI and morality. It provides a lot of valuable information, even if some of it is review for me.

When it comes to protecting against a tsunami, there is no definitive answer. However, some possible actions include evacuating the high ground, opening up flood gates to allow water to enter the city, and building barriers to protect against the incoming water.

In general, it is important to have models that are intrinsically aligned with our goals. However, we also need to have a system of checks and balances in place to ensure that our actions are having the desired effect.

One way to do this is to constantly reflect on our actions and their consequences. Did our actions reduce suffering? Increase prosperity? Or increase understanding? In cases where there are unintended consequences, we need to take corrective action.

For example, if we are huggers, we need to be aware of the social norms around hugging and make sure that we are not violating them. If we do violate a social norm, we should take corrective action to make sure it doesn't happen again.

In order to provide this feedback to a machine, we need to have a self-correction mechanism in place. This will allow the machine to constantly monitor itself against a set of standards and take corrective action when necessary.

Getting enough sleep is important for feeling happy and healthy. This may be why it is inferring that the user's difficulty of sleeping at night is a factor in their sadness. Raven is also thinking that the user might be sleeping enough and that there might be something else causing the user's difficulty sleeping at night. This shows that raven is empathizing with the user and trying to understand their feelings and problems.

So, by abstracting by observing its own thoughts and inputs and outputs, we can create an acog that is self-evaluating all the time and it's saying this is who i am right. This is called the persona. If we go back to the inner loop where we ask this question who am i, this is a critical step for what, in humans, you'd call ego formation or identity formation. I chose to use the term persona because that's a little bit more neutral. It's a little less anthropic, although you know personality persona that's still pretty anthropomorphizing.

I've already done a lot of this work, so I'll probably leave you today. I know this video is probably disappointing, but we're about to launch into a heck of a lot of prototyping and research and trial and error. So what I'll leave you with today is python import time time dot time.

So we're going to say this is the current time. So we're going to start what I call the genesis block or the genesis memory, and that is going to be the first memory in the nexus of our acog. This is like your first memory as when you wake up in the world. Human brains have to self-organize, so you there's infantile amnesia or childhood amnesia. You don't remember most of the first couple years of your life because your brain was not organized enough to form permanent memories yet. At least not explicit declarative memories. You do still have memory your body your your brain does remember a lot of stuff. This goes into it as well as what like what happens in infancy how that is remembered in the brain and what it how it affects your social behavior later on.

Anyway, we're going to establish the identity of what this thing is so when it wakes up because it's not going to have any sensory input or output, it's going to be running in a vacuum. And I've seen some other people on twitter do stuff like this and what happens when you when you start these things in a vacuum is that they go kind of crazy. I don't know if they're using a similar architecture but I have seen that some other people are kind of converging onto something similar. So I'm giving it the time stamp this is roughly when that happens so it will know like when it was created. And ideally, in the long run, because this is just a text file, I can modify it in the long run. We're going to want our acogs to have their memory stored in a blockchain so that they can't be modified.

Okay, I've thrown a lot of theory at you guys. I know that this is probably not what you were hoping for but a bunch of you did ask some really good questions and so I wanted to explain those before launching forward. I think that'll be it for today. Yeah, because this is a lot of brain juice going into it. I need to finish reading this book too. But thanks for watching, like and subscribe. I hope you got something out of this.