Artificial cognitive entities, or ACEs, are a type of artificial intelligence that focuses on natural language processing. This allows them to better approximate human thought. ACEs have the potential to be more intelligent than humans in some ways, and so it is important to build them in a way that is trustworthy.

There are three steps to building an ACE that is benevolent by design: earning user trust, demonstrating the ability to self-monitor, self-check, and self-correct, and adhering to principles of beneficence.

Earning user trust is essential for an ACE to be successful. This can be done through data privacy, security, reliability, and utility. The ACE must be useful to the user in order to earn their trust. It also must be reliable, so that the user can rely on it for good advice and direction.

Once trust is established, the ACE must demonstrate the ability to self-monitor, self-check, and self-correct. Self-monitoring is the ability to monitor one's own thoughts and actions to ensure that they align with a goal. Self-checking is the ability to check whether an action had the intended result. If not, the ACE must be able to correct itself.

Adhering to principles of beneficence is also important for an ACE. Beneficence is the principle of doing good. An ACE that is benevolent by design will do what is good for the user, not just what the user wants.

By following these steps, it is possible to build an artificial intelligence that is trustworthy and beneficial.

Self-checking and self-correction are important aspects of artificial cognition that will help machines learn and improve over time. The goal is to eventually earn our trust so that machines can be granted autonomy. This can be done by demonstrating reliability over many years and possibly multiple generations.

Raven is an artificial cognition platform that is being developed with the goal of creating a machine that can think and communicate like a human. The platform is composed of three primary components: the inner loop, the long-term chat, and the external data source chat. The inner loop is the part of the platform that handles the artificial cognition, while the long-term chat handles communication with external sources (such as Wikipedia or Google). The external data source chat is responsible for providing Raven with access to data from external sources.

The fine tuning data for the platform is being developed with the goal of compassionate listening and increasing understanding of the user. The data is being collected from synthesized conversations that center around two primary principles: compassionate listening and curiosity about the user. The goal is to help Raven learn how to reduce suffering for the user and increase their prosperity.

Raven's training data is designed to help the AI understand the world and themselves better. The three principles that guide this are reliability, utility, and transparency. In the future, Raven will explicitly adhere to these three objectives. Right now, they are only implicitly baked into the training data.

One of the most important functions for realizing an autonomous cognitive architecture is the ability to generate questions spontaneously and internally. This is something that sets humans apart from every other animal.

The top three problems that need to be addressed in order to create a safe and trustworthy AI are cognitive control, designing cognitive tasks, and measuring progress towards goals.

Cognitive control is about executive function or staying on task. This means keeping the AI focused and on track, as well as allowing it to task switch appropriately. For example, if the AI is talking to you about doing laundry and then hears your smoke alarm go off, it needs to be able to task switch to the emergency.

Designing cognitive tasks is about creating tasks for the AI to perform that are in line with its goals. For instance, if you ask the AI to tell you who you pissed off last year, it needs to be able to go through its memories and evaluate them for that result.

Measuring progress towards goals is about tracking the AI's progress and success over time. This is something that humans are able to do that other animals are not, and it is an important function to replicate in the AI.

In order for an entity to be an entity, it actually has to have a knowledge of itself - how it operates, what it's capable of, and what it's not capable of. This is what allows for self-correction, self-checking, and self-monitoring. Words are one thing, but code is another. I'm working on all this and a lot of it is closer to being realized than you might think.