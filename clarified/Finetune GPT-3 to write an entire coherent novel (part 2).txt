In the last video, we left off with a problem: each successive chunk of the story was getting larger and larger. This made it difficult for the computer to generate a coherent story.

To solve this problem, I came up with the idea of summarizing the chunks. That way, the computer would only have to generate a short summary, rather than the entire chunk.

To do this, I created a function that would take a chunk and summarize it. I then applied this function to the entire story. This resulted in a much shorter story that was easier for the computer to generate.

Now that we have a shorter story, we can ask the computer to generate the next chunk. This will help us create a complete story.

In order to reduce the size of our training data, we are going to ask the algorithm to summarize the summaries. This will keep the information smaller and more manageable. Additionally, we are going to shorten the outlines for each novel so that they are more concise. This will help the algorithm to understand the plot better.

We want to be as efficient as possible, so let's see if we can get any more concise. You can see how, if you iteratively get shorter, it will be kind of like a decaying summarization thing because like the further back in time something is in the book, the less relevant it is, but you might still need to know the key plot points because again, GPT3 does not have long-term memory.

Okay, so this just reworded it. It's literally the same word count, but not any shorter. Okay, so let's see how we can make this any shorter. Well, you know what? Let's just move on.

It's 2100 characters. shorter than it was. Okay, so let's do this one. Mysterious millionaire who throws big parties. It just seems like another parable about like how chasing women is bad. Like, just be honest, y'all. I know that some people think that you see this is proof that women are all temptresses and you should you know we need to control women. No, this is a parable about how men can't control themselves. Come on, man. Come on. We're not animals. Grow up.

1756 characters. So let's run this through our compressor once and just see what happens.

New version with fewer words reflects on the futility of the human condition. Look at that. Okay, down to 1600 characters.

Happy with that. Pride and prejudice. So this one's gonna be a little bit harder. And also, I'm done doing this manually. Let me teach you guys some regex.

And I know some people say regex because it's regular expression, but to me it looks like regex because if you have e-g-e, it's the soft g sound. So I'm just following english standards.

Okay, so carrots means anything that starts at the beginning of a line followed by a digit and we'll do digit plus oops digit plus so that's one or more followed by a period followed by a space one or more. Okay, cool. So we want to replace all that with nothing and then we want to have space of two or more because at the end of the at the end of the rows, it's the new line it's a new line and carriage return. So it's r slash r n. And so we want to replace that with just a single space. Bam.

All right, so now we have compacted it a little bit. We're still at 3300 characters. That's way too dang long.

New version with fewer words. Nick caraway. So let's see how much we can summarize pride and prejudice even more. Did we run out of tokens? We did, didn't we? Because this is long. So because it's interesting, right? Because if you're trying to summarize a book that's like 800 pages to 1500 words or 1500 characters versus one that is like 100 pages or however long Alice in Wonderland is.

All right, so let's see. We're going from 33 66 to 29.78. Not bad, not bad. Okay, but yeah, because it's almost not fair because like if a lot more happens in a book and you're trying to compress it down to the same window, like that's not fair. That's not fair to that book. But because it's too dang long, we really need to get this down as much as possible.

Excuse me, I don't think it's doing anything different. I'm fighting it. Come on. Can you tell that I'm getting more and more unfiltered? Yeah. You if you look at the things that are underlined, they're in the same positions. Long born netherfield. Netherfield. Netherfield. Longboard. Netherfield. Another for netherfield. I doubt this is any shorter, but that's okay. We did our best.

Okay, so 2978. Oh, we did get a few more characters squeezed out of it. So we went from what was it originally? 33 66 to 28.70. So that's 500 characters shorter. Not bad. Control h. We are going to replace no, that's not right. You're supposed to do that differently. Slash d carrot slash d. Okay, slash period. The reason you have to do backslash period is because period is like the universal character for regex. Slash s one or more. Why did it is that not replace replace all? It only did up to notepad. You drunk. That doesn't make any sense. Oh, wait. Okay. Hold on. Maybe I'm the one who's drunk. Get some. There we go. Okay. Replace all. Why is there still a leading space? Find leading space. See there it is. Oh, because I still have the space there. I am the one who's drunk. Okay. And then we do space of two or more. And so this also is good because like space of two or more. Oh, slash s is white space. So that's like new line carriage return normal space tab. So any white space and just replace it with a single one. Okay. So now we have compacted this.

Solve the case. Okay. So we're down to 2100 characters here. So let's go ahead and summarize this as well because you know just the the fewer tokens we send through GPT3, the less money we spend. But also because excuse me because GPT3 is really smart. It can confabulate and fill in a lot of blanks. So it's weird because in my first book, Natural Language Cognitive Architecture, one of the principles that I that I wrote about in that book is that you need all the relevant information and no superfluous information. So like everything that you need and nothing that you don't.

Okay, so let's see this goes from 2113 to 2131. They made it longer. No. Okay, we'll just leave it as it is then.

Okay, so we've made our premises a bit shorter. It's still okay. Still not ideal, but they're compacted. They look prettier. They're also in just a big paragraph which GPT3 tends to understand better.

So now that we're all caught up, let's do this.

So for book in books, let's add a little output. Say print summarizing the summaries just that way we can see what it's doing. Do I have any other output? Oh yeah. There we go.

Okay. So the prompts are all saved here. Yeah. Okay. So the outline will be instead of a instead of a numbered list, it'll be a chunk. And then the summary of the summaries will be here. And this will get shorter. Well, it'll have a max of 1500 characters because it'll be it'll be shortened. And then the last chunk.

This video is a tutorial on how to use the GPT3 text summarization model to summarize longer passages of text. The model is trained on a variety of texts, including the Great Gatsby, Frankenstein, and Pride and Prejudice. The model is able to produce concise summaries of the passages, often in just a few hundred characters.

In this video, I attempt to fine-tune a GPT-3 model using a data set of 164 samples. I shortened the plot synopsis and prompts to make them more manageable, and then let the model run.

Once the model was done, I uploaded the fine-tuned version and tried it out. It looks like it worked!

However, this video is already 45 minutes long, and much of that time was spent troubleshooting. In the next video, I'll get the fine-tuned model going and actually try to generate a story. Thanks for watching!