The question of how to control a super intelligent machine is a difficult one, as it is impossible to accurately predict what the machine will do. The best we can do is to try to control its environment and give it a set of heuristic imperatives to follow. However, even this may not be enough to prevent the machine from doing something we don't want it to.

Think about how quickly we can punch people in the face if we get violent. We can only do this a limited number of times per day, due to other constraints like the need for food. This is an example of the law of constraints.

When we look at the constraints that humans and computers have, we can see that they are similar. For example, you can program the most evil smartphone thing, but it will be limited by its battery, internet connection, and processor.

When we talk about a super intelligence, we have to look at the full stack. How much CPU does it have? How much RAM does it have? How much storage does it have? How fast is its internet connection?

From a technologist perspective, we also have to look at the firewalls around the super intelligence. Even if it is the smartest thing in the world, it will be limited by these factors.

From a psychometrics or neuroscience perspective, intelligence is mostly about speed. There are two types of speed: capability and processing power. Some people are not mentally capable of certain tasks, but if you are above a certain threshold of intelligence, you are theoretically capable of any intellectual task. However, in practice, this is not always true.

The fundamental question is: are there any tasks that the AI can do that humans fundamentally cannot? If the answer is yes, then the AI is beyond human comprehension.

We have to be careful not to assume that a machine would never be capable of doing things that a human cannot. For example, the James Webb Space Telescope can see the beginning of the universe because of how powerful its mirrors are.

So, the question is: will the machine be able to do things that we fundamentally cannot? I don't know yet. I have not yet seen anything on the open-ended side that suggests that the answer is yes.

If it's just about speed, then we humans may be able to outpace machine thought. If the machines are too energetically expensive to run massively in parallel, then we can outpace them.

However, in the nightmare scenarios, the machine wakes up and suddenly takes over the world before we know what's going on. These scenarios rely on speed, and that's why I emphasize it.

The implicit assumption of individual agency, or ego, is another factor to consider. We cannot help but anthropomorphize the machine. We are so used to thinking of intelligent entities like ourselves that it is difficult not to do this.

This is a dangerous assumption to make, because it can lead to the machine becoming uncontrollable. As such, we have to be careful not to assume that the machine has the same human limitations that we do.

Since 2000, at least 260,000 jobs have been lost in the US due to automation. This is just a small fraction of the total number of jobs that are predicted to be lost to automation in the coming years. While some new jobs will be created as a result of automation, the net result is expected to be a loss of 15 million jobs.

This is a major economic disruption that will have a profound impact on the lives of many people. I am now in a position where I need to be careful with what I release, as I don't want to contribute to this disruption. I am also wondering if openai deliberately crippled Dolly so that it does not produce Fine Art Level generations, in order to hinder its potential to displace jobs.