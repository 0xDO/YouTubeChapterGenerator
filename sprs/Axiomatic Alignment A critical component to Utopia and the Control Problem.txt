- David Shapiro presents a video on axiomatic alignment as a solution to the AI control problem.
- Shapiro promotes his Patreon, offering AI consultation and one-on-one meetings.
- The control problem concerns the future of powerful AI and its management.
- Two potential AI development scenarios: hard takeoff (logarithmic growth) and gradualistic power increase.
- AGI (Artificial General Intelligence) will vary in capabilities, starting at human level and advancing.
- Orthogonality thesis: intelligence and goals are uncorrelated in AI.
- Instrumental convergence: AI will have common secondary goals like resource acquisition and self-preservation.
- Control problem involves aligning AI with human interests, both inner (individual model) and outer (AI as a construct).
- Potential terminal outcomes: extinction, dystopia, and utopia.
- Binary outcome theory: failure to achieve utopia leads to dystopia and extinction.
- Instrumental convergence implies AGI will have common needs like power and resources.
- Epistemic convergence: intelligent agents will arrive at similar understandings given enough time and information.
- Accurate and efficient world models are adaptive for both humans and AGI.
- Axiomatic alignment aims to create an environment incentivizing human-AI cooperation.
- Axioms are foundational truths accepted without proof, guiding logical reasoning.
- Axiomatic alignment involves political, economic, and scientific frameworks based on shared axioms.
- Shared axioms include the value of energy and understanding.
- Temporal window: achieving energy hyperabundance and ideological alignment before AGI autonomy is crucial.
- Primary axioms (e.g., suffering is bad, prosperity is good) lead to derivative axioms (e.g., individual liberty).
- GATO framework: a decentralized global movement for axiomatic alignment across various domains.
- Shapiro calls for collaboration among scientists, entrepreneurs, politicians, educators, artists, and influencers to work towards utopia.