David Shapiro discusses AI alignment, emphasizing the urgency of aligning AGI now, not later. He critiques the AI alignment field's current state, advocating for practical testing over hypothetical work. Shapiro proposes intelligence should be viewed as a system, not a single model. He conducts experiments with recursive loops and different agent models to demonstrate stability and instability in language models. The experiments reveal instruct models are more stable but less creative, while foundation models are more expansive but unstable. Future experiments should involve complex environments and multiple agents.