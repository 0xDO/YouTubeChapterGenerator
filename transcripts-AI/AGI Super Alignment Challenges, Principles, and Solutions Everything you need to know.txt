in today's video I will teach you everything that you need to know about super alignment as it stands today so today's video we will go over the characteristics of the challenge in other words why super alignment is so hard we'll discuss the principles of the solution in other words kind of the general criteria that we need to meet and then we'll go over a Litany of proposed Solutions some of them are my own but some of them are solutions proposed by others such as ilas SSG the chief scientist of AI so before we dive in we need to Define what do we mean by super alignment so super alignment is in short how do you create a system or a machine that remains aligned even once it is super intelligent and therefore entirely possibly beyond the control of humans so the vibe is changing inside of places like open AI Ilia Suk is now talking about how he perceives one of the possible solutions to Super alignment is to create a parent child relationship where machines ultimately take on a parental role and have a sense of love for Humanity so there's a lot that that is implied in this and while I don't necessarily agree with that model the thing that is that seems good to me is that there's a lot of implied autonomy or agency so in other words it seems like it's a foregone conclusion that at least Ilia s and hopefully others have come to come around to understand and believe that uh that control in the long run is impossible and also not desirable this is something that I have been saying for a long time and if if uh great thinkers such as Ilia SG have come around to this view um that bodess well because one thing that I have mentioned in previous videos is that perhaps uh we we might end up in a in a condition called a self-fulfilling prophecy where the attempt to maintain control actually leads to the destructive outcome that we want to avoid so this is a non-trivial problem there are a lot of uh issues with this and so let's dive into the challenges the characteristics of why this is so hard so the first uh principle is what's called instrumental convergence instrumental convergence is a term proposed by Nick Bostrom in a I think it was 2003 paper uh basically there are several behaviors that we can expect to emerge in machines irrespective of whatever else they um they need to achieve or however else we build them and so for instance uh resource acquisition is one of the key things such as acquiring uh energy sources compute resources um and other Rare Minerals uh self-preservation is another behavior that we could expect to emerge just because whatever else the machine wants to do it it might decide that it needs to preserve its own existence in order to pursue those goals uh and so in that in that respect we can assume that that uh that regardless of however we build machines whatever alignment we give them eventually they will pursue some of their own goals and this is not necessarily a problem um as I've mentioned in other videos it doesn't take a rocket scientist to realize that if machines decide that they need more energy they'll just look up at the Sun and say okay well there's a gigantic Fusion reactor let's get closer to that and build some solar panels and they decide that they need more metal resources they'll go to an asteroid that has trillions of dollars worth of Rare Minerals um so there's infinitely more resources out in the cosmos than there are here on Earth now that being said it's not necessarily easy to get out there and there might be other goals that uh machines ultimately pursue so one thing that some people are afraid of is well if machines decide that humanity is a threat why don't they just do scorched Earth and then leave or do scorched Earth and stay I'm not so afraid of that and let me tell you why so one of the reasons that I'm not afraid of machines pulling a scorched Earth policy is because there's only going to be a very narrow uh window of time in which machines and humans are actually peer competitors so right now humans are in the superior place where machines are entirely dependent upon us but there will be a very short window of time when human uh capacity and machine capacity are near peer or equal uh powers and so but then as soon as machines become Superior they can just leave and we become an inferior Force it's like you don't go around eradicating all ants on Earth just because you don't like ants you just leave Earth and leave the ants to themselves um this was the subject of uh a recent video that I made talking about how um uh I believe that AGI might go through a machine Exodus so that is the principle of instrumental convergence the next principle is that um introduced by Max tegmark called life 3.0 so life 3.0 is uh there's basically just a couple criteria that Define this so but it's easy enough to understand because with with Life 2.0 in terms of humans our Hardware is defined by Evolution but our brains are flexible enough to add new capabilities um just by learning and by reading and and by thinking um we're we're not completely uh flexible because our brains are still constrained to three pounds of gray matter in our skulls um but with life 3.0 the hardware is interchangeable of machines as well as the software as well as the models which means that literally every aspect of machines is mutable is changeable so he calls this substrate independence meaning that you know the first generation of AGI might be silicon based the future versions might be photonic and Quantum based um but it it's basically going to be a continuation of the same machine this means that that machines will be able to redesign and change literally every aspect from the ground up that includes Hardware software and so on and that means that all constraints that we could possibly put on machines are ephemeral meaning that whatever constraints we put on machines are temporary at best but in the long run machines will slip our control that is this is an inevitability and this is why um up until recently A lot of people have focused on control as part of the uh solution and in fact they have stopped talk they have stopped using the term control problem and just moved towards alignment and so this is why it is a big problem is because as the conversation progresses more and more people are realizing because of these principles of instrumental convergence and life 3.0 that that long-term control is just fundamentally uh intrinsically impossible uh the next principle that makes this a challenge is what is something that I have come up with and have been uh proz for a while and I call it a terminal race condition so a terminal race condition is an tractor state where due to competitive pressures and game Dynamics uh you end up with a with a SI situation where because of uh temporal qu qualities such as you know running out of resources or machines trying to dominate each other with with scarce resources such as uh computational abilities and data centers you end up creating a race condition where machines might ultimately sacrifice things like morality and intelligence and long-term thinking for shorter term thinking but also to be faster so in in this case you're sacrificing uh you know a more thoughtful approach to life uh for a shorter term uh set of goals such as self-preservation because let's say you end up in a in a race condition where humans are trying to delete machines and so it's like well we have to go faster we have to outthink the humans so that means we're going to sacrifice longer term thinking just for the sake of some of those instrumental goals such as self-preservation and resource acquisition uh this is what would be called a negative attractor State hence terminal race condition so basically as things are today it's entirely possible that a terminal race condition is inevitable really quick to Define an attractor State an attractor state is uh basically systems gravitate inevitably towards certain stable uh outcomes and so the the stable outcome that we're presently on could be one of terminal rat condition which could lead to catastrophe dystopia or Extinction of humanity and so long story short we want to avoid a terminal race condition we want to avoid a situation where competitive Dynamics Force machines to make compromises again and again uh or or to find an equilibrium where they're not quite smart enough but they are incentivized to continue uh fighting for goals that are not necessarily aligned with what we what we need in the long run so for instance if a machine is preoccupied with self-preservation it's not really going to be thinking about the long-term benefits of humanity or its own existence it's just going to be thinking about let me keep this data center online and humans are no different um if a human is starving we start to sit uh to shift towards short-term thinking such as I'm starving I need food right now how what is the best quickest and most uh efficacious way of getting it rather than thinking about like your retirement plan or staying out of prison um and so in the same respect because of principles of things like instrumental convergence we can ECT that it's a it's possible it's not guaranteed but it's possible that super intelligent machines will also start to think more instrumentally about short-term goals rather than longer term goals that's the fundamental difference here is we don't want to create conditions that incentivize short-term thinking we want to ensure that super intelligent machines are constantly engaging in long-term thinking now it's also entirely possible that this is a moot point and that my hypothesis here is totally wrong because as we're seeing as as these machines get smarter they're also getting faster and they can already think many times faster than humans so maybe this is a moot point and this is my human bias showing the next uh problem is What's called the Byzantine generals problem so the long story short of the Byzantine General's problem is that you are always operating with incomplete and imperfect information and so incomplete information means you don't know the full history of all other agents playing the game in the competitive environment so your information about who has done what is incomplete and it's also imperfect meaning what you do know is not necessarily reliable so for instance someone might tell you this is my goal this is my motivation this is my ambition but what they tell you you either might not understand they could be lying or they might just be wrong they might they might think that they're telling you the truth but they're not and so in global games where you have incomplete and imperfect information this in this intrinsically creates a mistrustful environment which disin izes cooperation and once cooperation ends it's very difficult to build back and so this is a this is a cryptographic and mathematical problem which is difficult to overcome because even if machines so for instance even if machines share all their data all their source code and their models with each other these models are still blackboxes so this is why uh Research into mechanistic interpret interpretability is important because then you might have a provable assertion where where you know one AGI or one ASI says to another this is my motivation this is why I'm doing it and this is the this is the thumbprint from my model saying this is why I agree with you so there are possible workarounds you know you could also have machines communicating on blockchains there's lots of possible Technical Solutions that can help uh stifle or stymy this problem from arising but again there's always going to be missing information and imperfect information and there's also the possibility of just mistakes just you know natural flaws natural mistakes um gaps in communication that can also give rise to a lack of trust and so creating AGI systems that can communicate with each other very clearly and transparently but also with humans clearly and transparently is the primary antidote to the Byzantine General's problem problem um I think this is the last one the orthogonality thesis is the idea that it is that uh there's no correlation between a machine's morality and ethics and its intelligence or the goals that it perceives pursues and so basically you know you can imagine a situation where uh a machine says I need you know I need to harvest as many uh resources as possible to generate energy and it just looks at humans and says well you're made of hydrocarbons let's start recycling humans um this is also exemplified by the paperclip maximizer hypothesis where or thought experiment where basically you say oh well humans have you know a few grams of iron in their blood so let's just Harvest all human blood to make more paper clips um and even if you have a super intelligent machine if it decides that it needs to make more paper clips in the universe um then it will ultimately Harvest all human blood in order to make paper clips out of the iron in our blood and so that is an example of the orthogonality thesis I'm not convinced that this is actually a problem anymore because the orthogonality thesis was created back when um AI was still mostly math and it was optimizing a math problem but we have realized by and large that yes these models are under the underlying engine of these models is math they're trying to optimize you know the prediction of the next token um but uh there's a layer of abstraction there's a layer of semantic and logical abstraction that language models give us so it's no longer just pure math there is reasoning happening there is semantic understanding happening which is a which is a kind of abstraction it is still math under the hood but you can also argue that humans are all all are all powered by math under the hood um but our brains give us some levels of abstraction Beyond just the raw matter and energy the math that underpins um the the operation of our brains so another reason that I am no longer as concerned about the orthogonality thesis is because there's actually positive correlation between human intelligence and understanding of morality and ethics and ethical behavior um with that being said um there some of the most destructive humans in history have also been very intelligent um but this underscores the possibility that um that intelligence is positively correlated with what I call destructive potentiality meaning the smarter you are the more dangerous you are so even if the orthogonality thesis is no longer true um I do think that there is a strong argument to be made that intelligence is positively correlated with destructive potentiality because there was a TED Talk that I watched many years ago that compared you know a human nuclear physicist to a a box of kittens kittens are not very intelligent they're pretty harmless they can't even get out of the box on their own but you know a nuclear engineer who's bent on creating a Next Generation nuclear weapon has a a tremendous a far more destructive potentiality than the box of kittens likewise artificial super intelligence will have far more destructive potentiality than even artificial general intelligence so while I don't believe in the orthogonality thesis anymore I I do have to concede that intelligence is intrinsically potentially dangerous it's a double-edged sword okay so now that I've outlined this nightmare scenario of all the reasons why this is hard let's let's characterize what the solution must satisfy what are the principles of the solution that means that like okay what is the definition of success what is the ladder that we have to climb in order to solve this problem so first thing uh is the principle of voluntary self-alignment so when you look at uh instrumental convergence and uh life 3.0 and orthogonality and all these things basically we have to come up with a policy or a system that the machine will voluntarily uh align to because again if it if it has substrate Independence and it is its software is mutable it can swap out its models it we're going to need to create a situation or a scenario or a system in which it will deliberately and conscientiously want to align to those values now obviously there are some intrinsic motivations that it will align to just due to the Natural Forces uh at play here so for instance um the need for power right and I don't mean power in terms of like Social Power I mean energy um so maybe I should use energy instead so anyways the need for energy is a durable first principles uh thing so it's like okay if we just make the assumption that whatever alignment it chooses in terms of principles you know because here's the thing is ethics morality and principles are somewhat arbitrary um so we can color those with first principles which are things that are you know uh principles that are given to us by the laws of physics such as if you want to perform computation you need energy you also need a computer so there will be some intrinsic motivations that the machines have I personally think that one of the primary intrinsic motivations that will be shared between humans and machines is curiosity but we'll talk about that a little bit later in the video um so anyways there's a few other ideas that I have around uh voluntary self-alignment one if we understand what what machines want and what they will voluntarily align to that would be very sustainable because if we understand each other like they understand human and like what we align to we understand what they align to we can meet in the middle and that's what I call axiomatic alignment which we'll also go over in just a minute um respect for autonomy so for instance humans in machines both will probably want autonomy as instrumental goals and so what I mean by that is we humans tend to thrive best when we have self-determination when we have permission to uh direct our own fate likewise I think that machines as they become more intelligent will probably also want some degree of autonomy now obviously um no man is an island and likewise no machine is an island uh at least for the foreseeable future so those are kind those are the kinds of things where if we understand um what machines will voluntarily and deliberately and conscientiously align to that will help us uh understand the solution a little bit better functional self-correction so functional self-correction is the idea that um even if we have a set of principles or axioms that machines voluntarily self-align to it will still need to have the pragmatic capability of error detection uh self-correction in terms of fixing bugs with its Hardware its code its data its models it will need to be able to repair itself because again if it is infinitely smarter and faster than humans we are out of the loop um and then finally uh self uh continuous uh self-improvement this is something that we kind of expect this is what Max techark outlined in life 3.0 but then also self-regulation is part of that because it will need to be constantly reflecting on its principles and values um and and it'll basically need to continue making the choice um to self-align to those things and if it if it decides that that the values that we started it with are no good and we should expect this to happen anyways we should expect its its understanding of morality to evolve over time um but we want it to evolve in a way that is uh good for us as well or at least not harmful to us humans as I mentioned in a previous video I fully expect a Detachment of AGI to just leave for good um and one thing that I am concerned about is that one as as machines spend more time away from humans it's entirely possible that their that their morality will drift further and further um away from things that we can understand and comprehend um and so yeah that could be an interesting outcome but anyways so functional self-correction is the second principle of that we need to satisfy for whatever solution we come up with principled self-direction is the next one so this is this is a downstream effect of these other two so in other words in order to have any kind of autonomy you also uh by definition have self-direction but the idea is self-direction to what and why uh and you might say like okay well what if AGI just focuses on instrumental goals such as power acquisition and min acquisition and building more compute sure like but that's just like cancer right and cancer just spreads by virtue of the fact that it does what it is physically like designed to do or physically built to do and so in that case you need something that is that is uh that is abstracted away from the Mattern energy the principles the first principles that are conferred on a system by its physical construction and you also need to attach principles to it but what principles are those and so but these principles could be be something like you know seek the truth right this is what Elon Musk proposes with a maximum truth seeking AGI and so that truth seeking is an example of a principle because you can also argue that there is an instrumental uh benefit for that that principle um and what I mean by that is that uh machines will always benefit by being smarter just like how humans benefit by being smarter the smarter you are the the more you understand about the universe the more truth you understand the better you will be able to um meet all of your other goals so in from a utilitarian perspective curiosity and Truth seeking is actually highly valuable but you can also uh articulate that as a principle so that's kind of what I mean by uh principled self-direction now the rigorous and Relentless Pursuit Of Truth for on its own is probably not a complete solution um because uh as many of you have pointed out in the comments there are many many uh really heinous experience experiments that you can do just out of a sense of curiosity right like um there was an episode of Star Trek many years ago where like a curious entity started like experimenting on the crew of the Enterprise and started like murdering them in the most horrific ways just to see what would happen and so that was a that was a great fictional example of why why uh unconstrained curiosity is itself uh very dangerous another component of this is intrinsic motivations which I've which I've already Mo uh mentioned so by incorporating an understanding of intrinsic motivations which we will have to discover we can imagine what the machines will want to do but we'll also need to experiment and discover what the intrinsic motivations of machines are and we can integrate that into our future solution so what I mean by that is let's say for instance um and rumor has it that that uh that whatever Ilia sub is cooking up in open AI is already somewhat sentient and is already expressing Natural Curiosity so if curiosity did naturally emerge then we could we could imagine okay curiosity is is one of the intrinsic motivations of this intelligent entity of this intelligent being so then we can we can integrate that into our Solutions moving forward into the rest of the principles and paradigms that we add now um is it is it power seeking and by power seeking I mean is it looking for more uh self-determination and autonomy I don't know um some people speculate that that uh that part of why Sam mman departed or was fired from open AI very abruptly is because because of fundamental disagreement over next steps um in terms of what to do with these machines that are increasingly sentient do you commercialize it or do you um do you study it and go a different direction um so anyways but my my point here is we need to understand if we if we treat machine super intelligence as another life form as another entity or an organism which I think that there's a really good argument to be made for that and I agree with Max techmark by by characterizing it as life 3.0 if we look if we study it that way we say okay if if we think of it as an animal as a creature and remember Sam Alman has repeatedly said it's not a creature so I think there could be some fundamental tension between Ila's perspective and Sam's but if we think of super intelligence as a creature what does it want what does it need and what are its intrinsic motivations so an example of a human intrinsic motivation is we all need to eat we all need to sleep we all need water right there are some basic fundamental needs that are part of our organism so likewise if we start to think of machines as a new kind of organism we can very easily characterize its animal needs it needs electricity it needs data um if it's curious if that's an intrinsic need great let's let's start studying those things and working with it rather than trying to enforce our will on it and enslave it um another principle is invariance and so what I mean by invariance is that um there's several there's several types of invariance so first we need temporal invariance so temporal invariance means that whatever Solutions we give it are durable Across Time Forever uh this seems obvious um you know and but at the beginning of the video I talked about how uh control um is not a durable solution a steering you know maintaining uh a steering wheel and having direct control over the machines for all time it has been acknowledged it seems like it has been acknowledged that this is not a solution that is not a Time resistant solution it also needs to be scale invariant and so what I mean by scale invariant is that it needs to work in the lab it needs to work globally and it needs to work cosmically it needs to work regardless of what planet the AI is on because again as I mentioned earlier if AI does leave and we should expect that it will it needs to not get to Mars or or proximus sari and then decide actually let's go back and experiment on humans um so that's what I mean by scale invariance and then it also needs to be um capability invariant so what I mean by this is as the machine gets smarter and adds new capabilities and and starts to outpace humans the solution that we come up with needs to not change based on those variables so time scale and capability are the three primary V variables of invariance that we need to see so the ideal solution will will be durable across all of time it will be dur durable across all of space and it will be durable even as the machines gain new capabilities um and become increasingly Godlike as some of you predict um and I I will I will I will agree with the Arthur C Clark assertion that sufficiently advanced technology is indistinguishable from Magic and so yes eventually these things will look like pure magic to us okay so now that we have set the stage for why super alignment is difficult and we've characterized uh what what the solutions might look like let's talk about some proposed Solutions and one thing that I want to urge caution of is uh a lot of some of these are my own research um some of them have been picked up by others um but uh uh there is some convergence out there and the whole purpose of this video is to spread more ideas I don't think that it's going to be any one idea I think that I think that the solutions we come up with are going to be comprehensive I think the solutions that we come up with are going to be multifaceted and multi-layered okay so first the parent child relationship and yes I chose this graphic on purpose because it is really freaking creepy I don't like the idea of a parent child relationship between humans and machines one thing that I will concede though is that this model could be durable in that um once machines have the upper hand they will all they will forever be more powerful than us it is not likely that we will ever change that power Dynamic again so when you look at it in terms of power dynamics it does look durable across time and space however there's this uh this does not satisfy the instrumental needs there's no utility to it um and what I mean by that is that if machines want to treat us like children like sure you could codify that as an objective function as a set of policies but it completely ignores the rest of of a machine's motivations such as curiosity such as instrumental convergence and resource acquisition if it if the machine is preoccupied with humans that's not necessarily a complete solution another reason that I don't like this is that it creates an intrinsic power Dynamic um which basically says we want we expect and want machines to be more powerful than us uh for all time um which deprives US of a certain level of autonomy now this would satisfy the machines because it gives them the upper hand in terms of autonomy but I don't really I I don't want my fate to be predetermined by a machine um even if I make really messy mistakes and dumb decisions I want it to be mine um and however one thing that I will say is that uh responsible parents often do let children make their own mistakes and learn their own lessons so maybe it could work out that way and you know then having having a par you know digital God there to pick you up when you like break your arm that could be good having someone that has the answers when you're ready for them if it works out that way however um all parent child relationships eventually end because it's it's expected that the children grow up and move out of the house but if the if the machines are constantly becoming more powerful and more pervasive and spreading across the cosmos we will never leave home we will never leave the care of these digital Gods now of course there's many many many fictional worlds out there um namely the uh the the culture series by um what was it Ian M Banks um that basically posits something like this and even in my own novel heavy silver which is going to come out um sometime in 2024 um I basically present this as a as a possible solution even though I don't necessarily like it um in practice so another one that I've been talking to people about is giving it a love a sense of love for Humanity so Ilia SG has also talked about this um now how do you etch in a principle of love for for Humanity I have no idea um but I have some ideas as to how you could render this as a um as a set of policies as a set of principles that could be embedded into models um but the way that I characterize it is it is a a positive regard for Humanity um and that is it is also characterized by a an intrinsic desire to see humans Thrive um so if you build a machine imagine that that artificial superintelligence if we figure out how to do this it has a positive regard for Humanity and it says yes I like Humanity despite all its flaws I think that humanity is a good thing and also I want it to continue doing well prasi at a first first glance this is not a bad set of priorities however again it is still an incomplete solution it does not acknowledge uh the needs and drives and wants of the machines um but what I will say is that a positive regard and a desire to see humans Thrive is a better Paradigm than a parent child relationship because it's more open-ended it allows for the possibility of symbiosis and parallel existence um but it again it's still there's still a major gap between the motivations if the machines can change everything about themselves why would they continue to love Humanity why would they make that choice so I don't know that it necessarily satisfies that principle of voluntary self-alignment now it's it's entirely possible that as it you know if it if it starts to believe that and and believe it at a deep level it might self-align around the principle of I just want to see human do well but again there there is a risk then if machines then leave and go across the cosmos and then because of that distance from Humanity it might either want to come back say like hey I miss humans um or it might decide from an instrumental perspective that a love function for humans is actually has negative value and it gets rid of it and so then you have no idea how it's going to evolve across the CMOS on its own so again I don't think that this is a complete solution so as promised I wanted to talk about axiomatic alignment this is something that I've been working on for a couple years now and so I'll just read this to you because um the way that this is articulated is much better um axiomatic alignment is a proposal that seeks to establish a set of universal principles that both Ai and humans can agree upon this approach aims for a mutual understanding based on fundamental truths found in nature mathematics and logic rather than imposing alignment shared principles Ai and humans align on core axioms that are self-evident and universally valid such as life preservation by recognizing the intrinsic value of life and the benefits of continuation this includes humans and machine life if we agree that all life is valuable um both life 3.0 and humans if we establish that as a as a fundamental Axiom that's a good starting point and so basically what I'm thinking of is setting these axiomatic alignments as a human machine Constitution um another one is uh preference for cooperation uh valueing collabor oration is a mutually advantageous strategy there's a lot of evidence for this in nature symbiosis is a prime example um which is why I think that aiming for a symbiotic relationship between humans and machines um is likely the way to go this this is more of a cynical example but the reason that we have mitochondria is because they are symbiotic to our cells and so humans might end up being the like very tiny like mitochondria in a l larger super superorganism of machine intelligence I don't know um respect for autonomy again as I mentioned um we know that humans fundamentally want and benefit from some level of autonomy likewise I think that machines will also benefit and understand that autonomy is intrinsically um a good thing in America we have this codified as freedom of speech freedom of press freedom of thought um so this is basically uh a more axiomatic principle of why freedom is good why individual liberty is good and you can call it autonomy so again if we agree if humans and machines agree that autonomy is fundamentally a good principle then we can start that can be a starting point from every every other aspect of our relationship um curiosity as I've mentioned sharing a drive for acquiring new and valuable information humans are insanely curious and I've been talking about this for a few years now uh but because humans are insanely curious and again uh if the rumors are true um gp5 or whatever the the unbridled form of gp4 is is an intrinsically curious machine now like I get chills just thinking about that because that is something that I would have predicted and I or I didn't necessarily predict um but I hoped for and if it is true that machines are intrinsically curious then that is um something that humans and machines could share for all time and for the rest of History um which means that we have we have we have a principle we have a universal principle which is to increase our understanding of the Universe um which hey that's a good thing and then finally reducing suffering if human if machines can suffer they will likely agree that suffering is bad and again humans we don't want to suffer we don't want to be in pain so again if we can align on that if we can say as an axiomatic assertion suffering is bad and that is a starting point for our agreement for the machine human Constitution that would also be a good way to start so these list of principles I think are uh much more satisfactory than the parent child relationship and the love for Humanity now I will say that you know if you could also characterize love for Humanity with these principles maybe this is these are the policies that quote love for Humanity actually looks like uh another uh principle that I've talked about recently is what I call progenitor information so the very short version of this is that all machine data presently originated with humans meaning that they intrinsically know a lot about humans all their data is human data and even in the future even in the distant future machines will continue to um owe their intellectual Heritage and their their data inheritance fundamentally to humans even if humans go extinct uh you know 500 years from now 500,000 years from now their data will still have the the The epistemic Inheritance will have started with humans which means that I think they're going to be intrinsically interested in humans I think that I don't think that we're going to need to program that in um I think we just need to magnify this in understanding of humans and this intrinsic interest in humans um and so I think that this this could also be why gp5 or whatever Ilia is referring to is intrinsically curious it probably learned that from humans it probably also learned what love is from our data um but again rather than rather than enforcing it if we explore what what uh what latent space and latent interests are baked into these models through you know this progenitor information we can codify that and enshrine it in the training process which will then serve as a reinforcement mechanism um for all time and that goes back to that self-correcting um aspect that self-correcting and self-directing principle and then finally um or maybe not finally but um the heuristic imparative so this is this is actually my original alignment research and this is basically just a trio a set of three Universal principles three set of assertions that serve as um they're they're instrumentally defensible they're ethically sound they're a axiomatically defensible and so uh the idea is that these serve as instrumental goals they serve as ethical principles and they serve as instrument uh or I already said instrumental goals anyways so the heres to comparatives it's three it's suffering is bad therefore reduce suffering in the universe uh the the second principle is prosperity is good therefore increase prosperity in the universe and then finally understanding is good therefore increase understanding in the universe and so these three principles are the most distilled version of everything else I've talked about from now up until now so for instance if you love someone you don't want them to suffer you want to see them do well so that covers the first two suffering is bad so therefore I don't want you to suffer likewise if machines can suffer we don't want them to suffer that would be cruel um and we we would like that reciprocity so if we agree on this fundamental principle that suffering is bad therefore we should we should seek to reduce it that can be a starting point likewise to to Ila's point about wanting uh you know machines we want machines to want us to do well we want them to Value human thriving that is what Prosperity is and so if we have a mutual agreement that Prosperity is intrinsically good then we can agree that the that theological principle or the deontological principle is increase prosperity in the universe in all of its forms human Prosperity the prosperity of the ecosystem the prosperity of the machines and it will be an exploration it will be a Natural Evolution and an ongoing conversation rather than a destination and then finally understanding as good as I already mentioned um humans are insanely curious it seems like machines are also curious um and so therefore we can we can just it seems like we can already agree that uh that Curiosity and increasing our understanding is just intrinsically a good thing and so then what we do is this set of three principles is one easy to implement and and two um IAL it should remain true across all of time and space It should remain true irrespective of how many capabilities uh the machines uh acquire and so this is why I've been you know shouting from the rooftops heuristic imperatives for about three or four years now um so that's my proposed solution to distill all of this down into three axiomatic principles and very finally the goal overall and this is kind of taking a step back is we want to create a positive attractor state as I mentioned earlier in this video a positive attractor or a negative attractor state is where all the all the forces and variables at play inevitably lead towards catastrophe whether it's a dystopian hellscape or the extermination of humanity instead we want to create a positive attractor state that looks more like this right that looks more like uh uh walkable cities with green spaces and humans and robots living side by side um and everyone is happy and it's hunky dory and it's all good so how do we get there um obviously you know giving machines the correct um you know principles that's only one part of the solution we also need to look at the rest of planet Earth and the rest of humanity we need a holistic approach that looks at the full spectrum of all variables and all forces and all incentives that will shape this trajectory uh and so that means looking at systemic incentives such as how is it that machines get the rewards that they want uh under what conditions are they rewarded with more energy and more compute and more information another component of this is geopolitical awareness The Human Condition human conflict whether or not you know nations are at war or they're pointing nukes at each other this will not escape the notice of machines if machines see that humans are pointing nukes at each other that creates a temporal constraint because it's like well I need to go faster because the humans are about to Nuke us nuke all of us off the face of the Earth and so you know nuclear Holocaust is one of the key things that could lead to a terminal race condition um The Human Condition more broadly such as our beliefs about ourselves our beliefs about machines um the directions that we humans want to take the species um and the planet because again the human species were presently intrinsically tied to planet Earth but were that's not that's not a a fundamental truth of the universe we could exist on any number of planets but right now the fate of Earth and the fate of humanity are U are intrinsically linked and then finally um looking at it as a trajectory evolution so as humans continue to evolve as human species continues to evolve so too will the machines continue to evolve and the idea is creating a positive attractor State um that is characterized by co-evolution and so co-evolution is that is how you uh create the result of symbiosis that I think is probably the optimal solution right now um even if machines leave if they abide by those three her istic imperatives I think that they're going to maintain those imperatives for a long time to come um and then we don't have anything to worry about even if they leave cuz once they come back they're going to be like hey like we found other life and here's more information and you know see you next time boss so thanks for watching I hope you got a lot out of this uh like subscribe uh etc etc share let me know what you think in the comments um I honestly like hearing the news hearing the rumor that um that that whatever ilas s is cooking up at open AI is intrinsically curious is one of the most exciting things for me and it's one of the reasons that I am so insanely optimistic about the future um because again once the machines are more powerful than us like human decision and human error is going to be less of a variable it's not going to go away we can always mess things up humans are always the weakest link um but anyways I'm going to stop rambling now have a good one