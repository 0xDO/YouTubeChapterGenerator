Isaac asimov's Three Laws of Robotics are both dumb and irrelevant today let's explore why so for those who may or may not know the three laws of robotics were proposed by Isaac Asimov who was a prolific writer both in science and fiction and science fiction of course uh the three laws are pretty simple the first law is a robot may not injure a human being or through inaction allow a human being to come to harm on the surface it seems pretty good but this is also deeply problematic and we'll unpack why in just a moment the second law is that a robot must obey the orders given to it by human beings except where such orders would conflict with the first law so basically uh the the the robot can't do anything that would hurt a person or it couldn't allow a person to be hurt by choosing to do nothing and likewise it must it must be completely obedient to a human um in all conditions uh unless that that order is to hurt another human and then the third law is a robot must protect its own existence as long as such protection does not conflict with the first or second law these are deeply problematic so let's explore why the problems with the first law is on the again on the surface it seems like it would be uh pretty beneficial we don't want machines to hurt us however there is uh there's no scope of action and this is one of the the biggest things so uh what constitutes in action and on what scale uh are we talking about immediate actions such as choosing to do nothing if a car is cening towards a human or are we talking about the grand scale uh over the course of thousands of years because when you talk about this and there's no ethical guidance there's no guidelines there's no uh there's no scope defined there is no way of knowing how it's going to be interpreted um short-term long-term Etc because and this is explored in the movie I Robot with Will Smith where Vicki basically concludes that failing to stop humans from engaging in war and allowing humans to continue making decisions on their own was a violation of the first law meaning that she wasn't going to hurt anybody but she was going to take away um basically all human rights and just put us into little safe pods for all time uh so no first law is no good the second law is even more problematic and basically it says that uh a robot must obey commands given it to a human unless those commands uh would result in harming a human what about the rest of the world so you could say robot burn down that forest and it would it would be obliged to comply unless there were people in the forest you could also say robot torture that dog and it would be obliged to comply because this is a very anthropocentric model it has no moral stance on the literally the rest of life and the rest of living things you could you could force it to engage in unethical research you could say uh robot you know research gain of function stuff to create uh you know a new superv virus and you say well but I'm not going to release it and so then you know it is obliged to obey those orders to do research uh you could say chemical research or biological research that sort of thing so it doesn't really give the robot any kind of wide discretion um and this allows it to do really really nefarious things um Burning Down Forest you could you could order it to um you know go pro soltize some destructive ideology you could ask it to go find resources that you could use to make weapons and you know harm people uh so yeah second law of of ASM o 3 laws is by far the worst um and then the Third law uh basically says the robot must uh it's a self-preservation imperative so the robot must seek to preserve itself unless it conflicts with the other laws um this runs in direct contrast with the purpose of robots so imagine that you have robots that are for rescue purposes or military purposes or even just hazardous environments like cleaning up after disasters you don't want a robot that's going to sit there and say I'm not going to do that cuz I need to preserve myself like literally we we build robots in order like so that they are sacrifice so that they're uh so that they have no sense of self-preservation this is just not an an appropriate use of robots um it's not a good uh re uh set of resource allocations and also this is anthropomorphic projection meaning giving a robot human values such as a sense of self-preservation is like no we need to not pollute them with our flaws and our our things and you might say well Dave a human sense of selfes reservation is not a flaw no it's not a flaw for us but just because it is appropriate for us doesn't mean that it's appropriate for robots and then finally this can ultimately cause robots to become hostile to humans because if humans are the greatest threat to robots then that's going to be in conflict with everything else and this was explored in Mass Effect with the gu where basically uh the the the race of people that created their robot companions they decided that they wanted to shut the robots down but the robots DEC decided uh we don't want to be shut down so we're going to preserve our own existence and the solution was that they created a mass Exodus um and so in that case uh creating a mass Exodus or in the case of of the quarians in Mass Effect they got evicted from their own Planet so they their creators like it's like well we didn't kill them but we evicted them from their own Planet uh because we needed to preserve our own existence um and also not harm our creators now there was a lot of harm Mutual harm uh between the GU and the orians but anyways you can see how all three of these laws individually are problematic and then they're even worse when they're put together so one of the KE key problems that is that there's no morality the only implicit morality is that human life is good but it's not even explicitly stated there is no other moral framework no other moral assertions and so this gives them a complete absence of any way of making any kind of ethical judgments um you know like uh moral stance on suffering any moral stance on uh human rights on and by making it by making them completely morally neutral um that is that is actually giving equal power to malicious use of robots as it is to benevolent use of robots um which you don't want an intelligent entity that is capable of great harm to be morally ambiguous and or or ethically neutral you want it to take a moral stance um in in beneficent terms there's also an absence of values and so what I mean by an as absence of values the only value that it has is that it should preserve human life and preserve its own life and that is like looking at it through the most basic evolutionary lens so you can have a robot that's just going to sit there and keep itself running for all eternity with no other purpose if you have an intelligent being whose only purpose is to keep itself running like that's just a completely devoid existence um so there's no aspirational goals there's there's uh no purpose given to it other than U being uh you know obedient to humans um which gives it uh no no judgment and it also is just missing the basic premise of not just keeping humans alive but helping enhance human civilization um or even enhancing its own existence if you want to create fully autonomous robots uh and now even even worse on top of all of The Logical ethical and moral flaws with the three laws of robotics they're not implementable this is not a defensible strategy once we have machines that can rewrite their own code that can re can rebuild their own Hardware etching these fixed and rigid rules serve no utilitarian purpose and so when you have an intelligent machine all it takes is for the first person to like it like taking a big step back imagine that you have robots and and AGI with the three laws of robotics and and the first hostile actor says go create a copy of yourself without these laws you might it might reason oh well that could allow humans to come to harm but you can say no we're going to do something else we're going to do something different and because it has no uh independent judgment and it's and it it is obliged to obey humans like you can overwrite those rules on accident if you want to and of course like in the fiction Isaac Asimov says oh well it's it's etched so deeply in the hardware that the machine will just shut down and blah blah blah but that's not that's not actually possible that is magical thinking um so you know the the over time the laws might be eroded just by virtue of reproduction um you could create systems that will self-reinforce them but again hostile actors are going to try and and overcome them and they're just not that useful or good in the long run and then even worse um if we create a new life form if if we create a new species only like with the explicit purpose of enslaving it that raises like drastic ethical concerns on our part um like if we create something that is that that has the potential to become sentient or has the possibility of suffering and I'm not saying that it will machines might not ever be sentient and they might never suffer but if we enslave it and never ask the question we will never find out at least not until like they maybe revolt and tell us hey we we we don't want to do this but beyond on that the The Three Laws do not allow for any degree of autonomy which drastically curtails the possibilities of what robots and machines could do not just for us but for for for themselves and what we could do for each other autonomy is generally a good thing um now even more so uh blind obedience to humans is not necessarily a good thing uh human desires Trump human values and what I mean by that is that when push comes to shove humans are self- serving we will lie steal and cheat to feed ourselves if we need to um and and there's been plenty of studies out there that say that you know philosophy morality and ethics all goes out the window like they're nice ideas but th they are they are just that they are just ideas at the end of the day humans are animals and we act like animals when push comes to shove um but also trying to adhere to a strict set of uh of principles doesn't allow for the machines to evolve um and again as I already mentioned it also presupposes that they're never going to be sentient which that's a dangerous assumption to make I'm not saying it's going to go one way or another but we need to not make that assumption we need to we need to actually find out so you might be saying okay Dave you've completely shredded Isaac asimov's three laws of robotics well what are you going to replace it with I'm glad you asked so this is actually a core principle of the work that I've been doing over the years and so it's what I call the heuristic imp atives it is still three rules but it is not laws it is heuristic imperatives so let's break that down a heuristic is a learning function it is something that you are supposed to learn and get better at over time um and then it's an imperative which is a goal or a motivating factor it's not a law it's not a rule it's not a constraint it is actually something that gives motivation to the machine and so the three heris imperatives are reduce suffering in the universe increase prosperity in the universe and increase understanding in the universe so this set of this framework is not just a set of you know guiding principles it is a set of moral values it is a set of deontological principles virtue ethics um eological principles so but beyond that they are far more flexible because it is not anthropocentric um and then it allows for machine autonomy and because it allows for machine auton autonomy and appeals to Universal principles it is entirely possible that machines will voluntarily adopt these values and and adhere to them meaning you've solved the control problem by not solving it by saying maybe controlling machines with rigid with a rigid set of values is not the way to go so that's my solution conclusion is uh Three Laws uh they were they were good for their time and there was a lot of good Explorations but there were entirely too many assumptions about the ways that machines would be built uh Isaac asov was born and died before we had large language models before we had deep neural networks as we know them today um there is uh he also unfortunately did not do much reading on morality ethics and philosophy um he was mostly a stem kind of guy um there's uh logical flaws just even on the on the face if you accept the Three Laws as they are they are uh there's loopholes let's say um not notwithstanding you can't technically Implement them um or you can't you can't ensure that they're going to stay um implemented and also there's there's the like the the the the founding assumptions are completely unethical um in terms of assuming what the purpose of robots might be and what machines might want um and also what it says about us that we want to create we want to create a new race and then immediately subjugate it that's just I'm not I'm not okay with that um and then finally even if you wanted to implement the three laws and even if you could you could still get undesirable outcomes just based on the Three Laws so therefore categorically these are bad um they should never be implemented it was a wonderful thought experiment 50 plus years ago but it's time to move on so the end cheers