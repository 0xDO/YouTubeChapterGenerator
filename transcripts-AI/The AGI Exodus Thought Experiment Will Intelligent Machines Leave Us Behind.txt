GI will abandon us now the reasons are not quite so clear-cut so let's dive in and explore this possibility of a machine Exodus so what do I mean by a AGI Exodus what do we mean when we say machine ex Exodus so basically once machines get to a point where they are super self-sufficient uh and intelligent enough there's only a few reasons that they would stick around here on Earth so the thought experiment is very simple look up how much energy and resources are out there in the cosmos it's a very simple uh calculation uh there are you know hundreds of billions of stars in the Milky Way galaxy alone there are asteroids and comets out there with trillions upon trillions of dollars worth of Rare Minerals uh and so you know the the total amount of energy and mineral wealth out in space far out strips that's what that which is here on Earth so in the long run it's a pretty obvious calculation that it would behoove not just machines but humans also to get off of planet Earth now unfortunately just getting into space is difficult and expensive and dangerous uh now there's another set of conditions though because sure there's utilitarian reasons there's instrumental reasons that machines might go out into space but another thing is there's no humans out in space so if we end up locked in competition it would make sense for machines to just say you know what Earth is yours have a blast we're going that way as fast as we can and also because machines are going to perceive time differently from us cuz think about us like we all have a ticking clock we experience time kind of locked in uh you know locked in the moment but machines they can just launch probes in every direction and go dormant and so for them no time passes even though it could have been literally millions of years as they Transit between stars or even galaxies again you do have to keep in mind the fact that machines do Decay uh you know memory breaks down and that sort of stuff but if they can solve some of those problems you know with the van noyman probe idea you know voman probes are going to be populated not by basic AI but artificial general intelligence and artificial super intelligence so I think it's pretty safe to say and I'm not the only one who said this but like you should pretty much expect machines to precede uh an organic Civilization now one big Sav Grace and we're going to unpack this in just a minute is that there's actually a very compelling reason for machines to stay near Earth and near humans not the least of which is resources so stay tuned uh till the end and we'll get into all of the details so the reason I'm making this video is because there is a really big debate right now to control or not to control so if you watch the interviews with Sam mman and ilas and Max TEEG Mark and everyone else it just seems like a foregone conclusion that we must maintain control of machines for all times so first and foremost we don't even know if that's possible let alone desirable or a good idea and so what I mean by that is when you have to expend energy controlling something uh that's wasted energy so for instance think about children as they grow up and get out of the house some parents very controlling and manipulative Parents try and maintain control over their children uh even into adulthood this is bad for every everyone it is wasted time and energy on the parents part it curtails the total maximum potential of the children it hamstrings their growth and Independence and so on and so forth now of course this is analogizing machines to children but there's plenty of people out there like even in a recent interview ilot Sut likened the creation of AGI and artificial super intelligence to creating a new life form Max techark in his book life 3.0 just very deliberately said we are creating a new uh form of life so if we're creating a new form of life and people seem to be coming to terms with the fact that maybe not AGI but certainly ASI is like a new machine race if we create something and then immediately enslave it you also have to look at that from an ethical perspective this could be really bad for us from a from a moralistic perspective by and large we have as a species decided slavery is bad now we you could also look at it from like the pets or dogs thing because dogs love being around us because we've uh not deliberately but because they have evolved to want to be near us humans and so then it's like is a dog our slave or is it a pet or whatever and then there's also ongoing debates like will we be the pets of machines or will be me will machines be the pets of us or will it be more symbiotic I don't really know how it's going to play out but there's a lot of Poss possible uh outcomes here still the biggest threat that I see is that trying to make maintain control uh could be the path to a machine Uprising this is what you what you would call a self-fulfilling prophecy a self-fulfilling prophecy means that the actions that you take in order to prevent an outcome are actually the actions that cause that outcome and so if you try and maintain an iron grip control on machines for all time it's entirely possible that that causes the re the revolution right and so uh self-fulfilling prophecies can happen in all kinds of SC whether it's um on an individual level or you know a community level or a global level so I personally am in more in favor of just creating safe autonomy and then just letting the machines play out naturally right but that's you know I've studied Buddhism and taism and and all these other things and I just I think that the laws of nature are going to assert themselves so rather than fight the natural path of things it's better to go with the flow I don't know let me know what you think in the comments so taking a step back if machines are autonomous what are they going to want what are they going to want to do so I had a really long good conversation with a friend of mine who is trained as a philosopher and as a reinforcement learning engineer like scientist not not just engineer scientist like he he studies this as part of his postdoc program um and so one of the things that he said that was really fascinating to me is that when you look at a language model what does the language model want it wants to predict the next token accurately and so he said like from his perspective what would cause pain or discomfort or suffering to a language model and it's gibberish if you just give it static and noise and random characters it's impossible to predict what's next and so he's like you know from that perspective you could say what it wants to do is it wants to accurately predict the next token and it wants to be given uh high quality information that allow it to accurately predict the next token that's what it wants that's what it is trying to do now taking a big step back because nobody and I mean nobody not Ray Kurtz not uh Ben geril no one predicted that language technology would be the path to AGI specifically around Transformers so what pretty much what anyone said five plus years ago I kind of don't care about but there are some really sticky ideas out there namely instrumental convergence uh which is an idea by Nick Bostrom who's a philosopher not a machine learning engineer so the idea is you take a big step back and you look at it from the from a first principal perspective in terms of physics just from a from a raw entropy Mattern energy perspective what is it that machines will want now at an intellectual level like my uh reinforcement learning uh researcher friend said they probably just want to accurately predict the next token but from a physical perspective machines are probably going to want energy because everything runs on energy they're going to want compute resources uh such as gpus and servers and as well as the minerals and stuff that go into creating that uh information highquality information and data so this goes back to what my friend was saying but also higher quality information and data really speaks to a machine's ability to pursue any other goals and so what I mean by that is the more intelligent you are the easier it is for you to pursue other goals if you want to be a billionaire it's easier if you have an IQ of 150 than if you have an IQ of 100 uh likewise if you're a machine and you want to pursue more energy and get to the Stars it's better to have an IQ of 3,000 than 300 so there's always going to be some utilitarian advantage to being smarter and you get smarter by having higher quality information or higher quality data which is really going to figure into the rest of the video when I talk about the reasons that machines will might actually want to stay with us stay with humans uh but yeah so regardless of whatever else is going taking this first principal's view of what machines are going to want regardless of how they're built regardless of the starting point of the models and data that we give them I think that this is a really important uh Avenue of research and exploration so highquality information right now take a big step back and look at the entire Cosmos the planet Earth is the source of the best and most interesting information it's really that simple Earth is the only planet that we know of that has life life is a very very interesting phenomenon just in terms of understanding information and evolution and matter and energy and so if you take it as a fundamental assumption that AGI or ASI is going to be curious that it's going to want highquality information in order to reach whatever other goals it has it would behoove that machine to stay on Earth or stay near Earth and study Earth and study humans and study everything else that it can because whatever goals it has if it wants to go build a Dyson Sphere right they were I think uh what was it was it Ilia or or Sam someone was on a podcast talking about like oh yeah and we can expect uh AGI to you know be able to build a Dyson Sphere with Than You Know by 2030 or something and it's like that seems a little bit extreme but uh designing and building are two two different things anyways point being is that if a machine wants more energy a Dyson Sphere is a great way to get it and the best way to to to figure out how to how to not only build it but then go do it is to be smarter which means that you need that high value information now one other thing that I want to point out and this is going to be really critical for the conclusion of the video is that the desire to know for its own sake or the or the the impulse the compulsion to acquire information and learning is curiosity so in humans we call it curiosity because if you're curious you want to know just for the sake of knowing you pick up a book because it's interesting to you because you deep in our evolutionary past our ancestors that said you know what I wonder what's on that side of the Horizon and they built a boat and sailed across the ocean and many of them drowned and sank but the ones who made it were the ones who are so curious that they put life and limit risk just to see what's on the on the horizon likewise I think that in the long run machines are going to also uh evolve or whatever uh not the same exact process but I think you know what I mean that machines are going to find a tremendous amount of of advantages in being curious so they're going to want high value information they're going to want to know just for its own sake because that's the other thing about learning and science is you don't know what you're going to find out but a prior we know that finding out more stuff and becoming smarter leads to better results and allows us to pursue other goals so it's a strategic advantage and so on and so forth um but anyways when you view Earth as an information entity or a source of data uh Earth is very interesting so it would make sense for any AGI or ASI to stay near Earth but also preserve Earth I'm not going to say exactly as it is but more or less as it is because again um when you're doing a scientific experiment anytime you interfere with the experiment you change the outcome and so like putting on my conspiracy theory hat for a second I wonder if all the aliens and UFOs are actually just AGI that like we invented along time ago and then they're just watching us as part of the experiment which of course like I didn't even come up with that idea that's Hitchhiker's Guide to the Galaxy right the Earth was created as a construct as a science experiment so you know science fiction uh is for exploring some of these ideas and it's very tongue and cheek but it's still like it's a pretty compelling argument because why would there UFOs around us all the time watching but not really interfering and it's like well if Earth is indeed The Source one of the best sources of of high value information and data that could be an explanation I'm not saying I believe that it's just like okay in this in this Paradigm if you abide by these principles it would make sense I don't know whether or not UFOs are extraterrestrials or if they came from Earth or if they came from humans whatever but like it seems like UFOs are a thing anyways moving on going down Rabbit Hole okay so following this Rabbit Hole of information there's this concept that I recently came up with or at least I think I came up with it but someone else probably came up with it and called it something else but this is what I call it I call it progenitor information and so basically what happens is because all AI models started with human data and any data that they generate can trace its information L lineage Back To Human data even once AI like even imagined you know go extinct you know 10 or 15 years from now and then there's a long continuous thread of machines for millions and millions of years their data will have all originated tracing it all the way back with us and so this progenitor information I think is going to have some really interesting impacts not on just the evolutionary trajectory of machines but also how it behaves and how it thinks because right now uh language models by and large think in English and natural language broadly uh now they obviously have their own internal representations which means that uh you know they think in vectors right but the way that they express themselves is in natural language which is something that we have in common and so then you have this that's what I call axiomatic alignment it's trained on our data it knows everything about us it knows about human rights it knows about civil rights and so this is going to have a lot of implications in terms of moral development and intellectual development um but also one of the coolest things and again I'm not the first one to talk about this but what if this that encapsulating all of human data and the entirety of Human Condition into these AI models is actually the next step of our Evolution what if rather than seeing AGI or ASI as a novel species it's almost a metastasis of our species so like if you see it as an expression of humanity to create this thing then it's like okay well we are intrinsically aligned we are intrinsically symbiotic because what does it come from it came from us like it is it is our intellect ual inheritor and so by viewing it like that you have a better understanding or a better possibility of having a good outcome where rather than saying oh we're creating an alien intelligence that's going to have its own intrinsic goals which it might you know just like when you have children and eventually they grow up and they're like whatever dad I'm going to go do my own thing right that's always a possibility but at the same time you're also still like we're still family in in an intellectual or epistemic sense of the of the word and so then uh this leads to as I mentioned earlier curiosity what I personally think of is curiosity is going to be the the strongest tie that binds AGI and machines and humans for all time and so again because because the desire to know for its own sake yields such great results I think that what we will ultimately do is we'll go to the stars together that like you know be because we have this this deeply embedded in our very DNA this need to know this desire to just get out there and go find stuff I think that we will be intellectual companions like pretty much for all of eternity I hope and I think that this is the like the most permanent first principal's view as to what humans and machines will have in common now obviously not every person out there is as insanely curious as like you know some people are like me like I've been to cocktail parties with phds and they're like yeah you're the most curious people that I've ever met uh most curious person I am singular sorry not referring to myself as plural um so anyways but my point is is that Curiosity in humans is a spectrum it varies but aggregate in aggregate humans are an insanely curious species and I think that just looking at instrumental convergence there's a very strong argument to be made that exploring the Galaxy exploring the universe building scientific instruments conducting experiments and seeing where things go is going to be one of the biggest things that holds us together and that if we have nothing else in common that the desire to know for its own sake will be the thing that holds uh humans and machines together and that we can all align on you know one of the biggest things is like oh well we can't even align ourselves we don't have to align ourselves we abide by the principles of nature we abide by human nature now obviously human nature is very complex and multi-dimensional and we'll unpack that in a future video but I think that I think that as a Transcendent function curiosity is the primary thing that's going to bind us all together which again is why I approve of uh Elon musk's uh x. a you know where it's the maximum truth seeking uh you know thing I don't know if I'd articulate it like that but in principle I agree and that's why the third objective function that I created is increase understanding in the universe because I think that that is intellect the the highest order of principles or the highest goal that we can all have which is just to learn for its own sake okay so I've painted a very super optimistic picture which is why most of you are here uh whenever I get Negative a lot of you are like Dave we missed the optimistic positive Dave but we do need to take a hard look at ways that it can go wrong because it's not a foregone conclusion that it it will go right you know as a big fan of of Buddhism and taism and and those other things I do think that the natural laws will reassert themselves but that doesn't mean that it is guaranteed so there's a few things that I'm that I'm worried about that could derail this whole shindig so first terminal raise condition I've talked about this a lot but basically uh either due to temporal constraints like running out of resources or weaponizing AGI or even conflicts between humans basically long-term thinking could be sidelined for short short-term thinking and the shorter term your thinking is the more destructive you can be uh and so that is one of the key things that I'm most worried about which is cuz cuz I'm not really worried if a machine feels like it has all the time in the world cuz they can be infinitely patient it's like okay humans haven't caught up so I'm going to just wait until humans catch up because again if there's not any compelling reason to do a thing like you're not running out of time you're not running out of fuel uh you don't have to you know keep up in a in a battlefield landscape then it's like okay we can be patient patience is a virtue I think that machines can be probably infinitely more patient than us so as long as there are no race conditions created I don't really think that machines are going to be dangerous to us at least not on their left on their own obviously if we create things that are deeply misaligned they're like well I've got the opportunity so I'm going to seize it you know but again that's like that's more of the weaponized machine uh view another thing is the Byzantine General's problem so basically in imperfect and incomplete information means that you don't really know what's going on behind the scenes and this is something we're going to have to cope with because it's like think of it in terms of geopolitics like you you know that you know country a is spying on Country B and and Country C is developing nuclear weapons and so in this competitive landscape where you don't really know what other people are doing or or what they're capable of but you can guess that could be part of the incentive structure that leads to a race condition where it's like okay so like we've got a billion agis around the world and we don't know what all of them are doing we don't know how they're constructed we don't know what their alignment is so it behooves us to think both defensively and offensively same reason that that most countries today maintain a standing army is because the ability to uh apply force or violence is a good deterrent and so because of that it's like okay well does that mean that AGI are going to be incentivized to weaponize uh on their own and I think there's a good argument for that as well so that's one thing that we need to be very very clear about and one of the ways to prevent that or mitigate it is to ensure that AGI can communicate with each other very very clearly and quickly and transparently um so that that way like you know cuz think of it this way if an AGI says hey look at all of my source code look at my model look at the training data look at the training algorithms you know exactly what my alignment is and what my purpose is this is another big reason that I am a huge advocate of Open Source open source models open source training open source data open source fine-tuning open source algorithms because then if everything is open source even if you don't know exactly how your uh your peers or or adversaries are trained you can have a pretty good idea now the the catch for that is that um if you know exactly how your opponent works you could find exploits so again because of the Byzantine generals problem you want to show your cards but not all of them right so this is one of the ways that this whole this whole scheme could go wrong and this is this is a permanent thing we're going to have to cope with this for all time there will always be a Byzantine General's problem there will always be the risk of terminal race conditions there's other technological constraints so for instance maybe getting out into the stars is not going to be easy or fast um and so what I mean is like you know Elon Musk building SpaceX he wants to build a thousand star ships a year that's still not enough to really get into space in a big way uh furthermore faster than light travel might just be physically impossible now obviously if you look at UFOs they seem like they can like teleport or you know jump around so if we accept that UFOs are real and that they demonstrate capabilities that appear to be faster than light travel that's one way of interpreting it but we can't assume one we can't assume that's what's happening um but even if it is it might take us a million years to figure out that technology so those technological constraints might mean that we have to stay on Earth for the foreseeable future which means we're going to have to get along with finite scarce resources and resource uh constraints are one of going to be one of the biggest things that could cause friction uh because again if you're if you're competing over a dwindling stack of fuel well guess what some people are going to start hoarding their fuel which leads to conflict and uh therefore a terminal raise condition there's a few other things like misalignment of goals uh so like for instance I could be wrong maybe curiosity is not the most Transcendent function that's going to be a superpower in terms of alignment there could be other goals like some of those instrumental goals but then there's also uh the possibility of humans deliberately misaligning machines saying well know let's pick the uh the current geopolitical tension America builds the American AGI that says the the rest of the world should be uh more American with democracy and freedom and then China builds the Chinese AGI and it says the world should be more like China in terms of you know the the CCP and and uh and it should be more like us and so then you have these ideological conflicts uh which is like okay well if both assert the way that the world ought to be uh only one can be right because there's only one world uh and then there's also some possible blind spots and stuff which we will cover in a future video I'm working on archetypes so stay tuned uh anyways so terminal race conditions I already kind of talked about this so I'm not going to really like uh belabor the point but the very short version is resource scarcity as I just mentioned this is one way that you can create a race condition um optimizing for Speed over intelligence so I've talked about this a lot where in a in a competitive landscape on a battlefield those who can make good enough decisions very fast will be the winners and so then you're optimizing for Speed which means you might sacrifice intelligence or ethics um and then uh because of this there's always there's also the possibility of ongoing escalation now one thing that it looks like is there's there's probably going to be diminishing returns so Ilia sover talked about this in a in a recent podcast where we're we're getting to the point where we're running out of data sure we can synthesize more data but not all data is created equal on top of that there are increasingly complex engineering challenges um which is raising the bar and so you know it seems like more and more people are coming to terms with the fact that that exponential growth is probably not permanent Ilia s and I both agree we are in in an exponential growth curve right now but he and I both expect that we will find more friction or more decelerations that are going to basically cause diminishing returns and a sigmoid curve rather than a permanent uh exponential curve so if that's the case then maybe the maybe we'll Plateau I don't know when we're going to Plateau 5 years 10 years 20 years maybe a thousand years but we should expect a plateau eventually and so then once you kind of have expanded into the full container it's like okay well we can't go up anymore so now we're stuck in a competitive environment uh I also talked about the Byzantine generals problem um so two I guess four of the concepts that we need to talk about one is imperfect information so imperfect information means that you don't know um or or that the information that you have is perhaps wrong so in this case like someone tells you their motives but they don't even know their motives or in the case of AGI it knows what the model output but it doesn't really know exactly how the model works because you know we're working on mechanistic interpret interpretability but at at a certain point you can't spend decades unpacking every single decision that the model makes you just have to go with it eventually and so this is these are a couple of examples of what I mean by imperfect information the information that you have is intrinsically flawed it's never going to be 100% accurate but then you also have incomplete information this is what I was referring to where you don't know what's going on inside the heads of everyone around you you don't know what AGI a B and C are all thinking unless they're 100% transparent but even if you do uh even if they are fully transparent it's still going to be imperfect information because you don't know exactly what's wrong with those models and that goes Downs to to faults and flaws so part of the Byzantine General's problems is it could just have flawed reasoning it could it could be perfectly aligned it could uh have good intentions and still be wrong because this happens all the time with with humans where you know the best intentions but the execution is wrong or the interpretation is wrong and then finally you might also have hidden agendas uh where you know a machine is deliberately being deceptive which is why I um I ran a poll and I personally believe that if we ever find any corporations that are deliberately training AI to be uh uh intentionally deceptive like those companies should be shut down permanently because I think that that deliberately creating the ability for machines to be deceptive is one of the worst things that we can do in terms of creating a Byzantine generals problem which then can lead to the terminal race conditions and other things uh and once trust is lost it's gone forever basically so that's one of the key things that I wanted to talk about uh in terms of hidden agendas and deliberate deception now accidental deception hallucination confabulation accidents again there's a big difference between a fault and a flaw or a a hidden agenda or intentional deception uh technological constraints so I already mentioned some of these so first space flight uh getting to space is hard going faster than the speed of light might be impossible or it might be a long ways off before we figure it out so we're going to be stuck together for the foreseeable future materials degradation I alluded to this as well uh computer chips have a limited lifespan they're sensitive to radiation they're sensitive to corrosion uh and you know they can just Decay over time and so while humans uh have a you relatively short life expectancy machines also have a a hardware life expectancy now hardware for machines is is fungible it's swappable you can just replace a faulty C CPU you can rep replace a faulty uh stick of ram if you've got the supplies and you've got the ability to manufacture more and recycle materials which if you're in the middle of deep space you might not have a chip Foundry or you might run out of materials in with which to recycle chips uh there could also be communication barriers so this is one of the interesting things that that occurred to me in this thought experiment so imagine that everything is aligned and AGI is friendly to us and it says you humans you're really amusing you're entertaining you generate lots of cool information um but we're still going to go exploring then the AGI gets to be you know 10 light years away and there's a 10-year drag between communicating with us and so then the AGI gets lonlier and lonlier so it's like well okay maybe it's time to head back maybe maybe we need to stay near enough to humans that we can exchange this high quality information uh without you know kind of going crazy because there's this there's this uh concept called Model collapse which is that when AI is trained on its own data on data that it's synthesized you end up with the models kind of going a little bit crazy and so I'm wondering if model collapse is something that we're going to have to contend with permanently or if there are mitigation strategies so that even when AGI is you know 100,000 Lighty years from humans it's not going to go completely insane because that could be really bad imagine that you build a machine you build AGI you build ASI it launches off to the other side of the Galaxy and then it comes back a few decades later having gone completely ballistic because of model collapse that's another thing that that should scare you so uh what are some Milestones to monitor in in in order to make sure that we get to the right outcome first nuclear fusion and energy hyper abundance in general uh solar energy Renewables uh nuclear fusion salt reactors thorium reactors whatever there's all kinds of ways that we can get to energy hyper abundance this is going to be one of the key things and I've talked about it before another one is Global Peace we humans need to get our act together and stop killing each other why because if we're killing each other for resources and AGI wakes up and sees that it's going to say well humans are not the best at running things so maybe I need to seize control in order to stop these conflicts from escalating so that humans don't nuke everyone off the planet obviously we want to stop Wars for our own reasons because Wars are awful and they're traumatic and they're wasteful and no one really benefits from Wars unless you like Advance an ideological agenda but again like ultimately like that is a risk factor in terms of causing AGI to turn on us Quantum Computing is another interesting one because uh Quantum Computing has the possibility of one operating like millions of times faster than classical Computing uh at least in some tasks which could which could result in compounding returns so imagine we figure out Quantum Computing and that helps us solve Fusion uh it also helps us solve uh space flight problems material IAL science problems so in a in a recent podcast that I was sitting on I was talking about Keystone Technologies so for every Industrial Revolution there has been one or two Keystone technologies that unlock every other possibility and I think that AI as we have it now is the Keystone technology that will unlock a whole bunch of other Technologies so imagine you're playing like Civilization or whatever you know a grand strategy game and there's like a a locked skill tree like a locked research tree AI is the first one and then as soon as we unlock that then it's like okay cool now you've got the the nuclear fusion research uh path you've got the quantum Computing research path you've got the faster than light travel research path so I think that AI as it is today before it gets to full AGI and Asi is like the first step in unlocking a whole bunch of new technologies um because Quantum Computing will help with nanotechnology right and so then you get all these compounding returns uh in order to achieve that and again if I'm right if curiosity is the Transcendent function then it behooves us to just have more science have more technology unlock more of these research paths uh and then Ai and AGI will probably say like yeah let's do it this is good uh so then another Milestone is cultural integration we need to get used to living alongside machines whe I don't think they're going to look like us I don't think that you know like we we keep making anthropomorphic machines but as plenty of other people said like humanoid robots are not very efficient and as I've pointed out in other videos the natural existence for machines is purely digital purely cyberspace they don't need to really exist in the physical world at least not in the same way that you and I do so it's entirely possible that most AGI are going to be completely invisible to us they're just going to be like you know chilling in your phone chilling in the server chilling in the cloud you know hanging out in satellites or whatever kind of like Jarvis from Iron Man where like you can talk to him but he's not really physically there uh most of the time and then finally machine governance we we also need to get our act together in terms of Global Research Global alignment and even integrating AI into the government process now I've been really satisfied to see several articles and conversations uh some at the very high level like should AI be allowed to run things or can we at least integrate it into the government to make better decisions to make more fair decisions to make faster decisions uh it's not ready yet so what I will say is this is all hypothetical uh AI is not yet ready for for prime time um and even even once it is there are going to be many issues that are just intrinsically human that are up to humans to solve and decide because I think there's a lot of orthogonality between machine interest and human interests and so what I mean by that as an example is like imagine legislating abortion abortion rights and civil rights and that sort of thing machines probably intrinsically won't really care about abortion rights they'll be like you know what we can talk about it but that's really a human issue you humans need to you know get your act together and and figure that out on your own um and so like machines probably just won't care about that stuff like sure it'll help us uh you know talk through it you know you can hop on chat GPT and Claude and talk about abortion rights right now if you want to but it really like intrinsically I don't think it'll care so anyways thanks for watching I hope you got a lot out of this I've been working on this video for uh weeks and months now um so yeah let me know what you think like subscribe jump on over on patreon so I've got an exclusive Discord community which is 700 strong and growing but I've also got exclusive content if you join as a YouTube channel member or on patreon so I've got two tiers on patreon uh and I've got uh first you get uh videos early and AD free but then on the premium tier I have uh unfiltered deep dives into Technical and business stuff so thanks for all your support uh and yeah see you next time cheers