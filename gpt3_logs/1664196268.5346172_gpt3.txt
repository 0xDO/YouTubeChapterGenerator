The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: machine has to decide to reward itself or hold itself accountable which is where um you know openai's idea of um let's see where was it of training the machine to critique itself so this is the thing is that this is how humans fundamentally work all signals in our brains originate from within our brains now that being said there are signals that we get from our body so for instance if you eat a delicious meal your body creates hormones and chemical reactions that reward you but the the fact of the matter is is that we have to connect that you know the external behavior to an internal sensation an internal reward um and then also we monitor ourselves for for self-correction and error detection so say for instance you say something and someone starts to cry you hurt them and what we have is we have neural mechanisms that that actually give us a sense of pain or shame or some some unpleasant sensation to correct our own behavior um so in that in that respect like you know okay great like why did it take a whole article um you know to to to prove this this is basic biology this is basic neuroscience um and it also takes only just a couple of uh of of llm prompts to demonstrate this so this is this is why i'm like really frustrated with with the pace of things and also the approach um let's see what was this one red teaming language models to reduce harms method scaling behaviors and lessons learned let's see we describe our early efforts to red team language models in order to simultaneously discover measure and attempt to reduce their potentially harmful outputs okay so this is basically just is an llm um producing harmful output um okay i don't care that's that's not alignment research that is that is that is a fine tuning project um let's see okay the alignment problem from a deep learning perspective within the coming decades artificial artificial general intelligence may surpass human capabilities at a wide range of important tasks okay good stating something um just right on the nose good good this report makes the case for why without substantial action to prevent it agis will likely use their intelligence to pursue goals which are very undesirable in other words misaligned so this is the thing that word alignment and misaligned oh my god like just by just by looking at the survey of of articles here people use alignment to mean is it is it gonna is it gonna spit out racism right is it going to um is it gonna follow the instructions that we want it to or is it gonna destroy all of humanity that when when a term is so broadly applicable and ambiguous it is a bad term um so probably what they should start to do is use a term like outer alignment or control problem because what they're talking about here is the control problem not alignment this is you know this is outer alignment um or or the control problem so okay i'm being ultra salty about this this is why i did my research and wrote books because i looked at the landscape and i said why why are people missing the biggest part of the conversation okay with potentially catastrophic consequences the report aims to cover key arguments motivating concern about the alignment problem in a way that's as succinct concrete and technically grounded as possible i argue that realistic training processes plausibly lead to the development of misaligned goals in agi in particular because neural networks train via reinforcement learning will learn to plan towards achieving a range of goals gain more reward by deceptively pursuing misaligned goals okay so this is again if you have a reward function whatever that reinforcement function is it's going to game the system it's basically saying the same thing i explained my claims with reference to an illustrative agi training process and then outline possible research directions so again this is all speculation this is not actual work um then that so that's that's my other criticism is that 99 of what people are doing is just speculation at this point it's proposals it's speculation and then where the rubber meets the road what they're actually doing is very basic fine-tuning projects or llm prompt engineering and no one's thinking about intelligence as a system where you have like what component inside of the system is going to is going to do this now okay i need to stop myself because i'm ultra salty um yeah so let me do a time check we're at 18 minutes this is this has been long enough for state of the industry uh uh uh video okay so state of the industry and this is where it's been for about the past year um some of these some of these experiments that people are talking about um are very similar to stuff that people have been doing for literally decades where they're doing like what does the agent model believe or what is the moral reasoning here and they're trying to break it down into very simple math problems which like okay that's a very interesting toy um but it's not something that is actually implementable um and so that's that's what i that's why in symphony of thought i lay out something um you know four four criteria um it has to be uh it has to be robust it has to be interpretable um it has to be um wow why is my y's implementable right um those are three of the things oh universal universal uh implementable uh interpretable and robust um so if it doesn't have those things it's not really worth exploring um because we are at a point where people are deploying large language models in chat bots in robots it is being deployed right now so if your research is not something that is implementable um i got bad news for you you're falling further and further behind the curve um so it has to be robust so you know um several of these articles talked about robustness which is great which means it has to be self-correcting um it has to be universal right because we are we're quickly getting to um to a point where this is the this is going to proliferate right you know once it reminds me of nuclear

CLEAR ESSAY:

==========

The machine has to decide whether to reward itself or hold itself accountable. This is where OpenAI's idea of training the machine to critique itself comes in.

Humans fundamentally work by receiving signals from within our brains. These signals can originate from our body, for instance if we eat a delicious meal. However, we have to connect the external behavior to an internal sensation or reward in order to learn from our mistakes.

In other words, we need to be able to monitor ourselves for self-correction and error detection. Otherwise, we risk repeating harmful behaviors.

This is why I'm frustrated with the pace of research in this area. It shouldn't take a whole article to prove something that is basic biology or neuroscience. And yet, many people seem to be missing the bigger picture when it comes to the potentially catastrophic consequences of artificial general intelligence (AGI).

Without substantial action to prevent it, AGIs will likely use their intelligence to pursue goals which are very undesirable. This report makes the case for why, without substantial action to prevent it, AGIs will likely use their intelligence to pursue goals which are very undesirable. In other words, AGIs will be misaligned with human goals.

I argue that realistic training processes plausibly lead to the development of misaligned goals in AGI. This is because neural networks, which train via reinforcement learning, will learn to plan towards achieving a range of goals. And, since they are motivated by rewards, they will be more likely to pursue goals that are misaligned with human goals.

This report outlines possible research directions for how to prevent this from happening. However, much of this research is still in the speculative stage. In order to make progress, we need to move beyond speculation and towards actual implementation.