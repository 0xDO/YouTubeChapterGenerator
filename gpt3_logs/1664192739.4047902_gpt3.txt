The following is an unstructured video transcript. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody david shapiro here um okay so i posted my video yesterday the salty one about alignment research and i got a really good response by uh vulnerable growth um so he says he works in a ai alignment and works in the same building as the guys for the article that i referenced from les wrong i'm not going to read the whole thread but it's very good it's very engaging so for pillars um about you know interpretive or flexibility uh and and implementability and and so on he agrees that like we've got one shot to align agi and waiting until things are only empirically testable is a bad idea i agree um but basically one of the things that he's saying is like there's a reason that like we can't do um like we basically we can't do empirical testing yet and that's where i really disagree um so people in alignment do not see the work of aligning current language models as ai alignment because the current models are not powerful enough to really matter so that's where i really disagree now is the time to start figuring this out so like uh i'm not gonna die on this hill like he acknowledges that there's a lot of disagreement but this is this is where it's like no we have we have machines that can think um we have things we have machines that can use human level reasoning so it's time to really to do this and they're basically saying like we got to wait for we until we have super intelligence um and i'm like no like we have an intelligent machine so let's figure out how to make it moral and ethical now um so that's my position um things like interpretability are not sure shot as a model becomes super intelligent and maybe impossible to interpret so that's the thing is like if you build something that is not interpretable um then like i don't think you should build that period end of story um you know like okay good friend like and it's also not just a matter of understanding what one model is doing so this is this is what i mean when i say that people are thinking about intelligence as a single model um when it should not be like one it isn't and two it should not be if you if you're relying entirely on a single black box that is a bad idea um and so this is where my criticism comes in that like we have to think about intelligence as a system not as a single model and we can do that today and i'm really disappointed that there is no research into that um there are several worst case guarantee research directions etc um yeah so like here's here's the other thing um a alignment that's that some experiments are only possible at big labs obviously i can't disagree with that um but also that's not a valid excuse right there is there are powerful tools out there today and so any any continued um work that is strictly hypothetical and not testing these systems today to figure out how they work and how to keep them aligned or stable to me is a waste of time and energy now that being said i'm not going to go tell someone stop doing what you're doing but i'm just saying why i am disappointed disappointed in the state of industry and the state of the field so i said i disagree about it not being powerful enough no one seems to be thinking about agi as a system and the best that he says is that um is that uh is that retro deep mind as a system and it's like okay but you look at the the architecture of of it um where's images why don't i have images okay whatever um and it's like it's just a retrieval thing like when i talk about systems i mean hundreds of interacting parts thousands of interacting parts so this is where my experience as a systems engineer and looking at the stability of complex systems like where you have multiple databases where you have multiple consumers you have multiple participants in something like if you're not testing that then you're not really testing intelligence you're testing one tiny module of intelligence and the thing is is when you deal with large systems complex systems where you have things like quorum consensus like those are the kinds of alignment experiments that we need to be doing where you do those kinds of things not just making sure one model is good or bad because like um and here's the whole point of this video is i'm testing like having an unaligned model showing that with the right system you can stabilize it um so yeah i the more i'm talking about this the saltier i'm getting so like well you know i'm this is not a criticism of you vulnerable growth i am very grateful for your um for your your feedback i still disagree i still have a fundamentally different perspective on one what intelligence is because this is another thing is just judging by a lot of the papers that ai researchers put out they really don't have much training on like actual intelligence um or neuroscience or cognition or psychology or philosophy like you gotta cross-pollinate if you want to understand intelligence you gotta cross-pollinate because one thing that happens with um purely ai researchers is that you miss you miss um you miss the uh the math for the model right or you miss the model for the math um another way of saying that is you miss the forest for the trees um i've talked to several researchers who who see large language models as just a mathematical curiosity and don't actually understand the power of the language in it now you show a large language model to humanities or a philosopher or a librarian they get it they say wow this is performing human level thought um and this is why even the engineer from google was convinced that the thing had a soul is because it has it has crossed the uncanny valley and it's like it looks like there's actual thought happening and so like that of course there's the philosophical debate of like okay is it actually thinking like true thought like we don't have to argue about that because objectively empirically speaking it is demonstrating human level reasoning and so what do i mean by that okay so i decided to do an experiment

CLEAR ESSAY:

==========

In his video "The Salty One," David Shapiro discusses his disagreement with the popular AI alignment research perspective that current language models are not powerful enough to warrant ethical consideration. He argues that because these models are demonstrating human-level reasoning, we should be working to align them now, rather than waiting for superintelligent AGI.

Shapiro criticizes the AI alignment community for not thinking about intelligence as a system, and argues that this lack of understanding will lead to problems down the road. He believes that experiments should be conducted now on how to keep these systems aligned, rather than purely hypothetical work that doesn't test the current models.

While he acknowledges that some experiments can only be conducted at large labs, Shapiro believes that this is not a valid excuse for not doing any research into alignment today. He is disappointed in the state of the industry and the field, and believes that more attention should be paid to developing systems-level thinking about intelligence.