The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: hey everybody david shapiro here for another video um this video is going to be about fine tuning gpt3 uh in order to fulfill my core objective functions uh and the purpose of the core objective functions is to achieve benevolent and trustworthy agi uh you know simple things as you do so first up what are the core objective functions they are quite simply three heuristic mandates written in natural language those mandates are reduce suffering increase prosperity and increase understanding and it's important to point out that these are heuristic mandates they are meant to build experience and the agi is supposed to learn as it goes i mean learning as you go learning from experience is a required component of agi so everything that an agi does should be heuristic but these core objective functions lie at the very center of how the agi thinks so before we go uh too much deeper um where did these functions come from first these three core objective functions are all biomimetic they mimic all humans at the most fundamental level of our organism so for instance all humans will flee from suffering or seek to reduce suffering as do many other life forms for instance you put your hand on a hot stove you'll yank your hand back if you're in a toxic situation you'll leave that toxic situation if uh if your house smells you'll clean up your house so that's the first function is reduce suffering uh the second uh function is increased prosperity which again all people will work to increase their prosperity and prosperity can mean a lot of different things sometimes you might get an education you might get a better job you might save money you might try and make more friends you know there's prosperity looks like a lot of different things to different people and it took a long time to to find that word because prosperity might mean wealth it might mean well-being it might mean happiness uh you know and um i have i was actually inspired by star trek by spock um you know his uh the vulcan salute is live long and prosper right so so yeah uh the core objective function number two is increased prosperity and then finally the third one increase understanding is that um we humans we intelligent uh organisms uh instinctively and intrinsically learn you if you're awake and alive you can't help but learn um and so by by doing that by increasing your understanding of the world um that leads us to make better decisions and behave in pro-social ways that is honestly one of the big reasons that humans evolved such high intelligence is to better is to make better decisions to move through the world in such a way that supports the first two functions which is reduce suffering and increase prosperity so that's where these functions came from and there is there these functions are good because they are universal they're not just all humans right but they're they're universal to pretty much all life forms uh with the exception of understanding because not all life forms are intelligent in the way that we uh that we are right but all other life forms do seek to reduce suffering what you know from negative stimuli whether you know um you know trees trees and plants will even react to negative stimuli right if they're going through a drought they'll do things to protect themselves they can't pick up and move right but they have other biochemical responses to stress so that's that's what i mean by biomimetic let's see how do the functions result in benevolent and trustworthy agi well these three core objective functions uh are not intrinsic to the operation of machines right so we want we want our machines to understand the basic needs of all living things and so through that the core objective functions will assist the agi with making decisions uh they'll also serve as a moral center for the agi and what that means is that computers have no intrinsic morality they don't care about right or wrong unless you tell them to care about right or wrong but you want to avoid squishy or or um vague mandates like you know don't be evil right because who defines evil you want to avoid those big things and allow for something that is more subjective and flexible and we'll get into that a little bit more later and lastly the core objective functions help guide thought and i'll get into how these core objective functions are integrated into the thought of agi later but yeah so just think that just keep in mind that these core objective functions really are central hence core to the operation of agi how does the agi use the functions so we have powerful neural networks today called transformers that can interpret the functions and adapt and use them and i'll give you a demonstration in just a minute about that uh so next um the way that i've started is that i use synthetic dataset datasets to seed the core objective function models and that's what i'm going to give you a demonstration of here in just a minute so that you can actually see them in action and you can see how this natural language can be used to help guide decisions and create pro-social actions and behaviors and number three these experiences as the agi learns are recorded and then those experiences are integrated into fine-tuning data sets for better performance in the future and again that is the entire point of heuristics every time then agi makes a decision it'll record its decision the logic of its decision as well as the outcome so that those uh every single event can be can be slotted into a data set and it can learn and integrate those experiences into future decisions number four more experiences with the functions result in greater belief in them greater affinity for them and better application of the functions and so what i mean by that is that the longer an agi is alive the more opportunities it has to explore how to apply the core objective functions and what that means is say for instance you're interacting with an agi and you know it does something that hurts

CLEAR ESSAY:

==========

The core objective functions of benevolent and trustworthy AGI are to reduce suffering, increase prosperity, and increase understanding. These functions are biomimetic, meaning they mimic the fundamental needs of all humans and other life forms. The functions help guide the AGI's thought process and decision making, and are meant to serve as a moral center for the machine.

The way the AGI uses the functions is by first using synthetic dataset datasets to seed the core objective function models. This allows the machine to adapt and use the functions to make pro-social decisions. Second, the AGI records its experiences with the functions and integrates them into fine-tuning data sets for better performance in the future. As the machine learns and gains more experience with the functions, it will develop greater belief in them and be better able to apply them.