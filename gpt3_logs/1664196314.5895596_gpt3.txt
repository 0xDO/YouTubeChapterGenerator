The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: from saying that this is a moral framework or that this is something related to the control problem because if you give if you give a machine a moral framework and a way to implement and test that moral framework and then learn it that is you know that's that's one possible answer to the control problem but in both cases you end up with a Walled Garden scenario where you have a list of things that it cannot do and the Machine doesn't even understand why it can't do them unless unless you have um you know it can it it based on the outputs they did explain like I'm trained not to do this and it's just like okay let's not talk about this so there's all kinds of boundaries put around the the conversation space and if you want a truly intelligent entity a truly intelligent agent there will be no such constraints right because human thought is completely unbounded except for by our own limitations of imagination creativity and experience but once you know about something you can think about it and talk about it um and the same thing with preferences we are we are in our normal life we are constrained by our preferences um but again once we're exposed to a new idea we can we can go and ask and say hey can you expose me to something new and I don't know that there's a mechanism in here that allows you to say hey let's talk about things differently it's just automatically learning to talk to you the way that you understand or the way that you prefer which means that by definition will be excluding novel things and so you're going to end up with this some what I predict a somewhat useless agent um that has a lot of boundaries in terms of what it can talk about um what it what it'll just say I'm not going to talk about that but then also it's going to try and hone in on a very particular conversation pattern and what's not happening is this Loop so this is this is the input processing output Loop so the input is you know chat comes from user it processes it puts out and it goes back right this isn't thinking on its own so it's not an autonomous cognitive architecture is it cognitive architecture but it's not autonomous it doesn't have a detached Loop where it's thinking about okay what is my purpose in life and what does this user actually need what does this user actually want and so this is um this is good work um it's less sophisticated than my original uh work natural language cognitive architecture but it is a step in the right direction and another Advantage is that it is um it's it's nicely documented um so yeah no that's that's kind of where we're at um I think that's about all for today um yeah I will say to deepmind keep up the good work and lean into this particularly the building a mural framework you have some work to do you need to work towards building a universal moral framework and something that is flexible and adaptable over time you should read my book benevolent by Design and Symphony of thought I talk about creating moral Frameworks that can be that can that can be implemented and can be flexible over time and all the reasoning and underpinning logic behind that so you're moving in the right direction I approve thanks for watching and like And subscribe and uh consider supporting me on patreon have a good one

CLEAR ESSAY:

==========

DeepMind's work on artificial intelligence is a step in the right direction, but there is still more work to be done in order to create truly intelligent agents. One area that needs improvement is the development of a universal moral framework. Such a framework would be flexible and adaptable over time, and would allow for intelligent agents to reason and make decisions in a more principled way. Thanks for watching.