The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: and then i'll run this again just so that way i've got multiple options so we go to plain text wikipedia and sqlite simple so this is up to about half a gigabyte the maximum size for sql light is something like 218 terabytes or something like that so even if this is over a gigabyte that's still going to be well within reason so i'm not too worried about that even if we double the size by adding an index of the articles to two gigabytes or even if it goes more than that to three gigabytes i think it'll still be worth it because it'll it'll basically give raven a brain a knowledge about the world okay so the rest of the script um with open simple wiki underscore file name so it opens this file um with encoding utf-8 as in file we step through line by line and the reason that i found that i had to do this was because the text files were so big and um and it just it takes too much memory to try and do operations against the whole thing i in my initial experiments i would try and do like split on page and uh i just watch you know the the python memory just climb but we can ingest a file one line at a time and what you'll notice is that there are because this is exported from a database in the format of xml there are some nice consistent things so for instance the beginning of every page starts with this tag here page and then the title always is wrapped in these tags and then the id is wrapped in these tags or actually the first one wrapped in these tags and then the actual article body is wrapped in text so there's less than text and then it ends in less than slash text and then the the overall page itself ends in slash page so because of that nice consistent behavior it's easy to just read this one line at a time and then find what you need so that's what i did so if page is in the line then we know that we're at a new article so let's just start a new article else if the end if slash page is in the line we know that we're at the end of the article so let's go ahead and analyze this chunk and then if we get a good document back let's save it to the database is that simple otherwise if we if we're if we're just in the middle if we don't see this tag or this tag we'll just add the current line to the chunk that we're working on and so it builds up a subset of this so it'll go from this page tag to the next page tag um and and we'll we can parse it from there so the parsing it doc analyze chunk so first the first thing we do is we get a few we filter out a couple things so if an article is a redirect title i'm not really interested in that so we just return none and move on otherwise we split the title out of its tag simple enough um there are cases i think i got a version that doesn't have it where wikipedia is chalk full of talk category and other pages that are not actual encyclopedia entries and i found that universally they have a colon in them so if a title has a semicolon in it i don't want it either otherwise we run it through htt which is html to text so it just derenders the html into plain text and then we say okay so there is the uh there's the title uh we get the serial number which is out of the id so we split it from the first one and then we split it um and then we get we we split that uh split that out and then the content so the content is we we uh take it out of the text tags right so we get we we separate out the end text tag and the beginning text tag and then we run it through the d wiki function the d wiki function is where all the magic happens so here's what we do there first we remove all the new lines because this is just going to be a plain text article it's not going to be formatted for the web so we get a rig we get rid of all the new lines we get rid of excess white space so any tabs or other spaces become a single space we remove any audio files we remove the references at the end so if you see the dash or the equal equal references notes or related pages not really interested in that because those are mostly just links i've got this in a database a searchable database so i'm not too too worried about that we remove citations which are in the curly brackets again those citations are you know we're taking this is true and raven can't read html well i guess gpt3 can read html but we want raven to speak in plain english we remove the categories which tend to be at the front of the article or the end we remove links and this isn't actually removing a link we're just replacing the link so here's an example right you might see this where it says autumn and then and then vertical pipe autumn slash fall and so basically what i want to do is just take the right hand bit from all the links whether it's a simple link or a compound link or other um and then i just want this last bit of text so that will give me just plain and simple links and then i want to do the same with pictures so pictures have i think i've got an example right here yeah actually this was this was a copy that's audio this is not correct there we go remove pictures so pictures usually follow this format we got file colon earth flat blah blah blah blah and then another pipe and then the very last pipe or a very last section is actually a description a plain text description and so i want to actually keep that so i created all the patterns that i found for images this gets 99 of them it misses a few and then i just replace it out and then say here's an image and it puts it into the text so that so that the fact that there was a picture there is captured raven can't see yet raven doesn't have any ability to process images in the future we might have a full version of wikipedia so that raven can see pictures and compare what's in his encyclopedic knowledge of the world with what he's actually seeing in real time that's a that's possibly a long ways off and this is still running i expect it'll take a while we're probably um yeah we're at we're at half a gigabyte so we're about halfway done maybe a little bit

CLEAR ESSAY:

==========

The goal of this script is to take the unstructured data of a Wikipedia dump file and turn it into a more structured, clear essay. To do this, the script first opens the file and reads it line by line. If it finds a "page" tag, it knows it has reached a new article, and starts building up a chunk of text. Once it reaches the end of the page, it analyzes the chunk and, if it is a good document, saves it to the database.

The parsing and analysis is done in the "doc_analyze_chunk" function. First, a few things are filtered out (e.g. redirect titles and articles with a semicolon in the title). Then, the title and content are extracted from the tags. The content is run through the "d_wiki" function, which removes all the new lines, excess whitespace, references, citations, and links. For images, it just keeps the plain text description.

This script will take a while to run, as it is currently halfway through a 1 GB file. However, even if the file size doubles or triples, the script should still be able to handle it.