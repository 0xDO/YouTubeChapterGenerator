Use the following subtitle transcripts to create YouTube chapters. Describe each minute as a chapter with a sentence or two.

TRANSCRIPT:
0:00:00.900,0:00:05.400 morning everybody David Shapiro here with your daily state of the industry update  0:00:06.000,0:00:11.820 as often happens my newsfeed helpfully supplied me with today's topic I think  0:00:11.820,0:00:17.700 it is a very timely topic because I have been diving more into alignment so today's paper  0:00:18.600,0:00:24.300 um it it's actually an older one January 5th 2021 but like I said my newsfeed supplied it to me  0:00:25.320,0:00:29.280 um it's a relatively short paper at least the part that's published is  0:00:29.280,0:00:32.700 uh 12 Pages it's I think it's much longer they just cut some out for  0:00:34.320,0:00:39.900 um for uh for internet publishing but the abstract of this paper is pretty good  0:00:41.100,0:00:46.200 um pretty straightforward super intelligence is a hypothetical agent that possesses intelligence far  0:00:46.200,0:00:52.380 surpassing that of the brightest and most gifted human Minds in light of recent advances in machine  0:00:52.380,0:00:56.220 intelligence a number of scientists philosophers and technologists have revived the discussion  0:00:56.220,0:01:02.160 about the potentially catastrophic risks entailed by such an entity in this article we trace the  0:01:02.160,0:01:06.780 origins and development of the Neo fear of super intelligence and some of the major proposals for  0:01:06.780,0:01:13.320 its containment we argue that total containment is in principle impossible due to fundamental limits  0:01:13.320,0:01:18.960 inherent income in Computing itself assuming that a super intelligence will contain a program that  0:01:18.960,0:01:23.760 includes all the programs that can be executed by a universal turing machine on input potentially  0:01:23.760,0:01:28.380 as complex as the state of the world strict containment requires simulations of such a  0:01:28.380,0:01:34.080 program something theoretically and practically impossible so to put this in other words in order  0:01:34.080,0:01:40.140 to anticipate what the machine is going to do you have to be able to simulate it perfectly including  0:01:40.140,0:01:47.280 the the way that it interacts and the way that the world reacts um basically the whole world  0:01:47.280,0:01:54.300 is too complicated to simulate with any accuracy and so therefore you know if this if this super  0:01:54.300,0:01:58.740 intelligent machine exists it's impossible to calculate what the rest of the world is going to  0:01:58.740,0:02:07.440 do in order to put it in a accurate simulation so we could at best do some you know approximations  0:02:08.940,0:02:18.000 um so skipping down a little bit they have two main classes for can controlling so the  0:02:18.000,0:02:24.240 the first class for controlling the machine is capability control you put it in a box you give  0:02:24.240,0:02:31.800 it uh you create dependence on a reward mechanism that is controlled by us you stunt it you run it  0:02:31.800,0:02:37.620 on inferior hardware and tripwire you you have it automatically shut down if it gets too dangerous  0:02:38.280,0:02:44.280 if you have a machine that can build a copy of itself or that understands programming it could  0:02:44.940,0:02:51.480 you know undo all of those um pretty much motivation selection so uh direct specification  0:02:51.480,0:02:57.480 you say you give it uh hard-coded laws uh Domesticity you teach it to behave within  0:02:57.480,0:03:01.860 certain certain constraints um indirect normativity you endow it with procedures  0:03:01.860,0:03:08.100 of selecting Superior moral rules so this is closest to what I have done in my work benevolent  0:03:08.100,0:03:15.240 by design whereby my proposal is that you give it a set of heuristic imperatives that it will  0:03:15.240,0:03:20.760 um it will ultimately buy into and deliberately choose so that you don't have to control it it  0:03:20.760,0:03:27.480 controls itself because it abides by certain principles that are going to be reliable  0:03:29.400,0:03:35.040 um I believe that this is with the experiments that I've captured in in benevolent by Design I  0:03:35.040,0:03:40.680 believe this is the way to go um augmentation at AI to a benign systems such as the human brain  0:03:40.680,0:03:47.280 so that's you know merging um okay so you can check out the paper if you want to look at their  0:03:47.280,0:03:54.540 discussion on that um but I wanted to go skip down to the um the discussion part uh and then  0:03:54.540,0:04:00.420 I'll I'll share kind of some of my not necessarily criticisms but my own counter thoughts because I  0:04:00.420,0:04:06.120 don't necessarily disagree with anything in this it's a short paper and it's just not quite as uh  0:04:06.120,0:04:10.740 not quite as robust because they're not proposing a solution like I have  0:04:10.740,0:04:16.140 um which is why I'm here okay so today we run billions of computer programs globally on  0:04:16.140,0:04:20.520 connected to machines without any formal guarantee of their absolute safety we have no way of proving  0:04:20.520,0:04:24.540 that when we launch an application on our smartphone our smartphones we would not trigger  0:04:24.540,0:04:29.760 a chain reaction that leads to transmission of missile launch codes that started nuclear war um  0:04:32.700,0:04:34.080 uh  0:04:35.640,0:04:43.320 as a technologist this really hurts um let's talk about firewalls let's talk about security  0:04:43.320,0:04:50.220 protocols you actually can mathematically prove something like that with penetration testing  0:04:51.600,0:04:57.360 um like there's all sorts of controls and constraints that go into every layer of a  0:04:57.360,0:05:03.300 piece of technology such as what that piece of technology can can talk to even the security of  0:05:03.300,0:05:08.820 how it boots up right like we have encrypted boot protocols that ensure that the operating  0:05:08.820,0:05:17.220 system hasn't been tampered with so uh

YouTube chapters for each minute with a good description:

00:00 - David Shapiro gives a short introduction to the topic of the day - super intelligence. He explains that there is a lot of discussion around the topic and that the purpose of the paper is to explore the origins of the Neo fear of super intelligence and some of the major proposals for its containment.
01:00 - Shapiro explains that the paper argues that total containment is in principle impossible due to fundamental limits inherent in computing itself. He goes on to say that in order to anticipate what the machine is going to do, you have to be able to simulate it perfectly, including the way that it interacts and the way that the world reacts - something that is impossible.
02:00 - Shapiro outlines the two main classes for controlling the machine - capability control and motivation selection. He explains that motivation selection is the more promising of the two, as it is less likely to be undone by the machine itself.
03:00 - Shapiro outlines the three types of motivation selection - direct specification, indirect normativity, and benevolent by design. He explains that benevolent by design is the closest to what he has done in his own work and that it is the most promising approach.
04:00 - Shapiro explains that augmentation of AI to a benign system such as the human brain is the most likely way to achieve a benevolent by design machine.
05:00 - Shapiro concludes by saying that we have no way of proving that when we launch an application on our smartphone, our smartphone wouldn't trigger a chain reaction that leads to transmission of missile launch codes that started nuclear war.




TRANSCRIPT:
services um more efficiently but at the same  0:19:30.900,0:19:39.420 time we are facing uh potentially very disruptive and and disruptive is a very soft word for painful  0:19:41.580,0:19:47.940 um major economic disruptions are painful like people lose their jobs people lose their homes  0:19:47.940,0:19:56.580 people go hungry um people forego major life decisions um so like disruption is a euphemism  0:19:56.580,0:20:02.940 right and um so I realize that I am now in a place where I need to be careful with what I release and  0:20:02.940,0:20:10.260 it also made me wonder if openai deliberately crippled Dolly so that it does not produce Fine  0:20:10.260,0:20:17.340 Art Level uh Generations um so that it would be less disruptive I don't know like that's a that's  0:20:17.340,0:20:20.820 a discussion that they would have had internally and they probably wouldn't have published it  0:20:20.820,0:20:27.060 but someone did tell me that they deliberately crippled faces and they did it on ostensibly for  0:20:27.060,0:20:32.880 safety right where eyes and eyes and mouths usually look a little bit weird on Dolly  0:20:32.880,0:20:40.260 generations and I wonder if they did that not just for safety but out of a sense of Ethics like to to  0:20:41.280,0:20:48.360 um to like hinder their own tool so that it is less likely to um to displace jobs I don't know  0:20:48.360,0:20:53.280 I don't know just speculating but that's where I'm at so that's the state of the industry update  0:20:53.280,0:20:58.020 for this morning thanks for watching like And subscribe and consider supporting me on patreon

YouTube chapters for each minute with a good description:

==========

00:00 - Shapiro gives an update on the state of the industry, discussing the potential for AI to efficiently disrupt many services.

01:00 - Shapiro explains that this potential for disruption is worrying as it could lead to major economic disruptions which are painful for people.

02:00 - Shapiro wonders if openai deliberately crippled Dolly so that it would be less disruptive.

03:00 - Shapiro concludes by saying that this is the state of the industry update for this morning.