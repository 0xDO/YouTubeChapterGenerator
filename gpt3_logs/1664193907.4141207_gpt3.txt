The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: hey everybody david shapiro here um so there was a post on the open ai forum that was asking about well here let me just show you um what was it the guy was asking about where was it um he was asking how to fine tune from scratch uh oh it was this one okay okay so let me show you guys this so this guy asked i'm using short stories that i wrote to bias the voice of of what gpt3 generates i've tried to format data in multiple ways but maybe before i spend another chunk of money someone already went through this and has some tips to share so he tried a few strategies some someone tagged me thank you for that pulling my attention to to this post i had done a similar experiment to basically using um fine tuning or gpt3 to do fiction so that was my auto muse project so the code is here it doesn't really work i abandoned it six months ago i just got a couple things working i didn't know as much as i know then i've been experimenting with it ever since so that's the background so here's my response i said achieving coherent fiction as as we imagine it is perhaps one of the most elusive tasks yet i would recommend following the document example of fine tuning where you leave the prompt empty and put the whole story or a big chunk of it as the completion this requires more samples but it can fine tune the model on your style and tone alternatively you might try having one paragraph of the prompt and the subsequent paragraph as the completion so like paragraph a is the input paragraph b is the completion so it knows you know follow up this paragraph with the next one and i said apparently ai dungeon uses a lore book mechanism so that the major details of the story can be referenced at all times for each completion with this you might be able to have a lore book section of the input prompt as well as the previous paragraphs and then you can have the next paragraphs as the completion if that doesn't make sense don't worry i'll show you what i mean because i'm just talking it talking through it so i said this gives me some ideas let's go it so that's that's what we're doing today i'm we're experimenting you're just writing shotgun watching the whole process i don't even know what i'm doing this is this is rapid prototyping okay so we're going to call this auto muse 2 experiment to generate fiction um let's see novel length fiction from a single story premise and i've already got a premise for this uh add a readme i'll be recycling some code okay so we got this clone it down get clone auto muse too okay so i've got a few things that i'm going to borrow there's the where is it um what was it the movie script generator hold on what did i call this oh that's right i was named incorrectly um okay so i already generated a whole bunch of movie premises so if you watch a previous video i go over about like just generating this but basically this is like a whole premise of a story um okay so let's copy the premises into our new um into our new uh the repo that's the word i'm looking for the new repo will be brain what are you doing what did i just name this thing auto muse too there we go okay so we're starting with some data that's what this is so i've got a folder that just contains uh it's 200 story premises and yeah they must now find a way to warn other humans of the impending danger um yeah so all very dramatic uh so here's my idea so my idea intuitively is we'll put in the premise playground so let's just this is usually where i start is like again you're riding shotgun this is this is a cold open um so we'll say like premise um let's see write a novel based on the following premise um and uh and yeah so we've got the premise here and then after that we would have like uh the story so far and probably do this and then i would do like um [Music] uh opening so like nothing has happened right the beginning of the story um let's see the human race has reached a point and um someone i think was on youtube someone left a comment like oh you should you should do a test where you um you say like write in the style of so like in my first book natural language cognitive architecture i showed that gpt3 like if you say like write as if you're a victorian gentleman it'll write like that if you say write like shakespeare it'll write like that if you say write like a chav which is like um uh what is that that's like a a ruffian from england um it'll write like that um so let's see write a story based on the following premise um write like frank herbert so we'll say like write like the dude who wrote dune um copying his style there are ethical implications with that um we'll see how it does so um novel uh let's see chapter one let's just see what it does uh it looks like it's probably just gonna copy yeah i think it's just copying what it had already the human races where i had the human race has reached a point where it exhausted all of earth's resources in a last-ditch effort to save our species we sent a colonization so yeah they were soon by met by a hostile alien race let me copy this into um notepad plus plus this might not work at all yeah it looks like it's just gonna copy copy it verbatim so this is this is a problem um with uh with with gpt3 especially davinci 2. um let's turn up the temperature crank it up to 11. um so with with fiction you usually want to have um the the temperature higher because that makes it more creative and less deterministic um but also i found that turning the frequency penalty up uh a little bit not not a whole lot but just turning up the frequency penalty a little bit tends to make it be a little bit more creative i figured that out when i was making the um the the movie script generator was because it kept generating the same like story patterns over and over again but if when i turn the frequency penalty up and the temperature up it got a little bit better so let's see what happens now yeah it's just copying itself um write a novel let's see if i can conjol this into add a

CLEAR ESSAY:

==========

In a recent post on the Open AI Forum, a user asked for tips on how to fine-tune a GPT3 model for use in generating coherent fiction. In response, I recommended either following the document example of fine-tuning (where you leave the prompt empty and put the whole story or a big chunk of it as the completion) or trying a prompt that consists of one paragraph of the story followed by the next paragraph as the completion.

I also suggested that the user might try using a lore book mechanism, where major details of the story are referenced in the input prompt and the model is then tasked with generating the next paragraphs of the story.

To test these ideas, I decided to run an experiment to see if I could generate novel-length fiction from a single story premise. I started by recycling some code from my previous project, the movie script generator. I then took a folder of 200 story premises and copy-pasted them into the new repo.

Next, I created a playground prompt that asked the model to write a story based on a given premise, in the style of Frank Herbert (the author of Dune). Unfortunately, the results were not very promising; the model simply copied the premise verbatim.

I then decided to try increasing the temperature and frequency penalty, in hopes that this would make the model more creative. Unfortunately, this did not seem to help and the model continued to copy the premise verbatim.

Overall, it seems that generating coherent fiction with GPT3 is still a very difficult task. However, with more experimentation, it may be possible to find a way to fine-tune the model so that it can generate coherent stories.