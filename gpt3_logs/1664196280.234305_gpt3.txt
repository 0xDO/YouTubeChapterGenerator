The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody David Shapiro here with another video um sorry I missed yesterday I was feeling rather tired um I woke up after only four hours of sleep and uh and yeah I've been really focused on some of my projects make sure am I still centered okay yes centered all right so we've got a busy day today since I missed yesterday so this will probably be my only video of the day first up we've got um Nvidia Jetson Oren so for those who are not familiar um they're Jetson series is uh is small mobile Edge AI so I have a Jetson Nano which is like 10 times less powerful than this guy so let me just let's go ahead and just dive in I'll show you on the learn more tab so they've got the the board level um a little bit of a diagram of what what all is included on the board um but like taking a step back like what is what do you use this thing for this is a developer board for prototyping mobile applications or portable portable applications um and so you see here uh you got some like or I guess like another example like what can it do um you can do video inference you can control robots that sort of stuff but yeah okay so you've got um 512 core Nvidia ampere or 1024 core okay sure but how does that compare to previous generations so here you go I have a Jetson Nano um which you know okay sure um I think I spent like 400 on it um but look at this guy you go from um from a performance factor of seven to 134. so that is more than an order of magnitude that's two orders of magnet or almost two orders of magnitude um better performance um yeah so that's 20 times 20 times a performance um almost um yeah uh and you see that kind of increase across the board pretty much um so you go from uh for LPR um which my brain is not supplying what that means right off the moment um from 47 to 534 for dash cam 11 to 198 for body pose from 3 to 68. um so yeah you get these and this is just the the introductory version yeah and obviously you get you know twice the performance if you have a a um core that's twice as dense um yeah so these are these are Next Level they are more expensive um so for instance the full developer kit let's see where was it I think they're like two grand um let's go to Jetson aurum learn more autonomous machines it's pretty though look at that small form factor getting started guide that's the comparison where is the pricing I know I looked at it huh okay well I'll post it in the links if I can find it anyways the uh just the raw oh here's some prices yeah um so Jetson AJ Xavier yep so all the different prices are here um yeah so this is interesting if someone wants to send me one I'd love to play with it but usually I'm more on the high-end stuff portable portable mobile and Edge AI is a is a whole different domain okay so next up from Nvidia we've got uh Nemo their llm service so what is Nemo um the Nemo homepage sorry let me go back up to the top um Nvidia Nemo is an open source framework for developers to build and train state-of-the-art conversational models so this is this is um end to end this has voice and language models um this is this is kind of their Flagship it's based on Megatron the 530 billion parameter language model so this is four ish times bigger than than gpt3 and Bloom I have no idea how well it's going to perform but there there you have it and so they've actually opened up um you can apply for Early Access which of course I have as a researcher who studies alignment cognitive architecture and now I'm focusing also on uh on uh generating long form fiction I obviously want access to this they require an NDA for granting access so even if I do get access I probably won't be able to talk about it we'll see what the NDA says and that's of course if I get access but it's very exciting because this is a huge model um and it's it's going to be powered by Nvidia which is you know other than uh cerebrus wafer scale engine the best on the market for running AI um yeah so that's also very exciting um we'll move on from there uh another thing speaking of large language models is pedals so someone on my Discord helpfully uh shared this so what is petals it is exactly what it says right here it's a decentralized platform for running 100 billion parameter uh language models so if you remember seti at home or folding at home this was a case where you'd have an agent running on your machine and you would receive workloads from the cloud or from you know distributed peer-to-peer Network your machine would do would work on part of the workload and then send the results back this is similar but instead of running like you know a little folding experiment or processing some Radio Data what you're going to be doing is actually processing um one or two layers of a large language model and you will be a node and a network so you'll basically break apart a large language model into many many nodes to run across many machines and then you'll receive just a part of that layer and then you'll pass those results back um so they say that uh it's it's still pretty fast and so on I've got the paper pulled up so uh let's see there was a good graphic that kind of shows it um well so first here you see like they've got a little distributed Network um but yeah so here's here's a very simple example I love how the gpus have uh have little little googly eyes um so basically you have multiple clients all contributing workload uh demand and so then there's different people with gpus hosted all over and so then they get um you you'd get you contribute your GPU so you're running workloads for other people and other people are running uh workloads for you and so it's a peer-to-peer network of gpus basically so uh structurally I don't know if it's actually similar to the way that folding at home or study at home was done I'm just pointing out that this has been done before it's been done for decades um literally I I had a folding at home set up on uh on my laptop when I worked at uh more than 10 years ago and

CLEAR ESSAY:

==========

Nvidia's Jetson series is a line of small, mobile Edge AI devices designed for prototyping mobile applications. The Jetson Nano, the cheapest and least powerful of the bunch, is still ten times more powerful than a standard desktop computer. The Jetson Aurum, the most expensive and powerful of the bunch, is nearly two orders of magnitude more powerful than the Nano.

Nvidia's Nemo is an open-source framework for building and training state-of-the-art conversational models. Nemo is based on Megatron, a 530 billion parameter language model. Megatron is four times bigger than the largest existing language model, Google's GPT3.

Petals is a decentralized platform for running 100 billion parameter language models. Petals is designed to be faster than traditional centralized language models by distributing the workload across a network of computers.