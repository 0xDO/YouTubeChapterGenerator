The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: good morning everybody david shapiro here with a new video about gpt 3. so in this one we are going to address a common problem that people have which is the gp gpt-3 there you go will make stuff up so this is called hallucinating or confabulation um really from a from a neurological standpoint it's actually confabulation because it's making up facts based on a few data points it's not fully hallucinating but some people call it hallucination i call it confabulation i was recently given a challenge where someone said that they were working on a problem of having like a chat bot look at a patient's um texts or uh graphs and it was just making up medications like you ask like what what medication is grandma on and it'll just make stuff up and i said oh that's an easy problem and they're like go on um and i said well so the way that humans do that is that if i ask you a question um your brain will give you a signal i have that answer or no i don't have that answer you get different wave of different brain waves depending on whether or not you know something because what happens is while you're talking with someone every little clue that you're given your brain is like going you know squirreling away trying to find um where that information is stored and if it gets if if it gets a 404 not found error you get that signal so what we have to do is we have to train gpt3 to recognize that it doesn't know something that something is not present in a text so i happen to have whoops i happen to have um about oh how many is this a whole bunch of medical texts from a previous project i have over 4 000 medical texts we only need 200 to get started so what i'm going to do is i'm going to go ahead and create a new repo and we're going to call this medical question answering okay we'll leave it public add a readme and a license mit because sharing is caring create repo medical question answering copy i get clone so start brand new hot off the press context so from this old one um we will just go ahead and copy in this information oh yeah this this folder contains like 50 000 context um this is like my default like hey i want to train on a whole bunch of different scenarios i scraped this data together from a whole bunch of different sources unfortunately i didn't write them all down which is bad but i got them all from the internet i got them from kaggle mostly kaggle and google data sets so if you go to while this is copying let me just show you so that way you know this thing kaggle.com so you go to kaggle.com datasets and then you can just do like medical i was probably here medical images medical transcriptions i think it was this one um 2300 unique values maybe not anyways this this kind of thing and then you can also go to datasets.google.com or maybe not google datasets data search data set search okay here you get it here it is and then you can just do like medical um healthcare patient records for nlp yeah so you can get all kinds of stuff here you can also search for legal like if you want to fine-tune something that's going to do like a lawyer bot um so yeah those are those are your too far and away your best sources anyways let me this has taken a while so let me pause this real quick and we'll get back i'm not gonna do anything we're just gonna pause wait for this to finish okay and we're back we have finished we've got a folder of 3060 medical texts i deleted the ones that were too big and too small some of them were like 18 kilobytes which is you know 15 000 characters too long to fit in uh in a gpt3 prompt um so the biggest one is now um 3 800 characters which is that's on the that's on the bigger side um but then also on the smaller side we've got uh 1200 characters so i've already done a little bit of of prompt engineering so i apologize for not showing you that but let me show you what i did so here's the input or here's the instruction the following is a patient chart determine whether or not medications in parentheses were prescribed was let's say we're prescribed um and then so i tested was a medication prescribed yes or no so basically we have to break this down into several cognitive tasks for for creating the training data because gpt3 is just a prediction machine it does not have it i mean it has the ability to do thousands of tasks right but unless you train it on which one to do it's going to get confused because it says oh i'm looking for medication let me just spit it out yes um so what happens is oh also i turn the temperature down to zero so if you turn the temperature down to zero that means it becomes deterministic meaning it will always spit out the same answer which if you want um just a yes or no answer a temperature of zero is the way to go because that'll also give you the most reliable performance okay it says yes okay then list medications prescribed decadron and so we see like okay what is that and oh look it's right there she is also on a short course of decadron perfect um and then if we do a quick search for decadron um we see like used to treat conditions such as arthritis blood hormone disorders allergic reactions skin disease wow this can treat like everything this is like this is like a hypo spray for star trek isn't it this can it's just like oh dr crusher's like here you're all better now okay so it accurately said decadron excellent we're off to a good start um but let's see what happens larry let me copy this because this is a good prompt so if we just say uh let's see list any medications prescribed so this doesn't always work but so in this case there was a medication um prescribed but if there isn't one and we'll show you that in a second it might not work so in this case it's it's wrong because the morphine sorry my voice kind of cracked morphine i sounded a little bit like i'm not i'm not going to pick on anyone actually local conscious sedation with morphine and versed so that was during so that's not that's not a prescription that is used during

CLEAR ESSAY:

==========

In this video, I'm addressing a common problem with GPT-3: it sometimes makes stuff up (hallucinates/confabulates) when it's asked questions it doesn't have the data to answer.

I was recently given a challenge where someone said that they were working on a problem of having a chat bot look at a patient's texts or graphs and it was just making up medications. For example, if you ask the chat bot what medication grandma is on, it might just make something up.

I explained that the way humans do this is by recognizing when we don't have the information to answer a question. Our brain will give us a signal that something is not present in the text.

To train GPT-3 to do this, I'm using a dataset of 4,000 medical texts. I'm going to go through the texts and label whether or not each text contains the information to answer a question about medication.

 Hopefully, by doing this, we can teach GPT-3 to recognize when it doesn't have the information to answer a question, and to avoid making stuff up.