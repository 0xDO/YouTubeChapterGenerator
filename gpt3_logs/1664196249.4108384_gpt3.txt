The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody david shapiro here okay another state of the industry uh update so someone on the discord asked about um the control problem subreddit and i said that i don't really pay attention to here because i'm not impressed by the conversations but it is the state of the industry so let's go through it so i picked some of the top things from the last month this is is probably the best one it's on less wrong um and it was published about two weeks ago um and he's got this great table where he's just he breaks down like everyone who's working on uh this on the pro on the on the control problem so what is the control problem ai alignment um and nobody can even agree on what what it is because right now there's like one camp is just make the ai do what humans want because they kind of assume that ai will will always just be a tool it will always just be an extension of humans that's a real bold assumption i don't know that that's true and then there's other folks that are just kind of debating about like inner alignment versus outer alignment um and they're kind of picking it all apart and that's not and i i don't want to like devalue what and what these individuals are doing um but i i think that they're not quite seeing the forest for the trees yet um and so i had i had a i had a friend um a researcher in computer science kind of explain it to me so from the academic perspective what you do is you look at where the current conversation is in the academic world you go to that conversation and you add just a little bit to it if you want to introduce something like completely new like a brand new architecture or brand new theory like um that's that's pretty hard to do you have to have a lot of credibility and in the meantime private industry is surging ahead right we have microsoft and google and meta and and all these other companies that are like they are participating in the scientific process because they're still posting um research on like archive and stuff but they're like there's they're completely disjointed right because you've got the academic thing where they're doing like little toy toy level experiments and then you've got private industry actually producing new models actually producing um new cognitive architectures and stuff and so there's this huge disconnect and so i am particularly skeptical about anything that is purely academic like if you see like uc berkeley and they're the only authors on the paper like it's it's much it's very unlikely that um that if it's that if it's a single publishing author from from a strictly academic environment that it's going to be any good because when you think you know agi research you think deep mind which is private industry you think meta which is facebook you think google you think open ai you think microsoft you don't think stanford you don't think you don't even think mit and this is what is so disturbing to me is how far behind um academic uh you know american academia has fallen um when it comes to these things so anyways this article is pretty it's pretty good in terms of just kind of getting a survey of of what's going on um you know so like sam bowman and outer alignment it's an academic lab um you go down and take a look at it you know runs a lab at nyu you know he is on sabbatical working at anthropic right so like an open source consortium um inverse scaling prize creating data sets for evaluating language models as well as inductive biases and so right now like just implied in this is just understanding if there's bias in an llm and not thinking about how to use an llm to engage with the real world it's very strange but again implicit here is the idea that the ai and large language models are just tools they're just components of things that humans will ultimately be in control of whereas my approach is is fundamentally different where my approach is let's assume that the machine is going to become autonomous how do we build an autonomous machine that can think on its own and and and do moral reasoning now i was very happy to see artificial moral cognition from deepmind 2022 but all they do is propose a framework and this framework is even more primitive than lawrence kohlberg's stages of moral development because that's basically all it is you look at the actual paper and they're talking about they they talk about um you know moral development in in in humans um and they kind of list you know they list four uh milestones whereas lawrence kohlberg taught has six that goes up in through adulthood um and i'm like this research is 50 years old actually it's probably older than that how old is lawrence kolberg's research 1958 so this this research is let me do some mental math 50 uh 65 years old um almost so like why is this why is the academic research oh this is deep mind actually why is it so far behind like this is a solved problem we understand human moral development why are we reproducing it and doing these very very low value add things when we should be doing experiments with moral development and moral cognition in llms but do you see any any experiments with input and output of llms or fine-tuning data sets no it's not happening right now and so like when i get really salty and i'm like nobody is actually doing any research this is what i mean and if if you're like oh well dave what have you done like check out my book on um on benevolent by design and symphony of thought i actually even have a fine-tuning data set out there called core objective functions um so yeah uh oh yeah so anyways kind of getting back to the list of things here's the open ai blog about our approach to alignment research now in one in one capacity they do explicitly say like we are on the path to agi and then unaligned agi could pose substantial risk to humanity yeah like that's good like okay just say it out loud say it directly and let's move on um so but then like so their current goal is to is to automate the the alignment

CLEAR ESSAY:

==========

The control problem in AI alignment is one that has been hotly debated in the last month, with no clear consensus on what it actually is. However, one researcher has broken down the conversation into two camps: those who believe that AI will always be a tool of humans, and those who are debating inner alignment versus outer alignment.

This article is a great survey of what is currently going on in the world of AI alignment research. It is clear that private industry is surging ahead of academia, with companies like DeepMind and OpenAI leading the way. However, there is still a disconnect between the two, as academic research is often behind the curve.

One example of this is the research on moral development in humans. The article cites a paper from DeepMind that is proposing a framework for moral cognition in AI. However, this framework is less developed than Lawrence Kohlberg's stages of moral development, which is a 50-year-old theory.

It is clear that there is still much work to be done in the field of AI alignment. However, it is also clear that private industry is leading the way, with academia playing catch-up.