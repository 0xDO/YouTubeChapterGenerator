The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: and and hypothesis generation um all right so granted not a whole lot there um assuming we know our own goal proximal and distal models so this is interesting where it's like um you know what it what is your media goal versus what is your long-term goal um i don't necessarily think that those should be separated but it's interesting to think about um one thing is that they talk about world models a world model is kind of like okay in order to generate a hypothesis or a plan you have to have some understanding of how the world works with large language models like gpt3 the world model is built in um it's not a separate module that's why gpt3 and other large language models such as gbt neox and bloom are so powerful is because they are trained on so much data they have implicit knowledge of how the world works so you could say that the world model is built in or is unnecessary in those cases so when i see that they talk about a world model i kind of feel like they're not fully up to date on the state of the industry that's my opinion okay acting under uncertainty so this is a critical thing to talk about because humans we have always had we evolved to function with uncertainty um this is why we actually this is actually why we evolved to have magical thinking is because the reason that things happen is not obvious you know we have the bet you and i we have the benefit of thousands literally thousands of years of philosophy and science to understand the world when the seasons like when this when the when the the seasons change the days get shorter everything gets colder everything dies we don't really know like that's not intuitive as to why that happens so it could be that like the the origin stories of like you know this deity or that deity had a fight and they decided you know like persephone went down to hades and um and everyone was sad and then there's a resurrection like without science that's as good as of an explanation as you might get um you know ditto for why things decay why we get old why oh so um in in mythology um there's a whole lot of globally every um every society has is part of their cosmogonic cycle and explanation as to why there's two sexes um is fascinating to humans okay so that was a little tangent um yes we live in an uncertain world we have to assume that our agent will not know everything that is that is a good assumption to make because that is how the world works no matter how smart you or i are there's going to be stuff that we don't know assumption two an advanced agent planning under uncertainty is likely to understand the cost and benefits of learning and likely to act rationally according to that understanding oh this one's a doozy um i don't know that i agree with it uh and this is because for me one of the core purposes of creating an intelligent entity is to learn that learning should be a given um but okay uh you know but it and so here's the thing is if you still think of if you still think of learning as a something that needs reinforcement learning or needs a reward function rather than something that it's just intrinsically meant to do so let's go back to the example of a tractor motor it is intrinsically meant to ingest fuel and oxygen blow it up in you know contained spaces and generate torque on a shaft it can't do anything differently similarly human brains as long as you're alive you are learning um as long as you wake up and are conscious and engage with the world you are learning and so you could say that that is an intrinsic function of us now what this what this assumption to is that they're basically saying it can choose whether or not to learn um i don't think we should do that i think that learning should be automatic it should be a fully autonomic function and i talk about that in my book oh by the way symphony of thought this is the first print copy it should be available actually it might be available by the time you've watched this video so check in the comments for a link if you want to buy it on barnes noble it's also available for free on github as a digital digital file um yes so arbitrary reward sorry arbitrary reward protocols there we go try and say that 18 times fast um so all rewards are arbitrary right it's just a matter of what was it based on and are you measuring it correctly um for humans our rewards are all physiologically based so for instance if you're hungry and then you eat food you get you get a biochemical reward based on the quality of food that you ate and that's actually why we prefer what's called hyper palatable foods so stuff like pizza chicken parmesan spaghetti burgers you know stuff that has that is very high calorie dense usually has a lot of animal fats or dairy or sugar or carbs those are what's called hyperpalatable and that is your brain through tasting it and smelling it your brain identifies this is something we need a lot of eat as much of it as you can right now oh hang on my dog's outside i gotta go let him in he's barking oh there he goes he wanted to come hang out but then he got shy okay so we were talking about hyperpalatable foods um so a hyperpalatable food is an example of a reward um and this is because we evolved in a scarce environment um is it recording okay yes um we evolved in a scarce environment which means that the opportunity cost of eating high-value food basically dictated that if you have a chance to eat high-value food eat all of it because you might not get it again and so this is actually this actually triggers a feast reflex where you will your body will kind of like reduce the stomach pain and and increase gastric flows that says okay there's a lot of high value food eat all of it and that's why we tend to overeat especially um when we deliberately create feasts like at thanksgiving and christmas and whatever other holidays you celebrate now what happens though is that we get addicted to that way of eating because in in prehistoric times you would usually have to eat

CLEAR ESSAY:

==========

In order to generate hypotheses or plans, humans must understand how the world works. This understanding is known as a world model. World models can be built into large language models, such as GPT3, which makes them very powerful.

However, living in an uncertain world means that humans must always assume that they do not know everything. This is why we evolved to have magical thinking â€“ because without science, it is often not obvious why things happen.

 Acting under uncertainty is thus a critical skill. And an advanced agent planning under uncertainty is likely to understand the cost and benefits of learning, and act rationally according to that understanding.

This means that learning should be an intrinsic function of any intelligent entity, something that is autonomously done all the time. And arbitrary reward protocols should not be used to dictate learning, as they can often be based on incorrect or incomplete data.