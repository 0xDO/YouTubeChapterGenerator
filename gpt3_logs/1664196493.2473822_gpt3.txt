The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: all right so I think we're ready to go I'll get it running um so this is the stress test where basically I'm just going to upload it um and do it then and time it so we'll do um so we'll do upload data so start equals time and then batch size 256 and then we'll do print um uploaded in and then we'll do time minus start seconds okay um yep so I think that should be it go ahead and clear that out all right let me make sure this runs so we'll do python quadrant stress test and so now it's got to load a few thousand of uh of these um make sure that it it succeeds and doesn't bomb out and then we'll let it upload and I'll come back once it's done um so yeah spinning up 88 000 almost 90 000 records I'll go ahead and pause it and just show you when it's done you don't need to watch this all right got ran into a bug so it says IDs batch equals record dot ID for record in record batch uh dict object has no attribute ID so it's looking for a record object but I gave it the wrong thing Okay so quadrant client and upload records upload collection oh hang on okay we'll try this a different way um I'm switching to upload collection which worked with the quadrant populate so basically what you do is you just separate the vectors from the payloads and then you just upload them together so we'll give this a try and uh we'll go from there because yeah it blew up when um when I tried to so it started started the client created the collection and then as soon as it's tried to upload the records it blew up um also important note you do recreate collection and it nukes it and then starts starts from scratch so we'll give this another try and we'll be right back wow okay so I was wrong uh I thought that it would look for a uh a numpy array but it says it's not a valid list so it's just looking for a list so that's fine one thing that I noticed though is that storing these is an as a uh storing it as a numpy array was actually way more memory efficient so let's do this um so removing so it's no longer numpy array it's just vectors and payloads um so yeah let's see if this works oh boy we'll get there okay I'm hoping this is the this is the third time's a charm um so let me show you what I mean so we're at 65 66 it's going up fast we're already using 1.5 gigabytes 1.6 this will be two gigabytes of of ram just to load it um oh hey look it's uploading records now now we see the VM memory is going up slowly CPU is not going up that much let's see if it's using GPU um video in code the video encode actually could be what I'm doing um GPU memory is not going up so I don't think that I don't think it's that um but yeah so VM memory is slowly Rising as it's uploading the records but it worked it seems like it's uploading so we'll see how long it takes I have no idea how long it's going to take um but yeah so the the trick was it looks like this is the final version so recreate collect collection we're doing um Vector size 512 distance cosine ID is none so it'll figure out the IDS itself and we've got vectors and payloads separate so basically what it does is it just says okay each index or uh in in the list is going to match so you got a vector and then a payload and it'll match and you can have whatever information you want in the background all right so I'm gonna pause it well it should come back one last time um and we'll see how big it is let's see volumes volume okay so it's it's going up slowly 268. I have no idea how big this will be total because I've got um like 1.1 gigabytes um so we'll see how big this gets total um 286. all right so I'll pause it we'll be back we'll look at the final final numbers in just a minute okay it may or may not have finished because it didn't error out but it was giving me these these are just info level things um then this is from the docker container but then here let me minimize this a little confusing it says um uploaded in so 88 700 in 163 seconds so that's not too bad um so let's see result green vectors 88 788 okay it did it indexed vectors count seven seventy one thousand so I think it's still indexing in the background let's run that again segments six disk data size RAM data size I don't know why it's a zero Vector size distance um so let's see 7168 so I'm not I don't know what's going on in the background but we can look at um the container it's still running the volume is 393 megabytes I don't know how it stores it that seems like it's pretty efficient um but yeah so let's do a quick search so we'll do uh search so we'll copy this guy and we will do collection so it's not that it'll be um stress test um here we'll just do this um stress test search dot text all right so here's the vector top three um all right so we'll go grab a random embedding and then we'll come over here so the vector and we'll do um all right so we'll do slash white space so anything greater than two regular expression find next okay there we go so replace and we'll replace that with zero space so replace all [Music] right so there we go we've got we've now got a vector and so we'll replace that Vector with this guy all right I don't know this is gonna work um stress test Point top three oh yeah those are escaped okay so let's see if this works Json D serial into file well okay so it didn't like it I think it's too long hmm all right I'm gonna pause again and I'll use the client to do the search actually one second okay so this is really class watch this um yeah CLS search uh okay so point zero one four seconds and um so the score I picked a vector from the data and it found you know score 1.000 so it found the exact match um and then another score that was only half you know et cetera Etc so got the exact right one um and it pulled it all I didn't get the job you know Etc oh man that was that was really fast so it's search 90 000 records and gave it all back to me exactly as I put it in in .014 seconds so really fast all told it's using three and a half gigabytes of VM memory now keep in mind when

CLEAR ESSAY:

==========

The stress test is a way of timing how long it takes to upload a large amount of data. In this case, I am uploading 88,000 records. I start the timer, upload the data in batches of 256, and then print the time it took to upload the data.

I ran into a bug where it was looking for a record object, but I had given it the wrong thing. I switched to uploading the collection, which worked with the quadrant populate. This allowed me to separate the vectors from the payloads and upload them together.

The trick was to store the data as a numpy array, which was more memory efficient. I also had to recreate the collection and figure out the IDs myself.

The stress test was successful and I was able to upload the data in 163 seconds. The final version was using 7168 vectors and had a disk data size of 286. The volume was 393 megabytes.

I did a quick search and was able to find the vector I was looking for in .014 seconds.