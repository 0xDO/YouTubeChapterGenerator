The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody this is going to be a slightly different video from my normal um this is more of a research update um so i've done a lot of work recently with you know just showing different things that you can do with gpt3 but what i'm going to do now is i'm going to get back to my core purpose which is to work on artificial general intelligence or what i call artificial cognition i don't like the term agi i made another video about that artificial cognition is much more on the nose of what i'm doing so years ago i think it was 2017 or 2018 i cooked up this idea that i called maraghi which is um microservices architecture for robotics and artificial general intelligence now i kind of gave up on that idea because it was there was too much complexity at the time and also only gpt2 was available at the time which was not sophisticated enough then when gpt3 came out i basically created a simplified version of that called natural language cognitive architecture i wrote a book about that and you can actually get it online for free natural language cognitive architecture it's right here it's up on github and so here's the diagram for natural language cognitive architecture where there's a shared database that links up two loops um so that's that's great but now that now that one gpt3 has the fine-tuned models well fine tuning is out when i created natural language cognitive architecture fine tuning wasn't even available so a few things have happened one is fine tuning is now available the instruct series is available which makes it much lighter and faster um and then the third thing that happened is i have learned a lot i've read a lot of neuroscience and done a lot of experiments in the meantime so i'm revisiting uh microservices architectures um for for artificial cognition um and what i'm introducing today is kind of the overall architecture and i'm writing a book about this it's called symphony of thought um it's uh currently with its first uh beta reader actually and then i'm working on draft three and so on um so look for that book in the coming months you all know me i work very fast uh so anyways the nexus is exactly what it says it's at the heart of artificial cognition now what does it do it holds the stream of consciousness it holds all the memories um that happen in artificial cognition and basically what it is is um it's a vector database so this was another problem that that i had to solve was i could use a conventional backend like solar or sqlite or something but those are just not optimal for for associative memory search that you need for artificial cognition now enter vector databases so this is what i started with is vdblight so you've this is becoming more popular it's already got five stars on github and three forks i also created a discussion page because lots of people wanted to ask questions about it so there you go what vdblight does is it's basically sqlite but for vectors and it's not nearly as sophisticated so it's a very aspirational association there um but one thing that i'm going to do is that i'm going i'm already working on this is integrating it with um with feis whoops uh yeah so uh facebook ai search uh semantic search feis which is very fast very scalable um so with all that said um let's see where are we uh yeah so there's vdblight so what i'm doing is i'm going back to the microservices architecture idea and i and i created a version of vdblight that is a microservice it's a rest-based microservice that's called the nexus so what the nexus does is it allows for an arbitrary number of other microservices to participate so basically we compose artificial cognition we compose a brain and they all talk to the nexus and so the nexus has a very simple job um it receives messages from arbitrary numbers of microservices and then it allows those arbitrary those those microservices to then fetch messages to search for them right now there are two primary ways to search one is through associative memory or semantic similarity so vector you just do a cosine dot product of two vectors and it says here are the most similar vectors um because that is the primary way that human memory works so human memory works by by association and so if you see something that reminds you of something else your brain is like ah i've seen this before or if you hear something familiar smell something familiar you see a familiar face you walk into a familiar room associative memory is the primary way that human memory works now the other way that human memory works is that it is temporal so we don't have a clock in our head but we generally remember about how long ago something happened we also remember clusters of memory based on memories that happened coincidentally or at the same time so those are the two primary mechanisms of human memory and those are that is what i have replicated in um in not natural language cognitive architecture in the nexus or vdb light and so if you look at the endpoints in the nexus zoom in a little bit there is add so that's just you add a uh add a record um it just takes in a uh a vector and then you or it takes sorry a dictionary and the dictionary needs to have at the very least a vector and a time stamp if that's all you use it for that's fine but those are the two required fields so we'll see required time and vector um you can add we can have whatever else you want right it doesn't care and then there's search which so this is semantic search and there's three fields required for that vector field and count so that basically says what's the vector that you want to search for similarity which field is going to be in contain the vector i might remove this because like everything just has a vector um so we might just default to vector and then the number of results that you want because say for instance you have like you just want like you know you're searching for a wikipedia article or something in memory you don't need a thousand results but if

CLEAR ESSAY:

==========

In my last video, I showed different things that can be done with GPT3. In this video, I'm going to get back to my core purpose, which is working on artificial general intelligence, or artificial cognition.

I came up with the idea for microservices architecture for robotics and artificial general intelligence a few years ago. I abandoned the idea because it was too complex, but now that GPT3 is available, I've revisited the idea and created a simplified version called natural language cognitive architecture.

The nexus is at the heart of artificial cognition. It holds the stream of consciousness and all the memories that happen in artificial cognition. It's a vector database that allows for an arbitrary number of microservices to participate.

I'm currently working on a book about this called Symphony of Thought. In the coming months, I'll be releasing more information about the book and the microservices architecture.