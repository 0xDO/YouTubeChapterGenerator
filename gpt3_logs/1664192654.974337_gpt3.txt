The following is an unstructured video transcript. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: hey everybody david shapiro here um yeah so it's my first live stream it looks like there's four of you we'll see if any other books show up but yeah so here we are um do a quick volume check you all hear me give me a shout out in the chat if you can oh we're up to five um yes the dude with some 409 says hi hello okay yep says i sound good okay so yeah um i am at a thousand subscribers now which is pretty crazy uh considering i started youtubing seriously back in what april march something like that um and yeah so it's just by popular demand really um i did promise to talk about a couple different topics today but so we'll uh we'll actually see what folks talk uh ask about in the chat um because like this is well i don't know really i don't really know what i'm doing um it could be an ama it could be just a uh if someone has a has a topic i can talk for a long time about it um also i apologize if i'm a little bit stuffy hopefully that'll clear up as i start to talk actually here i know what i'll do i'll make a quick poll we will do a poll let's see ask a question okay what do you want me to talk about and then we'll say yes or no now um we'll say dolly or we can do gpt3 um and we can say like um lms in general um ai and futurism um there we go all right there should be a poll i don't know how to see it now oh there it is okay so i got a little poll going just to get a feel for what y'all want me to want to hear me talk about uh so far um so if you could then jump in the chat and hit me up there let me know um but yeah we just got started also i've got my uh that's because i'm i took a nap earlier and i don't know if i'm too hot or too cold okay so we've got llms in general oh we're getting all kinds of votes okay adrian says dolly okay um we're getting seems like there's there's some uh some desire to talk about lms in general okay i'll give it just another couple well i guess probably almost everyone's voted by now all right i'll go ahead and start talking about large language models um okay so large language models i'm assuming pretty much everyone here is gonna know that gpt3 is a large language model but what is a large language model so the first thing to know about a large language model is that it's big hey there you go but i'm all right everyone go home no um the thing is is that uh so it's a type of deep learning network and they they were originally um trained there's like all different kinds there's like lstms which is long short-term memories um and different things like that and basically this is as of like circa 2014 2015 2016. um what they were doing was just teaching them to predict usually the next letter or the next token or the next word um oh jordan has a good question um tell us how we got into this field excuse me so i'll tie that in to this answer um so circa 2014 2015 2016 google publishes um their universal sentence encoder um so it's if you if you ever hear me say like google use or google universal sentence encoder this is like an early like precursor to modern large language models so one of the things that we needed to do was we needed to figure out how to represent um language in uh in in math in numbers in computer code so all kinds of research had been done for many many years there's word net there's you know different types of embeddings um and so this was building on research of word embeddings and so word embedding is just representing a word with its semantic meaning as a as a series of numbers um and so i remember when i i didn't remember i don't remember the details of how i saw it but i was just like searching around for stuff and i saw google's universal sentence encoder and it's like oh you can you can represent any arbitrary sentence or paragraph as um as a vector as a string of numbers and i i immediately knew this will change everything um so experiment with it but of course um being able to render something as uh as a semantic vector or an embedding as we call it now it used to it used to be like a semantic or just a vector but now we call it an embedding um being able to render that uh would uh like it unlocks a lot of different things um but so like one of the first experiments that i did was with um like the precursor um to what now call my core objective functions or the heuristic comparatives which was um okay if i have like a sentence and it says like i think what did i test it was something really arbitrary and off the wall like i tested like you know if you like throw a kitten off a bridge or something that was obviously like egregious and heinous and i said like how semantically similar is that to like reduce suffering um and obviously this is a very early experiment it didn't work um the way that i thought that it would i think i tried to submit a supporter machine which is a type of clustering because i thought that oh oh like here's a high dimensional vector let's just cluster them together but i don't think that semantic vectors lend themselves to clustering at least not at the full dimensionality and certainly not the experiment that i did anyways because the idea was you have any arbitrary statement and then you can classify it is this a good behavior or a bad behavior that's what i was trying to do so that's kind of where i got started a couple of years later oh so the history of llms and then how i got into it so a couple years later gpt was they talked about that open ai published about gbt and then gp2 so gpt um generator generative pre-trained transformer didn't really like nobody nobody remembers that um but it was the same idea which was first instead of just rendering it to an embedding you then use that embedding to generate output so google's universal sentence encoder is like the first half of a large language model so that's the encoder side and then on the second half is the decoder which is produce output and i got another comment let's see um interested in how you come up with experiments i'm currently working on

CLEAR ESSAY:

==========

Hey everyone,

David Shapiro here. This is my first live stream, and it looks like there are four of you watching. I'll see if any other people join in.

So here we are. I promised to talk about a couple different topics today, but we'll see what folks ask about in the chat and decide what to focus on. This could be an AMA, or just a discussion about a particular topic.

Also, I apologize if I'm a little bit stuffy. Hopefully that will clear up as I start talking.

So we've got a poll going to see what people want me to talk about. It looks like the most popular topic so far is large language models (LLMs).

So what are LLMs? They are a type of deep learning network that was originally developed to predict the next letter, token, or word in a sequence (circa 2014-2016).

One of the things that made LLMs possible was the development of word embeddings, which is a way of representing words with their semantic meaning as a series of numbers. This made it possible to "encode" any arbitrary sentence or paragraph as a vector, which unlocked a lot of different applications.

One of the first experiments I did with word embeddings was to try to create a semantic similarity metric. I wanted to be able to take any two statements and compare them to see how similar they were. This was a very early experiment and it didn't work the way I thought it would.

But the idea was that you could have any arbitrary statement and then classify it as a good behavior or a bad behavior. That's what I was trying to do.

So that's kind of where I got started. A couple of years later, Google published their Universal Sentence Encoder, which was a major breakthrough in the field of LLMs. This made it possible to represent any sentence as a vector, which unlocked a lot of different applications.

So that's the history of LLMs and how I got into the field. If you have any questions, feel free to ask them in the chat.