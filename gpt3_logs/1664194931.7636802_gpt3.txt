The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: realize it was not muted okay no more distractions okay so once you get a whole bunch of questions and a whole bunch of contexts so here's the folder i accumulated all the context and then i accumulated a bunch of questions so you can see here like it generated questions i clean them up and so what you end up with is a pair of contacts which is a context is just a situation can be a reddit post oh here we go medical questions what is the patient's diagnosis what is the anesthesia used for so this basically the whole point of this was i wanted to create um a a curious model so a curious model increase understanding that is um core objective function number three curiosity is going to be an imperative uh quality so that we can have benevolent um artificial general intelligence and so the ability to have a model that will just automatically ask questions that was the purpose of this so we've got context so scraping together context that's the input that was a big big chunk so we got context and questions right and then i started working on summaries i didn't really do that that's fine um ultimately though this was the result and i know it's huge and super confusing so we will just say like okay so here's a prompt which was dialog and then i end it with questions so you need a dmarc and then i just copied those questions so you end up with a pair and let me actually copy this so i can show you a little bit better what it would look like to a person okay so let's clean this up just a little bit and i will zoom in bear with me there we go okay so let's remove let's find that find what backslash backslash replace with a regular backs a new line there we go okay so then we say this is what the input is going to look like and then so i'm basically i'm just reverse engineering what all my dataprep scripts did because when you fine tune it needs to be in a particular um it needs to be in a particular format and so the input is going to be this bit here and you and i what i what i always recommend doing is you you append at the very end kind of the the instructions and this will this will train the model that whatever came before it whether it's a medical text or a legal text or a reddit post this says okay time to ask questions and so then this is the output this is what i trained it to output and i because i was using a more primitive gpt3 model when i did this originally i had another cleanup script that removed anything that didn't have a question mark at the end of it and so the ultimate result is now i've got a fine-tuned model that is an expert at asking questions for all kinds of things so there you have it there's step one or experiment one all the data prep scripts are in here you can take a look at those there's also a few scripts out here like format the training questions generate the questions you can look at the prompts that i wrote for asking questions like predicting the future and whatever but really this one is what are the most important and salient questions that this wording turned out to be the best because it allowed gpt3 to be flexible it allowed it told gbt3 okay be a smart person and ask probing questions about this um oh that that term probing questions that can also be a really good prompt so yeah okay so that was that was synthetic data one come check out this repo if you want to the second one is going to be my core objective functions so i'll show you let's see to do i started the same way with actually no let me go to experiment one so i copy-pasted those contexts and i created a series of prompts and you can see this repo is a little bit simpler again this is publicly available you can come take a look at it yourself but i had prompts that were a little bit different so these are few shot prompts and what they did was discuss suffering in the following contexts so i gave it a context and then suffering context suffering context suffering and so then i would just replace the context with one from the internet from a data set and then i would ask gpt3 to produce some dialogue about the suffering and then the ultimate result was something like this where it says this passage indicates that the person has suffered from isolation and feeling trapped so again the goal here was to give agi the ability to understand human suffering because if your agi if your machine learning model has an intrinsic understanding of suffering well it can avoid suffering so that was the point of that experiment but again it followed the same process where you start with some raw data so step one get some raw data step two get a prompt and step three is record all of the output so then i did the same for these core objective function two is about understanding so increase understanding means that it wants to teach people it wants to ask questions that sort of thing and all those prompts are going to be um so here's uh here's some of the prompts um here's some other prompts and so there uh so on and so forth now i started working um i started working on downloading data directly from reddit so that that way i could just say okay let me go to a reddit download some some contexts from there um that i developed in future experiments so let's see i've got even more here download reddit posts yeah so here you go this is reddit is a great source some people are concerned about privacy and the legality of downloading posts the us supreme court and superior court have both recently reaffirmed that any any data that is publicly available on the internet is legal to use for pretty much any purpose and this is certainly not a malicious purpose so as far as i can tell the law of the land is settled you're allowed to use stuff that's publicly available on reddit um and that's as far as i can tell that's the end of the end of the conversation oh sorry one thing that i did forget to mention is the end result is always a json l file so in this case here you see the same same format there's a prompt and then

CLEAR ESSAY:

==========

In my first experiment, I created a model that would automatically ask questions based on a given context. To do this, I scraped together a bunch of context from various sources, and then generated questions from those contexts. I then used a script to clean up the questions, and format them into a pairs format that could be used to train the model. The end result was a model that could ask questions about a variety of topics.

In my second experiment, I focused on creating a model that could understand human suffering. To do this, I created prompts that discussed suffering in various contexts. I then used a script to replace the context with real-world examples of suffering, and ask the model to generate dialogue about the suffering. The end result was a model that could generate questions about suffering, and provide some insight into the causes of suffering.

In my third experiment, I focused on downloading data directly from Reddit. I developed a script that would download Reddit posts and then format them into a pairs format that could be used to train the model. The end result was a model that could ask questions about a variety of topics, and provide some insight into the Reddit community.