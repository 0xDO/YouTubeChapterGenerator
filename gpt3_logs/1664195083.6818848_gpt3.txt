The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: hello everybody david shapiro here for another video about raven today i'm talking about uh the solar uh application as well as the encyclopedia service so i've already shown uh another video about the encyclopedia service which is basically um wikipedia but it's running it was running on sqlite sqlite has a lot of limitations uh namely that it's just a relational database uh and it's also not meant for for large data sets solar however uh is a search engine so if you think about google google is the most familiar search engine to most people it has things like analytic and semantic understanding to help you with search functions so what solar does is it gives you that same power but it gives it to you locally so first i need to show you what solar is and what it does uh so this is this is docker desktop running my instance of of solar uh and actually i meant to bring this up um localhost solar admin there we go um okay so here's solar and solar is organized into cores uh and what what a solar core does is basically it's a core is is uh like an index database that's the simplest way to put it i'm not going to do a deep dive into solar if you're curious about how solar works there's plenty of material out there to look on it but i just wanted to show you that i've got 267 000 documents in this um in this index in this core and it it took let's see that's about 400 gigabytes i think or megabytes let me show you solar data so within data i've got the wiki core uh and it is 343 megabytes so solar is also very efficient um yep so there's that uh let's see go ahead and oops there we go okay so within the plain text wikipedia github repository i've now got let me minimize that there we go i've now got a few a few files so that's the point of today's video is i wanted to show you how all this works to bring it all together so let's go to the plaintext wikipedia directory okay so first i've got a batch file and this is this can be rewritten for for mac or linux and basically what this does is all it does is tell docker to run solar and what it does is it says use this directory for your data and then it says use you know use this nat so map port 8983 to port 8983 use the solar pre-create script and call it wiki so there you have it it's pretty simple and that causes this to run here and you can you can rename this whatever you want um so there's that and then going back to the directory so that's how you start solar uh it's pretty simple pretty straightforward and then let's see d wiki functions so this is really the meat and potatoes of of using wikipedia as a data source what i've done is i've created a python file that's got all of it separated out into individual functions so for instance i've got this one that says remove simple links and it's got the regex for for that then it's got compound links all other links pictures remove audio files so on and so forth the reason that i use regex or regex i i don't know i just say regex the reason i use regex is that it is way way faster than wiki text parser and html2 text so when you've got the full wikipedia is something like six and a half million articles if you use regex you can process about i don't know 20 articles a second if you use html to text and wiki text parser only it can take five to ten seconds per article uh and so you do the math it takes you know a couple hundred times longer to do it with using wiki text parser and html2 text so i'm rely heavily on regex your mileage may vary there's still a few things that i need to figure out primarily the tables wiki tables i have not figured out a good way to parse that data into something that is just plain text uh let's see so there's that i also created a second file just for uh for solar functions it's pretty straightforward all it does is if if you give it a payload if it's a single dictionary then it will send it to one endpoint and if it's a list it'll send it to another uh another key point is the um the the url uh the ural argument there commit within if you don't have that then then solar will just keep it in memory until you tell it to commit uh so i i say i give it five seconds to commit uh so that way it can keep up one thing that i found was that sometimes uh as using posts as fast as you can go sometimes solar needs a second and it'll it'll crash so rather than rather than have it uh just fully crash out i i gave it a while true loop and i gave it six tries so that if it if it failed after six tries then it would then it would bomb out but invariably it would usually go through after after one or two tries sometimes i saw it take three tries for the solar engine to catch up and the problem there was that i was i was sending in uh individual dictionaries at once if you send a larger payload uh but but slower uh it's entirely possible that it won't crash as often this is this is going to be subject to improvement um let's see so those are the two uh functions and then the start solar bat i guess that's that's pretty much it i just wanted to uh i just wanted to show show this uh here's here's the final test that i made you can see it's it's what four lines from d wiki functions import all uh here's the file name which is just a uh an export of the simple wiki uh simplified english and then i use this this function process file to solar i guess i can show you that function so right down here process file to solar you give it a file name it goes through the article opens the file and then goes through line by line i detailed in the previous video about parsing wiki wikipedia and that is that the exports the xml exports are too large to hold entirely in memory so you have to read them line by line fortunately the exports are organized in such a way that it's really easy to separate out each individual article every wikipedia article starts with this tag page and ends with this pack with this tag slash page and so you know if you see this you're at a new article and if you see this

CLEAR ESSAY:

==========

In this video, I'm talking about the Solar application and the encyclopedia service. I've already shown another video about the encyclopedia service, which is basically Wikipedia running on SQLite. SQLite has a lot of limitations, namely that it's just a relational database and it's not meant for large data sets. Solar, however, is a search engine. If you think about Google, Google has things like analytic and semantic understanding to help you with search functions. Solar gives you that same power, but it gives it to you locally.

First, I need to show you what Solar is and what it does. This is Docker Desktop running my instance of Solar. Within Solar, cores are basically like index databases. A core is like an index database that's the simplest way to put it. I'm not going to do a deep dive into Solar if you're curious about how Solar works there's plenty of material out there to look on it but I just wanted to show you that I've got 267,000 documents in this index in this core.

I've also got the wiki core within Solar data. The wiki core is 343 megabytes. Solar is also very efficient.

Going back to the directory, I've got a batch file that can be rewritten for mac or linux. Basically, what this does is it tells Docker to run Solar and use this directory for your data. It also says to use this nat so map port 8983 to port 8983 and use the Solar pre-create script and call it wiki. There you have it. It's pretty simple.

I also created a second file just for Solar functions. It's pretty straightforward. All it does is if you give it a payload, if it's a single dictionary, then it will send it to one endpoint. If it's a list, it'll send it to another.

One key point is the url argument within the commit function. If you don't have that, then Solar will just keep it in memory until you tell it to commit. I say to give it five seconds to commit so that way it can keep up.

One thing I found was that sometimes, as using posts as fast as you can go, Solar needs a second and it'll crash. Rather than have it just fully crash out, I gave it a while true loop and I gave it six tries. That way, if it failed after six tries, it would bomb out. But invariably, it would usually go through after one or two tries. Sometimes I saw it take three tries for the Solar engine to catch up.

The problem was that I was sending in individual dictionaries at once. If you send a larger payload, but slower, it's entirely possible that it won't crash as often. This is subject to improvement.

Those are the two functions and then the start Solar bat. I just wanted to show this. Here's the final test that I made. You can see it's four lines from d wiki functions import all. Here's the file name which is just an export of the simple wiki simplified english. Then I use this function process file to solar.

Process file to solar takes a file name and goes through the article line by line. I detailed in the previous video about parsing wiki Wikipedia and that is that the exports the XML exports are too large to hold entirely in memory. Fortunately, the exports are organized in such a way that it's really easy to separate out each individual article. Every Wikipedia article starts with this tag page and ends with this pack with this tag slash page. So you know if you see this you're at a new article.