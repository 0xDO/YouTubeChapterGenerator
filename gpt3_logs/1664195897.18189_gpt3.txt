The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody david shapiro here for my third video in the zero to python and gpt3 boot camp um what the heck are embeddings i get this question all the time it is by far the biggest hottest topic so this is why i'm doing it as episode three but before we get started i'm going to ask that uh you consider liking and subscribing this video and also jump over to my patreon to support me there if i get to enough support who knows maybe i can do this full time one day anyways let's go ahead and jump into the video what are embeddings first i need to give you a little bit of background so basically what an embedding is it's a vector so what is a vector let's just start from scratch a vector is any string of numbers in an array this is a vector here let me make this a little bit bigger so that you can see it okay so we'll do vector so hashtag vector let me set the language to python so that it looks right ta-da okay this is a vector oops this is a vector so the mathematical definition of a vector it is a one-dimensional matrix so i'll do aka one dimensional matrix or a list okay so that's what a vector is great so what then is an embedding so the difference between a vector and an embedding is mathematically they're the same but an embedding has semantic meaning um and if you want to take a deeper dive i'll have this link in in the description this is uh tensorflow.org which is made by google they really advanced this technology um a few years ago with universal sentence encoder this is the like the progenitor technology that allowed gpd3 to exist and they did this starting in i think about 2014. um anyways so if you want to deep dive here follow this link but uh just for the sake of this video i'll show you kind of a short version so embedding equals vector with semantic meaning um so older older nlp stuff like nltk would do like um word webs and stuff where like semantic meaning was always relevant to other words so like cat is a type of mammal for instance it's also a type of animal it's associated with pet but that was not very efficient those technologies are really old and uh they thought it was going to revolutionize nlp it certainly did a lot but it was not as flexible as neural networks are today so an embedding is just a vector with semantic meaning so how do we like what does that mean okay so let's say each position each index in our um in our vector has a meaning so we'll say um we'll do x and y so we're going to populate this this vector with just two values and we'll say that position x equals uh social power right so max of 1.0 min of negative 1.0 so we're basically going to make our own embedding um and then uh this is based on one of the original examples that google used to use position y equals gender gender or sex so we'll say max is 1.0 min of negative 1.0 so we'll say like one equals ultramasculine and negative one equals ultra feminine okay so we have this this two um this this one by two matrix and we're going to use it to represent a person so or semantic meaning so if we have a semantic meaning that is one by one and we say that social power is uh a maximum of one and and 1.0 is ultra-masculine right so what would this be so we can we know that okay so what who has maximum social power theoretical um so that sounds like an emperor so we'll say this is the padesha emperor of the known universe known universe uh dune reference there so maximum social power possible and also maximum masculinity i actually don't know if the if the emperor was maximum masculinity he might actually be closer like 0.5 so we'll just say that because like you think ultra masculine you think of like what if what if the emperor was like the rock um so then let's duplicate that we'll say uh 1.1 punisha emperor if he was dwayne johnson um and so there we have we have our first two embeddings um now okay so what if we do the other one what if we go the opposite way so like negative 1.0 that would be like a peasant right or actually probably someone who doesn't have free will so like a prisoner or something um and then we say like zero so this is like um someone with uh no free will or agency um and also gender neutral right so that's what a semantic meaning is now with gpt3 the smallest one if you go to their embeddings the smallest one ada has 1024 dimensions so what happens is these models are trained to break down semantic meaning into many many different dimensions and da vinci has 12 000 dimensions so here we are just we're just doing uh doing it with one or sorry two vectors of uh with two dimensions each so you know this is this is like super super simple all right so now you now you know the basic of what i mean when i say vector or embedding so again the only difference between a vector and an embedding is that an embedding has uh has semantic meaning um and each of those positions is somewhat abstract um okay so then what do you do with it though like how do you compare one of these to another i'm glad you asked so we're going to do a basic um we're going to do a basic similarity search we're going to do base a classification problem so let's start with oh first let me introduce um i've added import numpy as np so this is um a standard math package i'll say module for python and so when you do import something as something else this allows you to refer to it as shorthand so if i double click on that you see that numpy is used down here so the way to do the way to use these vectors is to compare them with a dot product um and then the dot product the higher the dot product the more similar the vectors are that's it it's that simple um so i've added this function it's super simple all it does is return the dot product um between these two vectors uh and then i've added the gp gpt3 embedding where you just pass it a string um and i've got the engine already set to text similarity eta um this will suffice for for many things especially if you're just doing like a single sentence um if you

CLEAR ESSAY:

==========

Embeddings are vectors that contain semantic meaning. This meaning is derived from the position of the vector in relation to other vectors. The dot product can be used to compare the similarity of two vectors. The higher the dot product, the more similar the vectors are.