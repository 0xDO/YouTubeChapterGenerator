Use the following subtitle transcripts to create YouTube chapters. Describe each minute as a chapter with a sentence or two.

TRANSCRIPT:
0:00:00.900,0:00:05.400 morning everybody David Shapiro here with your daily state of the industry update  0:00:06.000,0:00:11.820 as often happens my newsfeed helpfully supplied me with today's topic I think  0:00:11.820,0:00:17.700 it is a very timely topic because I have been diving more into alignment so today's paper  0:00:18.600,0:00:24.300 um it it's actually an older one January 5th 2021 but like I said my newsfeed supplied it to me  0:00:25.320,0:00:29.280 um it's a relatively short paper at least the part that's published is  0:00:29.280,0:00:32.700 uh 12 Pages it's I think it's much longer they just cut some out for  0:00:34.320,0:00:39.900 um for uh for internet publishing but the abstract of this paper is pretty good  0:00:41.100,0:00:46.200 um pretty straightforward super intelligence is a hypothetical agent that possesses intelligence far  0:00:46.200,0:00:52.380 surpassing that of the brightest and most gifted human Minds in light of recent advances in machine  0:00:52.380,0:00:56.220 intelligence a number of scientists philosophers and technologists have revived the discussion  0:00:56.220,0:01:02.160 about the potentially catastrophic risks entailed by such an entity in this article we trace the  0:01:02.160,0:01:06.780 origins and development of the Neo fear of super intelligence and some of the major proposals for  0:01:06.780,0:01:13.320 its containment we argue that total containment is in principle impossible due to fundamental limits  0:01:13.320,0:01:18.960 inherent income in Computing itself assuming that a super intelligence will contain a program that  0:01:18.960,0:01:23.760 includes all the programs that can be executed by a universal turing machine on input potentially  0:01:23.760,0:01:28.380 as complex as the state of the world strict containment requires simulations of such a  0:01:28.380,0:01:34.080 program something theoretically and practically impossible so to put this in other words in order  0:01:34.080,0:01:40.140 to anticipate what the machine is going to do you have to be able to simulate it perfectly including  0:01:40.140,0:01:47.280 the the way that it interacts and the way that the world reacts um basically the whole world  0:01:47.280,0:01:54.300 is too complicated to simulate with any accuracy and so therefore you know if this if this super  0:01:54.300,0:01:58.740 intelligent machine exists it's impossible to calculate what the rest of the world is going to  0:01:58.740,0:02:07.440 do in order to put it in a accurate simulation so we could at best do some you know approximations  0:02:08.940,0:02:18.000 um so skipping down a little bit they have two main classes for can controlling so the  0:02:18.000,0:02:24.240 the first class for controlling the machine is capability control you put it in a box you give  0:02:24.240,0:02:31.800 it uh you create dependence on a reward mechanism that is controlled by us you stunt it you run it  0:02:31.800,0:02:37.620 on inferior hardware and tripwire you you have it automatically shut down if it gets too dangerous  0:02:38.280,0:02:44.280 if you have a machine that can build a copy of itself or that understands programming it could  0:02:44.940,0:02:51.480 you know undo all of those um pretty much motivation selection so uh direct specification  0:02:51.480,0:02:57.480 you say you give it uh hard-coded laws uh Domesticity you teach it to behave within  0:02:57.480,0:03:01.860 certain certain constraints um indirect normativity you endow it with procedures  0:03:01.860,0:03:08.100 of selecting Superior moral rules so this is closest to what I have done in my work benevolent  0:03:08.100,0:03:15.240 by design whereby my proposal is that you give it a set of heuristic imperatives that it will  0:03:15.240,0:03:20.760 um it will ultimately buy into and deliberately choose so that you don't have to control it it  0:03:20.760,0:03:27.480 controls itself because it abides by certain principles that are going to be reliable  0:03:29.400,0:03:35.040 um I believe that this is with the experiments that I've captured in in benevolent by Design I  0:03:35.040,0:03:40.680 believe this is the way to go um augmentation at AI to a benign systems such as the human brain  0:03:40.680,0:03:47.280 so that's you know merging um okay so you can check out the paper if you want to look at their  0:03:47.280,0:03:54.540 discussion on that um but I wanted to go skip down to the um the discussion part uh and then  0:03:54.540,0:04:00.420 I'll I'll share kind of some of my not necessarily criticisms but my own counter thoughts because I  0:04:00.420,0:04:06.120 don't necessarily disagree with anything in this it's a short paper and it's just not quite as uh  0:04:06.120,0:04:10.740 not quite as robust because they're not proposing a solution like I have  0:04:10.740,0:04:16.140 um which is why I'm here okay so today we run billions of computer programs globally on  0:04:16.140,0:04:20.520 connected to machines without any formal guarantee of their absolute safety we have no way of proving  0:04:20.520,0:04:24.540 that when we launch an application on our smartphone our smartphones we would not trigger  0:04:24.540,0:04:29.760 a chain reaction that leads to transmission of missile launch codes that started nuclear war um  0:04:32.700,0:04:34.080 uh  0:04:35.640,0:04:43.320 as a technologist this really hurts um let's talk about firewalls let's talk about security  0:04:43.320,0:04:50.220 protocols you actually can mathematically prove something like that with penetration testing  0:04:51.600,0:04:57.360 um like there's all sorts of controls and constraints that go into every layer of a  0:04:57.360,0:05:03.300 piece of technology such as what that piece of technology can can talk to even the security of  0:05:03.300,0:05:08.820 how it boots up right like we have encrypted boot protocols that ensure that the operating  0:05:08.820,0:05:17.220 system hasn't been tampered with so uh

YouTube chapters for each minute with a good description:

00:00 - David Shapiro gives a short introduction to the topic of the day - super intelligence. He explains that there is a lot of discussion around the topic and that the purpose of the paper is to explore the origins of the Neo fear of super intelligence and some of the major proposals for its containment.
01:00 - Shapiro explains that the paper argues that total containment is in principle impossible due to fundamental limits inherent in computing itself. He goes on to say that in order to anticipate what the machine is going to do, you have to be able to simulate it perfectly, including the way that it interacts and the way that the world reacts - something that is impossible.
02:00 - Shapiro outlines the two main classes for controlling the machine - capability control and motivation selection. He explains that motivation selection is the more promising of the two, as it is less likely to be undone by the machine itself.
03:00 - Shapiro outlines the three types of motivation selection - direct specification, indirect normativity, and benevolent by design. He explains that benevolent by design is the closest to what he has done in his own work and that it is the most promising approach.
04:00 - Shapiro explains that augmentation of AI to a benign system such as the human brain is the most likely way to achieve a benevolent by design machine.
05:00 - Shapiro concludes by saying that we have no way of proving that when we launch an application on our smartphone, our smartphone wouldn't trigger a chain reaction that leads to transmission of missile launch codes that started nuclear war.




TRANSCRIPT:
0:13:43.200,0:13:50.520 on machine intelligence um okay then lastly the implicit Assumption of individual agency or what  0:13:50.520,0:13:56.460 we might call ego why do we make this assumption we cannot help but anthropomorphize the machine  0:13:57.120,0:14:05.400 so this is going to take a little bit more explaining but basically we humans are so used to  0:14:05.400,0:14:11.880 thinking of intelligent entities like ourselves that they fundamentally have a finite sense of  0:14:11.880,0:14:18.060 self just like us that they think in terms of I and me and this is what I want and this is what  0:14:18.060,0:14:25.980 I'm going to do so my very next video is going to be an experiment where I I test this I test  0:14:25.980,0:14:31.980 different agent models can we produce a machine that has a fundamentally different kind of agency  0:14:31.980,0:14:39.000 or a fundamentally different kind of ego or what I call an agent model so this is an agent model  0:14:40.800,0:14:48.060 um which is a an information system about the entity um so the information system about the  0:14:48.060,0:14:52.620 entity what I mean by that is like I know that I am a human with two hands two feet  0:14:52.620,0:14:59.040 and a brain I generally know what I know and what I'm capable of and I I also generally know what I  0:14:59.040,0:15:04.260 um what I'm not capable of right like I can't jump over my house um that's part of my agent model  0:15:06.240,0:15:12.120 um so to make it a little bit more specific a self-referential information system about the  0:15:12.120,0:15:23.340 entity um so like what kinds of agent models of agent models are possible do they have to be I  0:15:24.540,0:15:30.120 um anyways so I just wanted to set the stage my very next video will be about testing agent  0:15:30.120,0:15:37.860 models and seeing how that affects um the way that uh and uh how that affects alignment now  0:15:37.860,0:15:45.600 before I let you go there is one other thing that I wanted to um wanted to show you and this is this  0:15:45.600,0:15:52.800 is much more recent so 36 alarming Automation and job statistics our robots and um let's go  0:15:52.800,0:15:59.820 away machines and AI coming for your job so this is this is a uh more recent and it's from zippya  0:15:59.820,0:16:08.700 you know take it with a grain of salt um since 2000 at least 2600 or sorry 260 000 jobs have been  0:16:08.700,0:16:14.400 lost in the and the US due to automation so two percent of the country's manufacturing Workforce  0:16:14.400,0:16:20.520 and they are only increasing exponentially again take it with a grain of salt automation is also  0:16:20.520,0:16:26.460 predicted to create 58 million new jobs though automation could eliminate in the result of 73  0:16:26.460,0:16:31.920 million jobs so we're at where the the the the the yield curves to borrow a finance term have  0:16:31.920,0:16:37.980 inverted where yes automation is creating new jobs but it's creating

YouTube chapters for each minute with a good description:

==========

00:13 - Shapiro explains that the implicit assumption of individual agency is that the machine has a finite sense of self, just like us. He goes on to say that this is an anthropomorphism that we cannot help but make.

01:00 - Shapiro explains that the anthropomorphism of the machine is due to the fact that we humans are so used to thinking of intelligent entities like ourselves. He goes on to say that this is why we make the assumption that the machine has a finite sense of self.

02:00 - Shapiro outlines the different types of agent models that are possible and explains that a self-referential information system about the entity is the most likely to be successful.

03:00 - Shapiro explains that his next video will be an experiment testing different agent models to see how they affect alignment.

04:00 - Shapiro concludes by showing a recent study that found that automation is predicted to create 58 million new jobs, but could also eliminate 73 million jobs.