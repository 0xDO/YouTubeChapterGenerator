The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: pre-training or token pruning we leave this for future work to explore conclusions in this paper we introduce Hydra it can work in tandem we've taken the first step in showing that Hydro tension can work at all and hope future work can explore this in more token intensive domains okay so what does this mean one thing that I suspect that this means is when you look at the difference between other Transformers such as Google's Universal sentence encoder version four and five and it suddenly becomes much much faster and computationally and expensive that means that it is ready for Universal deployment now it sounds like this is not quite ready for Universal deployment but it is a big step in that direction now imagine if all image based Transformers can suddenly become this much faster and this much more efficient imagine if your Dolly and mid-journey and stable diffusion Generations instead of taking 15 seconds to 60 seconds they take half a second or a tenth of a second and then so that's just image but if you look at the uh on r slash machine learning where the author post one of the authors posted this someone asked if this could apply to language models and they said it probably could so imagine if this allows us to greatly expand the window size of gpt3 and other language models What If instead of you know uh 2000 tokens or 4 000 tokens this allows us to go up uh in window size by a factor of 10 or 100 or alternatively what if it accelerates um the the inference instead of taking four to five seconds per per uh uh per uh inference what if it takes 0.4 seconds or a few milliseconds and so then if you have a language model that can perform at human ability but it Ultra like superhuman speed then like even if it's only human equivalent but it's many times faster than people that's still like superhuman because it will do much more work in the same amount of time as a human now they're still energetically less efficient but the fact it that they're faster and they can work tirelessly kind of might bring that down to parity with uh with humans because uh when you talk about energetic efficiency you have to also think about time because if something takes you like years to do you know your your brain was using 20 watts over that entire period of time but if the machine can do it in an hour even if it uses a lot more energy during that hour it still might be energy you might reach energy parity sooner so taking a big step back this is when I when I frequently talk about like oh we'll find more efficiencies this is the kind of paper that I'm talking about where um because this space is so new and huge and it's growing all the time we are constantly finding new efficiencies new algorithmic efficiencies um so one of the things that started this whole thing was uh was an advancement in understanding loss functions I think that was back in 2014 or 2015. that's what kicked off this entire Renaissance of deep learning was figuring out how to optimize larger and larger networks and so that one little algorithmic Improvement um and then you know subsequent Downstream improvements enabled everything that we're seeing in artificial intelligence today and then and then seemingly small algorithmic improvements like this which are just small structural differences right like let's use as many heads as possible Let's do an experiment oh hey look we cut we cut the uh the cost down from 58 to 0.02 percent that is a gravimetric shift that is a seismic shift in the computational uh cost of some of these things and as a systems engineer it doesn't surprise me um you know it's like if you have a single threaded application or you have it running on 300 threads right if you can if if you can divide the work up like that go for it so in computation there are four food groups there's CPU or processing we also have GPU so whatever you have you have a machine that does the processing that does the math you have memory then you have storage and network so you have those four basic food groups of computation now for a model that runs on a single GPU you're going to be constrained by that that gpus processor speed and its memory but depending on how big it is you might have to be reading and writing to Storage storage is a thousand times slower than memory and network is is a thousand times slower than that right so if at all possible you want to keep all all uh computation bouncing back and forth between your processor and your memory and with a GPU you have those things combined into one on a single card right now if you have to switch to normal CPU and RAM because GPU has vram right it's got its own built-in uh video RAM so you keep it there but then if you have to switch to CPU in memory you have a slower CPU that's not purpose built for those Matrix operations and then you also have general purpose system Ram which is usually a little bit slower right but then if you're doing really big things and you have to write to disk or swap or whatever that's going to be much much slower and so then you'll end up like so in terms of speed GPU is the fastest by far CPU and RAM is the next uh next one down and then if you have to swap to disk that's even slower but then if you have to share over networks granted you can have specialized network connections to Cluster gpus but if you don't have that kind of specialized Network clustering going going over the network is even slower because then if you don't you're not accessing storage on your local system you're having to package up everything communicate out on a network protocol share data and every all the overhead that networking provides now granted networking can be very fast fast but you know your fastest back plane might be 10 gigabits a second between one host and another whereas you know you might have a thousand gigabits a second between your local storage if you're using ssds so again right you know it's 100 that's a factor of a hundred times slower throughput

CLEAR ESSAY:

==========

In this paper, we introduce the Hydra transformer, which can work in tandem with other transformers to greatly reduce the computational cost of transformer operations. We show that Hydra is able to significantly reduce the cost of transformer operations in a variety of domains, and hope that future work will explore this in even more token-intensive domains.

This is a big step forward in making transformers more universally deployable. Previously, transformers have been very computationally expensive, making them impractical for many applications. However, Hydra's efficiency means that transformers can now be used in a much wider range of applications.

This is just one example of the many algorithmic improvements that are constantly being made in the field of artificial intelligence. As deep learning continues to grow and evolve, we are constantly finding new ways to make it more efficient. These improvements enable the development of ever more powerful AI applications, which can have a profound impact on our lives and the world around us.