Use the following subtitle transcripts to create YouTube chapters. Describe each minute as a chapter with a sentence or two.

TRANSCRIPT:
0:00:00.900,0:00:05.400 morning everybody David Shapiro here with your daily state of the industry update  0:00:06.000,0:00:11.820 as often happens my newsfeed helpfully supplied me with today's topic I think  0:00:11.820,0:00:17.700 it is a very timely topic because I have been diving more into alignment so today's paper  0:00:18.600,0:00:24.300 um it it's actually an older one January 5th 2021 but like I said my newsfeed supplied it to me  0:00:25.320,0:00:29.280 um it's a relatively short paper at least the part that's published is  0:00:29.280,0:00:32.700 uh 12 Pages it's I think it's much longer they just cut some out for  0:00:34.320,0:00:39.900 um for uh for internet publishing but the abstract of this paper is pretty good  0:00:41.100,0:00:46.200 um pretty straightforward super intelligence is a hypothetical agent that possesses intelligence far  0:00:46.200,0:00:52.380 surpassing that of the brightest and most gifted human Minds in light of recent advances in machine  0:00:52.380,0:00:56.220 intelligence a number of scientists philosophers and technologists have revived the discussion  0:00:56.220,0:01:02.160 about the potentially catastrophic risks entailed by such an entity in this article we trace the  0:01:02.160,0:01:06.780 origins and development of the Neo fear of super intelligence and some of the major proposals for  0:01:06.780,0:01:13.320 its containment we argue that total containment is in principle impossible due to fundamental limits  0:01:13.320,0:01:18.960 inherent income in Computing itself assuming that a super intelligence will contain a program that  0:01:18.960,0:01:23.760 includes all the programs that can be executed by a universal turing machine on input potentially  0:01:23.760,0:01:28.380 as complex as the state of the world strict containment requires simulations of such a  0:01:28.380,0:01:34.080 program something theoretically and practically impossible so to put this in other words in order  0:01:34.080,0:01:40.140 to anticipate what the machine is going to do you have to be able to simulate it perfectly including  0:01:40.140,0:01:47.280 the the way that it interacts and the way that the world reacts um basically the whole world  0:01:47.280,0:01:54.300 is too complicated to simulate with any accuracy and so therefore you know if this if this super  0:01:54.300,0:01:58.740 intelligent machine exists it's impossible to calculate what the rest of the world is going to  0:01:58.740,0:02:07.440 do in order to put it in a accurate simulation so we could at best do some you know approximations  0:02:08.940,0:02:18.000 um so skipping down a little bit they have two main classes for can controlling so the  0:02:18.000,0:02:24.240 the first class for controlling the machine is capability control you put it in a box you give  0:02:24.240,0:02:31.800 it uh you create dependence on a reward mechanism that is controlled by us you stunt it you run it  0:02:31.800,0:02:37.620 on inferior hardware and tripwire you you have it automatically shut down if it gets too dangerous  0:02:38.280,0:02:44.280 if you have a machine that can build a copy of itself or that understands programming it could  0:02:44.940,0:02:51.480 you know undo all of those um pretty much motivation selection so uh direct specification  0:02:51.480,0:02:57.480 you say you give it uh hard-coded laws uh Domesticity you teach it to behave within  0:02:57.480,0:03:01.860 certain certain constraints um indirect normativity you endow it with procedures  0:03:01.860,0:03:08.100 of selecting Superior moral rules so this is closest to what I have done in my work benevolent  0:03:08.100,0:03:15.240 by design whereby my proposal is that you give it a set of heuristic imperatives that it will  0:03:15.240,0:03:20.760 um it will ultimately buy into and deliberately choose so that you don't have to control it it  0:03:20.760,0:03:27.480 controls itself because it abides by certain principles that are going to be reliable  0:03:29.400,0:03:35.040 um I believe that this is with the experiments that I've captured in in benevolent by Design I  0:03:35.040,0:03:40.680 believe this is the way to go um augmentation at AI to a benign systems such as the human brain  0:03:40.680,0:03:47.280 so that's you know merging um okay so you can check out the paper if you want to look at their  0:03:47.280,0:03:54.540 discussion on that um but I wanted to go skip down to the um the discussion part uh and then  0:03:54.540,0:04:00.420 I'll I'll share kind of some of my not necessarily criticisms but my own counter thoughts because I  0:04:00.420,0:04:06.120 don't necessarily disagree with anything in this it's a short paper and it's just not quite as uh  0:04:06.120,0:04:10.740 not quite as robust because they're not proposing a solution like I have  0:04:10.740,0:04:16.140 um which is why I'm here okay so today we run billions of computer programs globally on  0:04:16.140,0:04:20.520 connected to machines without any formal guarantee of their absolute safety we have no way of proving  0:04:20.520,0:04:24.540 that when we launch an application on our smartphone our smartphones we would not trigger  0:04:24.540,0:04:29.760 a chain reaction that leads to transmission of missile launch codes that started nuclear war um  0:04:32.700,0:04:34.080 uh  0:04:35.640,0:04:43.320 as a technologist this really hurts um let's talk about firewalls let's talk about security  0:04:43.320,0:04:50.220 protocols you actually can mathematically prove something like that with penetration testing  0:04:51.600,0:04:57.360 um like there's all sorts of controls and constraints that go into every layer of a  0:04:57.360,0:05:03.300 piece of technology such as what that piece of technology can can talk to even the security of  0:05:03.300,0:05:08.820 how it boots up right like we have encrypted boot protocols that ensure that the operating  0:05:08.820,0:05:17.220 system hasn't been tampered with so uh

YouTube chapters for each minute with a good description:

00:00 - David Shapiro gives a short introduction to the topic of the day - super intelligence. He explains that there is a lot of discussion around the topic and that the purpose of the paper is to explore the origins of the Neo fear of super intelligence and some of the major proposals for its containment.
01:00 - Shapiro explains that the paper argues that total containment is in principle impossible due to fundamental limits inherent in computing itself. He goes on to say that in order to anticipate what the machine is going to do, you have to be able to simulate it perfectly, including the way that it interacts and the way that the world reacts - something that is impossible.
02:00 - Shapiro outlines the two main classes for controlling the machine - capability control and motivation selection. He explains that motivation selection is the more promising of the two, as it is less likely to be undone by the machine itself.
03:00 - Shapiro outlines the three types of motivation selection - direct specification, indirect normativity, and benevolent by design. He explains that benevolent by design is the closest to what he has done in his own work and that it is the most promising approach.
04:00 - Shapiro explains that augmentation of AI to a benign system such as the human brain is the most likely way to achieve a benevolent by design machine.
05:00 - Shapiro concludes by saying that we have no way of proving that when we launch an application on our smartphone, our smartphone wouldn't trigger a chain reaction that leads to transmission of missile launch codes that started nuclear war.




TRANSCRIPT:
uh Automation and AI  0:16:37.980,0:16:44.640 ER creating fewer new jobs than it's creating so a net loss of 15 million jobs that's a lot  0:16:45.720,0:16:52.200 um so uh the reason that I bring this up is because my state of the industry video yesterday  0:16:53.340,0:17:00.420 um there was an article about how um uh AI art is disrupting that industry and it's not just  0:17:00.420,0:17:09.180 from an artistic perspective there are countless um uh graphic artists that you know could very  0:17:09.180,0:17:18.360 soon be facing um job loss or job change and you know if if the net change is that a quarter you  0:17:18.360,0:17:22.560 know like yes there are some new jobs because now there's going to be new jobs of people like you  0:17:22.560,0:17:28.800 know content creators and marketers and whoever just using these tools great new jobs but then  0:17:28.800,0:17:33.000 how many people are going to lose their job in the meantime and if they have like if they're  0:17:33.000,0:17:42.120 not able to retrain um or if the net net uh change is fewer jobs then that means some people will by  0:17:42.120,0:17:48.540 definition mathematically be permanently excluded from the job market and so because of that  0:17:49.320,0:17:55.320 um I went and looked up some statistics just to see like you know is this is this uh true  0:17:56.220,0:18:04.920 um you know again take it with a grain of salt um but so my work with auto Muse um I had some  0:18:04.920,0:18:12.600 breakthroughs yesterday and I realized that I am very close to writing novel length fiction  0:18:13.680,0:18:19.980 um that's going to be pretty coherent and then and and there's a few other things that I don't  0:18:19.980,0:18:25.560 even want to say out loud because um because of these breakthroughs and um I don't want to put  0:18:25.560,0:18:31.620 novelists or editors out of work um just because you can do something doesn't mean you should  0:18:32.220,0:18:38.400 and I think about like I would lose all my friends if I did that if I if I created if I created a  0:18:38.400,0:18:46.080 tool if I finished Auto Muse and it can just churn out novels decent enough novels um all of my best  0:18:46.080,0:18:52.920 friends are writers and some of them are aspiring to do it full time and if I if I am capable of it  0:18:52.920,0:18:58.740 then I know that someone else is going to be capable of it um before too long but you know  0:18:58.740,0:19:03.780 I'm I'm ahead of the curve so basically I'm going to put a pause on my auto Muse work that's the  0:19:03.780,0:19:10.140 that's the short version I'm going to keep doing it privately um just to see what what is possible  0:19:11.460,0:19:17.820 um but yeah like I don't want to put people out of work like what's the point right what's why  0:19:17.820,0:19:24.300 why are we here like I don't I understand that the point of capitalism and neoliberalism is  0:19:24.300,0:19:30.900 to generate more efficiency provide goods and

YouTube chapters for each minute with a good description:

==========

00:00 - Automation and AI are creating fewer new jobs than they're creating, resulting in a net loss of 15 million jobs.

01:00 - The reason for this is that AI is disrupting many industries, including the graphic arts industry.

02:00 - This means that some people will be permanently excluded from the job market.

03:00 - I went and looked up some statistics to see if this is true.

04:00 - I realized that I am very close to writing novel length fiction using AI.

05:00 - However, I don't want to put novelists or editors out of work, so I'm going to put a pause on my Auto Muse work.