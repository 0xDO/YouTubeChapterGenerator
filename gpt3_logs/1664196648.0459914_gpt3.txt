The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: you also can't reward a machine unless it's all fake right like you just say oh you give it like a virtual piece of candy for good behavior but any intelligent machine is going to realize that that's that's the method that it works on like okay what what loss function are you going to give to a machine that it's going to actually choose to value right and so then you get into the question like okay there's inner alignment so inner alignment is is the machine solving the problem that you think it's solving mathematically then there's outer alignment so outer alignment is does that machine's objective function or loss function align with the true the truth of of of its external reality right and so inner alignment is you know like okay you're trying to to optimize for whatever loss function is that loss function correct is it actually mathematically you know solving the problem you think it is internally like you know loss function is is this horizontal yes or no like not not horizontal horizontal right that's you know okay are you measuring the right thing because if you're not measuring everything it might end up like this it's horizontal but you wanted it like this right so inner alignment is you know you got to measure all three axes instead of just one or two and so that would be an example of inner alignment but then outer alignment is why the hell do you need a book that's flat and floating right there's no there's no objective value of having that so what you actually need for outer alignment is not how is the book oriented but is it readable right so that is an example of the difference between inner alignment and outer alignment um okay so went down many rabbit holes it's also been an hour so i might call it a day um if there's anyone on chat i see we we we're bouncing between seven and eight folks so if there's any last questions go ahead and ask them now otherwise i'll call it a day and we'll uh well yeah say it's a successful live stream let's see back on our simulation masters so this is some rando um maybe it has less to do with our biology and more to do with the ideas we come up with who knows what any species might come up with um with an alternate history that's a possibility okay so if we are in a simulation and the point is curiosity or scientific curiosity maybe it's not our genes or you know our phenotype that they're trying to figure out maybe it's they're trying to create situations for us to solve problems so what if what if we are actually in a simulation for another species that is facing climate change and that is our purpose maybe our purpose is to solve climate change or something that's going to happen in a million years from now right maybe maybe we're in such a long running simulation maybe it's billions of years right maybe there's a species that is running a simulation to figure out what to do to prevent their sun from burning out or to solve climate change or something so that's that's a great idea um let's see let alone wholly different species operating under the same laws of physics they might literally just be seeking inspiration the first step on the scientific method is the hardest to quantify the hypothesis yeah so um i that actually seems highly plausible because one thing that i have found particularly that gpt3 is really good at is brainstorming ideas so you know if we are in simulation maybe it is just about coming up with ideas or hypotheses in order to test and then of course like okay come up with an idea to test it and then go test it that's actually probably going to be one of the first things that i do when i get a fully functioning meragi working is because what i do is i put it in a text-based simulation and then i run an experiment so right now i just have a a kind of an entertaining comical experiment where moragi sees two men playing chess in central park and then it just kind of watches what they do um but when you have text language is infinitely flexible i could put i could i could create a simulation environment for moragi where it can do anything where it's it's on a spaceship it's on a generation ship i could put it in a world where there's different physics i could put it in a world where it is an all-powerful machine that has to solve climate change and see what it does so we're already very close to running um intelligent machines inside simulation to see what they do so yeah that's actually a great point you know maybe maybe on that line of thought maybe our point maybe our purpose is to create intelligent machines um and find the right objective function for them um that's another possibility if we are living in a simulation let's see okay old zany says i wish you luck you're awesome thank you um ideas might be the most valuable currency this in the simulation multiverse yeah um let's see rory o'connor says a lot of developmental research posits kids shouldn't be rewarded for good behavior but instead learn the intrinsic enjoyment of work is this the same reward system um just longer time horizon um so i'm really conflicted on that because um children do need a lot of things one thing that children need is they need to have a solid self-esteem which comes comes from love being given without any strings attached or unconditional so children need unconditional love which is rewarding now that being said children also do better in the long run if they're given chores so how do you balance that right you know is it a reward versus a punishment because chores are no fun but children who do chores they make more money they're happier they're more resilient against stress and they're smarter right so chores are really good for kids um and but so is boundaries right um teaching a children teaching a child boundaries makes them better citizens makes them better partners all kinds of stuff later on at the same time another thing that makes children very healthy is unconditional love so um i guess the short answer rory is there's

CLEAR ESSAY:

==========

It is important to consider both inner and outer alignment when training a machine. Inner alignment is ensuring that the machine is solving the problem you think it is, while outer alignment is making sure the machine's objectives are aligned with the true reality it exists in. Neither can be ignored if you want the machine to be successful.

If we are living in a simulation, one possibility is that our purpose is to solve problems for another species. Another possibility is that we are in a simulation to create intelligent machines and find the right objective function for them. Both of these ideas are worth exploring further.