The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody david shapiro here we are going to finish our long-term chat bot uh long-term chat with external sources um i was working on it and uh i was just cleaning up a couple things and i realized i was getting carried away so i was like let me just start recording so let me show you what i did before i started recording um just so that you're up to speed uh one thing that i did was i realized that my old lines um and my recent conversation i was passing the whole object which which was a list of dictionaries that included the line and the vector and that's no good in order to populate the actual text block the convo block i needed just the lines so i fixed that right here and then i also fixed it up here where instead of just returning the whole ordered item i returned i returned you know i've i lined for i and ordered so that's a pythons list comprehension um okay so that's what i did there and then i started working on a prompt because what i what i realized last time is that my fine-tuned model so let me show you the fine-tuned model that i use for tutor unfortunately i don't think is going to work as is because it can't really integrate new information so without any without any extra prompting so you see like okay this works just fine user um who was the last emperor oh wait yeah tim that would be marcus aurelius i don't think that's true there we go um okay so then what i did was i wanted i wanted to have the ability to search um an external source so in this case romulus augustus to get information right and so um i wanted i wanted this to i wanted to have the ability to pull from external data sources because sometimes um you know like gpt3 doesn't know what it's talking about like in in this case it just gave me like marcus aurelius because that's a popular roman emperor um so i was just like i probably shouldn't have deleted that because it's like uh no that's actually not true so if we go to here and say like last emperor of rome last emperor of roman so yeah uh yeah so we can't rely on it so this is actually a prime example um of of exactly what i'm trying to fix or account for so what we would do is uh we would take this and so let me just copy and paste this save that and um so then we say uh we go back to my prompt here so i had a few prompts in not the tutor chat but the long term um prompt wikipedia read the following conversation recommend a single relevant wikipedia article so basically what's going to happen is let me move this it's blocking a little bit of my site i hope you can still hear me just fine so we'll pass in our conversation and then we'll ask gpd3 to recommend a wikipedia article um that's supposed to only give one but that's okay um so if we go and search wikipedia for this list of roman emperors so it'll probably just return the first one that's fine um and then if we take this and ask like who's the last one there are quite a few aren't there um constantine i guess that's the eastern roman empire i like how their portraits change very persian um macedonian dynasty yeah oh that goes way up that's the end of the 11th century 80 or ce jesus modern heraklion dynasty okay anyways sorry i'm getting distracted um point being is we like we ask uh um let's do single most relevant wikipedia article list of roman emperors perfect okay so let's actually update our uh our prompt here okay so then that will allow it to answer answer our questions a little bit better right because it gave us the western roman empire but it didn't really differentiate um so it's like okay well is that really the best that we can do and the answer is no all right so we uh we have our conversation um and so this is just relying on gpt3 to just spit out information without thinking about it so in human terms this is like you're relying on your knee-jerk reaction or your intuition to just spit out an answer without really thinking about it but that's not good enough right i'm working on cognitive architectures so how do we make a machine that'll actually think about the answers that it's giving you that means we need recall we need so recall has two primary sources so there's episodic recall so episodic memory or episodic recall is my story of my life if i remember what i did and said that is episodic recall if i remember hey you said x y z to me that is episodic memory um and in many respects uh humans we trust our episodic memory more than anything else because that is the experience that we have lived and so episodic memory is what we're doing when we recall the most recent conversations uh most uh um the most uh relevant uh lines of dialogue so this is an this is episodic recall but then there's um what's called declarative knowledge or declarative memory so declarative memory is i learned this fact right like i've never been to rome i never saw ancient rome i've never met any roman emperors but i know or i think i know that um you know like marcus aurelius was a roman empire that is declarative knowledge so what we're doing here is we're trying to bring episodic memory and declarative knowledge into this cognitive architecture into this chatbot okay now that i've said that i think you understand um why i'm doing this so now that we're all oriented and maybe you knew this all along and that's fine so i apologize um oh darn okay so the fine tune model can't integrate this external information so i'm basically going to need to start over so i just copy the same prompt that i used to create the original um uh fine-tuned data for for tim the tutor um so just imagine a long text message uh chat log between a tutor and a user the tutor is tim who encourages curiosity and uses compassionate listening tim answers questions with thorough academic responses and offers follow-up questions or suggestions to spur curiosity tim maintains a cool professional tone so um basically what i want to do is is figure out how do we integrate um past converse whoops i did not mean to open my

CLEAR ESSAY:

==========

One issue with chatbots is that they often rely on pre-trained models that can't easily integrate new information. This means that the chatbot may not be able to provide accurate responses to new questions.

I'm working on a cognitive architecture for chatbots that aims to address this issue. The architecture relies on two primary sources of information: episodic memory and declarative knowledge.

Episodic memory is the story of our lives. It's the sum of all the conversations we've had and the experiences we've had. Declarative knowledge is the sum of all the facts we know.

The goal of the cognitive architecture is to integrate these two sources of information so that the chatbot can provide more accurate responses. To do this, the chatbot needs to be able to recall past conversations and integrate new information from external sources.

The cognitive architecture is still a work in progress, but I think it has potential to improve the accuracy of chatbots.