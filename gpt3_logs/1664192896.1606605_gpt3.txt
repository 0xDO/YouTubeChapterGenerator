The following is an unstructured video transcript. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: is we get we get embeddings from open ai and this one i did with ada we'll probably do babbage or something well no just for search i think eight is probably fine so we'll copy this function um say all right we'll get we'll grab a gpt3 embedding we'll also go ahead and grab the similarity function because it's just a real simple numpy where you do you give the dot product of two vectors and then the dot product gives you how similar those two vectors are it's super simple um okay search index let's see oh yeah so this is this was the function that i wrote to actually search it um so we will copy this function as well um so basically what this function does is you give it a text that you're going to try and match and then you pass it the index which is in this case it's it's a what was it let me make sure i'm remembering this correctly so search index um the nexus index update index yes um and update index how was that made so we list all the files and then we okay yeah so this is all it is it's a list of uh dictionaries where you've got a file name and a vector so that's it um okay so we'll probably do it a little bit differently where i'll just have the whole thing in memory and instead of having a file name i'll just have the chunk of text so it'll all be in memory um actually no it should be it should be on a file so that way it can be saved okay so the first thing we're gonna have to do here is build our index so let me also just go ahead and copy the update index file um or not file function sorry okay so and we'll rename this to just build index i had i had this function as an update index because this was supposed to be for an artificial cognitive entity which means it has to index its memories as they're accumulated so that's an even more complicated problem okay still following along good okay so we've got chunks text wraps so we've got all the chunks the result there we go that's fine um open file we don't need to do this uh we don't need we don't need we don't need to run it through a prompt all we need to do is get the um get the embedding now embeddings if i remember correctly you can get multiple embeddings at once i think open a open ai embeddings um let's see take a look at it input simple sample document goes here because if we've got like 150 different you know chunks to um to to do all right i'm not seeing anything that does multiple embeddings i'm not going to worry about it right now we'll just do one embedding at a time that's fine um and actually i don't even think we'll need the search in this one so i can probably take that out okay so we'll comment this out for chunk in chunks we actually just need the embedding so we'll say gpt3 embedding so content yes embedding equals gpg3 embedding and then we'll do chunk dot encode well here we'll just copy this so what i had to do here um this particular bit um this i found that sometimes there are unicode characters that gpt3 cannot handle and it errors out and so what i found what i started doing is just adding this little bit of um code to to change it so you're basically encoding it from unicode to ascii which is simpler and then you decode it back into just a regular string variable and that seems to prevent any gpt3 problems um okay so we got the embedding and we don't need a summary because essentially an embedding is a type of summary um it's just it's a numerical summary okay so what we're going to do then is instead like what we did here where it's it's just alluding to a file name because with an acog here let me just show you um with an artificial cognitive entity you might have um a list of where did it go did i delete all the memories i might have deleted all the memories or no i haven't gotten started yet so basically what you do is you have a log so all the memories all the experiences of an acog is just going to be like a list of log files and they could be multimodal files right you could have audio vid video text whatever other sensory information it's got you can also have output information but the point is it'll all be accumulated there and then you can represent it as a vector which is a way of of representing the semantic meaning of it okay so we can get rid of that close that all right so we'll say uh content equals uh chunk so that'll be just the the bit of text and then we will say the um the vector equals the embedding um i prefer the word vector because one it's two syllables and it's easier to say you got vector embedding is too slow and i don't know maybe maybe that's just me being weird but my brain i prefer the word vector it's easier to say okay so then what we'll do is we will we will save this as a json file i think json so that way it'll be it'll be readable um yeah i think that'll be the way to go um let me make sure that we've got um let's see import json and i always have to do this um because i always i can't ever remember like with open um we're going to say index.json um right binary i think that's it encoding equals utf-8 um uh where was the last time that i used this function where i saved json it's not here it's probably going to be something older where did i save json um it might have been here do i have a json file here i have json l that might be close enough format training data import json okay yeah that's fine okay oh wait no that's dumping it to string so i'll need json.dumps i think that's the one that dumps it to file [Music] um as out file uh let's see jace i literally typed out jason json.dumps and then i believe it'll be uh result out file and then what is it like indent equals two i'm totally misremembering this um need to make sure that i get this right though because nothing is is worse than like um really i don't believe that code there we go hey there we go return json dumps now that's a flask response that can't be right there we go json. that's all writing it as json l i can't be right yeah there we go json.dump data out file indent one okay hey i've almost

CLEAR ESSAY:

==========

We get embeddings from OpenAI, and this one is with Ada. We'll probably do Babbage or something, but for search, I think 8 is probably fine. So we'll copy this function.

Basically, what this function does is it takes a text that you're going to try and match, and then passes it the index. The index is a list of dictionaries that have a file name and vector. So what this function does is it compares the text to the vectors in the index, and finds the most similar vector.

We'll do it a little bit differently, where we'll just have the whole thing in memory, and instead of having a file name, we'll just have the chunk of text. That way, it'll all be in memory.

The first thing we need to do is build our index. We'll copy the update index function, and rename it to just build index. Then, we'll list all the files, and get the embedding for each one.

For each chunk of text, we'll get the embedding. The embedding is a numerical summary of the text. Then, we'll save it as a JSON file. That way, it'll be readable.

Once we have our index built, we can then search it. We'll take a text, and find the most similar vector in our index. That way, we can find the most similar experience in our ACOG's memories.