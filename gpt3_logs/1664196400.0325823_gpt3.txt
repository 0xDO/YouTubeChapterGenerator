The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: passing through those materials changes in a predictable way you can actually use those materials to to do com computation the examples that they had were just you know very basic lab levels um they were not particularly dense or sophisticated so it'll be a while before those are are commercially available because i first remember seeing mem resistors many years ago probably 10 years ago so the cycle to go from you know theoretical demonstrated in the lab to commercial application five to ten years sometimes longer oleds were in the lab for decades i remember a friend of mine his dad told me about oleds in the 90s so the and then you know we only just started having oled phones and stuff come out within the last few years so that's like a 20 plus year development cycle so sometimes you have an idea and you prove that it works but it takes a while to commercialize it so powerless like zero power or almost zero power neural networks are coming um but they will be one they probably won't be programmable because it'll be elements based on like the material unless they're unless they're tunable it could it could be uh it could be that you get um uh similar similar things to mem resistors who knows or the maybe maybe these other processes will make the mem resistors more efficient who knows but what this does is it allows you to embed a deep neural network on a chip and it uses very little power and theoretically it could use only ambient energy ambient heat energy to do the processing and when you when you get that level you could actually end up this might be the path towards having ai that is more efficient than human brains because right now the human brain operates at hypothetically um exascale right but the the first exascale computer that we have requires 21 megawatts of power and the human brain only requires 20 watts of power so the human brain is a million times more efficient than an equal equally powered supercomputer so how do we get a reduction of power of a factor of a million or more and it's probably going to be these hardware embedded um ai matrix mem resistors those kinds of things good grief sorry um so one of the things that this does is you've got compute in memory processing so you've got compute and memory paired very closely together that's what that's what neural networks do in the brain so in the brain you have memory is in the synaptic connections but also so is computing so for the brain memory and processing happen at the same time um which is kind of difficult to wrap your head around there's people that have articulated it better than me so then so this is this is on the low end right this is low power this is mobile these are the kinds of chips that are going to be embedded inside cameras inside cars drones that sort of thing that do things like video navigation that sort of stuff but then on the high end you've got sarah brus with their wafer scale engine so this is a giant um die that does uh let's see it has 850 000 cores um and 20 220 petabytes a second or petabits a second of memory um so this is how the largest models are being trained and run today so there was a rumor that open ai got their hands on one of these guys or maybe multiple ones of these guys because suddenly some of their models got way faster so it either they did something like quantization um where they they reduce the um the the from floating point 16 or 32 down to end eight and or they got better hardware so you get better hardware that has faster memory bandwidth um and and integrates everything closer together because the distance that it travels that information has to travel is a big thing that slows it down because then you get latency right so instead of having a cluster of gpus scattered across multiple servers you do everything in one chip and that's also kind of what they're doing here is rather than having a gpu and a cpu and storage and memory all separate it's all together um and so by by shortening the distance that things have to travel you get lots of efficiencies in terms of time because it's lower latency but also you get efficiencies in terms of power because you have less superfluous systems so we're getting more and more purpose built so this is on the low end the small portable end and then on the high end you're getting this stuff and then in the middle you get um you get you know conventional server general purpose server hardware that's going to integrate some of these abilities okay so i think that's it for the day i just wanted to bring you up to speed with kind of what's happening out in uh in ai land with at the hardware level but then also kind of how that hardware is meeting the software and how that software is meeting the hardware so anyways uh thanks for watching like and subscribe and consider supporting me on patreon have a good one

CLEAR ESSAY:

==========

Over the past few years, there has been an increasing trend of integrating computation and memory in a more efficient way. This is evident in the development of memristors, which are devices that can store data and perform computations. This trend is being driven by the need for more efficient and powerful AI hardware.

Currently, memristors are still in the early stages of development and are not yet commercially available. However, they have the potential to revolutionize AI hardware. This is because they would allow for the embedding of deep neural networks onto a chip, which would use very little power. In theory, memristors could even be powered by ambient energy, such as heat. This would make AI more efficient than human brains, which currently operate at an exascale level.

There are already some AI hardware platforms that are beginning to integrate memristors. Sarah Brus with their wafer scale engine is one example. This platform has 850,000 cores and 20 petabits per second of memory. This is the kind of hardware that is being used to train and run the largest AI models today.

As memristors continue to be developed, they will become more and more integrated into AI hardware. This trend will result in more efficient and powerful AI systems that are able to meet the demands of the increasingly AI-powered world.