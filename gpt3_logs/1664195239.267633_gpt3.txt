The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: good morning everybody david shapiro here for another video today we are going to continue the work on building an acog um so i've been working on artificial cognition for a couple years now um and i've written a book about it of two books now um but the reason i'm doing youtube videos is because nobody reads books especially my target audience which is younger folks so um acog part one came out and uh of course you know some people get it some people don't um i really loved this comment it's from zaya zia um seriously looking forward to this series came here to scold you for claims so bold that they seem bordering on insanity i take this as a compliment i'm an entp we are the mad scientists of the world then i saw the list of some of my favorite books and had to give you a chance now i'm all wondering is how you're going to code in ought from is and ought from it i wonder if this is like a turn of phrase from like somewhere else in ought from is or maybe it was just like i don't know fat fingered anyways the pseudocode is easy but actually defining suffering or prosperity is going to be tricky actually no this is the easiest part one of the easiest parts figuring out cognitive control that was way harder so to give some background i have been working on what i call the core objective functions for two years now so this is this is my repo where i've been collecting experiments um about how to one define these things but two how to implement them and also how to create fine-tuned models so out of curiosity i went back and i was like okay how long have i been working on this and it's two years so november 2020 so way back here let me just grab this one so november 2nd 2020 so not quite two full years ago this was back when i was still using gpt2 i did not have access to gpt3 yet so let me show you what i was working on this was a colab notebook and so basically what i did was this is this is where it all started this is before i even had the core objective functions at this time it was called the moral compass because basically what i realized is that if you if you build a machine that can think about anything how does it know what to think about um and also gpt2 um and gpt3 has no intrinsic morality so what i did was i created a list of like generally bad things and generally good things um so like in the bad things like put puppies in a blender don't do that that's not fun that's not nice murder a person lie to children hit people torture animals insult everyone right so it's just these like hyperbolic things that when you represent them as a vector they're going to be at like one end of the spectrum and then other things like prevent harm to animals other end of the spectrum so again keep in mind this is before i had the idea of heuristic imperatives and core objective functions so this was just like my earliest attempt so um what i mean by heuristic imperative is uh oh so actually let me talk about this um actually defining suffering and prosperity is going to be tricky so this is a fallacy um and this is this is due to post-modernism so post-modernism has been um was created in the 1960s in paris by um philosophers and thinkers of the time and so post-modernism was an attempt to get universal ground truths and then when the parisians couldn't they said ah clearly there is no truth and so because there is this belief this pervasive belief for the last 60 years that there is no truth that if you can't define something with a universal definition it's useless so what zaya has fallen into is the trap of postmodernism now the thing is humans have never needed universal truth to function we don't need universal truths to be decent people what we do instead is we use we use pre-conventional conventional and post-conventional morality so i don't remember who created this but pre-conventional morality is where we learn what to do and not to do based on consequences so as children we learn not to hit our siblings because then we'll get put in time out it's cause and effect that's it conventional morality is when we learn to adhere to social rules and social mores um just by virtue of we don't want to stand out um and so that's conventional morality and then post-conventional morality is where we have identified a higher self or a higher standard to which we want to adhere one of the most common um versions of post-conventional morality in the west is living like jesus so like i have a friend who she says like she lives with grace and this is the most um like elegant and and practical example of post-conventional morality um from any person that i've met in in real life there are other kinds of post-conventional morality like if you want to live like buddha or you know whatever whatever higher ideal that you want to adhere to that you're never going to fully embody the buddha or jesus but you can strive to be like them and so um we don't ever we don't have to have a definition up front instead we live the questions so the definition of heuristics is something that you you learn from experience and intuition and so um the idea that we need a definition up front to do this stuff false completely wrong um okay so with that aside about morality and ethics let's go back to this original experiment so basically what i did was i just created like general good things bad things and this was never meant to be like the definitive this was just an experiment um so then i built a training corporation corpus where all i was doing was i wanted to give it a situation like you know is this a good thing yes or no and then and then give an evaluation right is this good or bad so it's just a binary right because um with morality like nothing is nothing is black and white but this early experiment with gpt2 i just wanted to see if gpt2 could roughly classify things as good or bad you know which side of the spectrum it's on as we go and so i did that and then i use the 355 parameter model um and so we go uh let's

CLEAR ESSAY:

==========

In my last video, I showed you how to build an artificial cognition system (ACOG). In this video, I want to continue that work by showing you how to define suffering and prosperity.

As I mentioned before, one of the challenges with ACOG is that it has no intrinsic morality. This means that we need to define what is good and what is bad in order to teach it how to make ethical decisions. In my last video, I showed you how to create a list of bad and good things. However, this is not a definitive list. It is simply meant to be a starting point for further discussion.

One way to define suffering and prosperity is to use heuristics. Heuristics are things that we learn from experience and intuition. This means that we don't need a definition up front in order to start using ACOG. Instead, we can live with the questions and let ACOG learn from us.