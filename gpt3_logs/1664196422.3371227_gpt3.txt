The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: whatever you could get and it wouldn't necessarily be hyper palatable right if you're just scavenging you know picking nuts and berries those are not as delicious as like a cheeseburger or whatever but you still have to eat and so most of the time what you would eat in a natural environment is going to be fruits vegetables basic fish basic game and it's not going to have lots of dairy lots of cheese lots of concentrated gravies that sort of stuff but we have mastered our environment and now we can for every meal we can create a hyper palatable meal if we want and this creates that's an arbitrary reward that was chosen by evolution to maximize our survival and that is one of the reasons that we have such a problem with obesity today is because we have uh instant access to high reward foods at all times um now okay so with that in mind and humans have multiple reward functions um you know we were rewarded for food we're rewarded for like getting comfortable and finding a nice place to sleep and that's why we love our comfy beds we're rewarded for well alcohol creates a reward because it actually short circuits the dopamine cycle that's not actually on purpose um although it might be we might have evolved to enjoy alcohol for social reasons i'm not as up to date on that research anyways i don't want to get too boxed down on on that i think you get the idea okay assumption three in it sorry i'm a little stuffy um assumption three an advanced agent is not likely to have a large inductive bias against the hypothetical goal your proximal goal which regards the physical implementation of goal and formative percepts like reward in favor of the hypothetical goal dist so it might it basically it says it's not going to favor one goal over the other which we want the agent to learn this is a large assumption but again it's also first this is making the assumption that you want to separate out a proximal goal versus a distal goal personally i think that the distal goal which is the overarching goal should always come first and that it should have the ability to spontaneously create proximal goals or short-term goals or what i would call an auxiliary or peripheral objective function so if you're familiar with my work i propose what i call the core objective functions which are universal goals that should always be true and everything else should flow from them and i make that argument in my book benevolent by design where um instead of calling it an objective function i call it a heuristic comparative so again that's their assumption and it's good that they're calling it out um so we'll we'll go from there assumption 4 the cost of experimenting to disentangle proximal from distal is small according to both again that is a bold assumption and it's not one that i agree with so here's an example of what i mean by distal and proximal functions and let's look at it in terms of human evolution so our distal goal according to richard dawkins is to maximize the the population of our species or to put another way to maximize the amount of our dna in the universe he wrote about this in the selfish gene okay so that's our distal goal our distal goal is maximize the amount of human dna in the universe our proximal goals might be eat breathe sleep there is no obvious direct connection between that other than if you don't eat and breathe and sleep you eventually die and your dna decays but we have so many proximal and short-term and midterm goals that are that are all exist um they all exist in service to that distal goal that we're not even conscious of right because the distal goal is so of of humans of being alive and this is all living things the distal goal of maximizing the amount of dna of your species in the universe is so abstract and it's also unconscious it's intrinsic to every cell of our body right and so when you're of a certain age and you've got your hormones going and you're at the peak of fertility you want to have sex just because it feels good you're not thinking oh this is to perpetuate the species right um when you're hungry you eat because it feels good because you don't you don't feel good when you're hungry and you feel better when you eat but all of those proximal goals flow from that distal goal of maximize survivability of the whole species and so this assumption i think is dead wrong this is not based in science or reality or anything that uh that is testable um or demonstrated today so again yeah um i don't know that i agree that they have thought about um uh goals correctly um okay so moving on um intervening in the provision of reward so this is this is uh where based on the assumptions that they made they talk about like those rewards um and again so like talking about the feast reward like so here's a problem with um with this isn't it this is a human example of what this paper is talking about that feast reward where you you crave and seek high value foods and eat as much of them as you can when you get the opportunity to that results in obesity heart disease diabetes and so in that case our inner alignment is broken because we're killing ourselves prematurely or making ourselves less fit for our distal goal of maximized species and it's because we have this circuit that breaks or short circuits when we seek that reward um ditto for people that are adrenaline junkies and end up getting killed from you know getting killed in an avalanche or falling out of an airplane or whatever um so that there can always be or alcoholism right that is an example of human inner alignment breaking um and so we're not perfect right but it's a good example so all right assumption five if we cannot conceivably find theoretical arguments that rule out the possibility of an achievement it is possible for an agent with a rich enough action space i think that's poorly written let me let me step through this if we we cannot conceivably find theoretical arguments that rule out the possibility of achievement

CLEAR ESSAY:

==========

Assumption 5 states that if we cannot find theoretical arguments that rule out the possibility of an achievement, it is possible for an agent with a rich enough action space. This is a bold assumption, and one that I do not agree with. Let me explain why.

If we take a look at human history, we can see that there have always been achievements that were once thought to be impossible. For example, humans once thought that it was impossible to travel faster than the speed of sound. However, we eventually achieved this feat. Similarly, humans once thought that it was impossible to travel to the moon. But again, we achieved this.

The point is, there are always going to be achievements that we once thought to be impossible. And if we cannot find theoretical arguments that rule out the possibility of such achievements, then it is possible for an agent with a rich enough action space.