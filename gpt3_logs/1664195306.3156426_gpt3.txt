The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: good morning everybody david shapiro here for another video we are going to pick up where we left off with build an acog so acog is artificial cognitive entity that's the shorthand term that i use um yeah so this is based on my work called natural language cognitive architecture it is available for free here on github you can also get a paperback copy on barnes noble the overall architecture is right here oops let me zoom out a little i know what i'm doing i promise why isn't it there we go okay now you can see it um so a cognitive architecture is just a computational or mathematical model that is meant to mimic a computational engine or a cognitive engine such as our brains um the so this was inspired um based on the the primary model of robotics which is input processing and output and that creates a feedback loop with the environment so this is environment input processing and output and so there's a loop that's the outer loop so that's if you look at like boston dynamics and tesla um that kind of automation is strictly an outer loop um so that's just that's handling a body and like i'm gonna get from point a to point b or i'm gonna move the box or you know i'm gonna drive the car so that all happens in what i call an outer loop one of the key innovations that i added was that i added a second loop so these are two separate loops that overlap kind of like a figure eight and the inner loop is the thought and the thought is what we're working on with acog there were some really good questions so this is this is basically a review because i know some of you will not have seen the other videos yet so this will be kind of a review bringing everyone up to speed but also i got a new book to talk about um so let's see uh this dude zaya um had some good questions um and i said i will be happy to uh to talk about these um let's see so let's see what was one of the last comments um so oh yeah so we were talking about zeo is concerned because in my last video i was talking about my core objective functions or the heuristic imperatives which is kind of like the moral framework that i'm giving my acog which is reduce suffering increase prosperity and increase understanding now i articulate those as just six words but as you'll see later the training data that i'm going to use is is going to be much more varied and robust you can see some of that here go to core objective functions on github and so you can see examples of how i'm training this um i've been working on these experiments for a while um and so basically what i do is i accumulate many many stories of of um like from all over the internet whether it's from reddit or news sites or other articles or questions on stack exchange um and i use that as like fuel because right you know data is fuel for ai and i use those each of those to discuss suffering prosperity and understanding um and so the training data like that's the the saying it as simple as just reduce suffering that's shorthand for us but the training data is much more complex so i gotcha isaiah i know that you're worried about word definitions but those are just organizing principles it's not like i'm just gonna give you know it's not like it's written in a constitution well in my book natural language cognitive architecture one method that i propose is to write a constitution for your uh for your robot um so that's one one method of attack but also fine tuning because when i wrote that book fine tuning wasn't available so once i got access to fine tuning i was like oh we can train our agi with tens of thousands of examples of the behavior we want to see so like if we want it to reduce suffering we can give it you know examples from individual to local to global examples of how to behave how we want it to think and so remember that the inner loop right here this is just thought this does not have anything to do with the outside world it's not like saying okay i'm gonna like i'm gonna move this book from here to there it's saying what should i do with this book oh that's a book that that contains information maybe i should read the book right because you can't take any of those those impulses for granted our brain does a lot of this automatically behind the scenes which is why i read so much neuroscience and so the inner loop handles all that stuff that happens in our brains consciously and unconsciously so with all that said i wanted to introduce a new book that i'm reading it's called brain trust what neuroscience tells us about morality so if you're new to this channel you'll know that i am a big big critic of philosophy especially contemporary philosophy contemporary philosophy to me is just navel gazing and i say that as someone who's writing a book on contemporary philosophy my book that i'm working on is called post nihilism so basically a lot of people assume that philosophy and ethicists or philosophers and ethicists are the correct people to handle moral and ethical um uh questions when it comes to artificial intelligence i disagree with that i think that sociologists psychologists and neuroscientists are the correct people and i went and found a book by a neuroscientist talking about morality so the reason that she's that so this is uh by patricia s churchland um who does have training as a philosopher um and she was not satisfied with philosophy because it wasn't anchored to anything measurable she calls philosophy um at least contemporary philosophy um confident opinion um or armchair opinion um she does have respect for the old grapes like david hume and um adam smith so don't be fully offended but um she is cr she says we have more tools available today we have more knowledge available so let's look at the actual origins um of morality which to me is a much better model when designing stuff now remember that my core objective functions one reduce suffering to increase prosperity on page 30 in chapter 3 she says brains are organized to seek well-being and to seek

CLEAR ESSAY:

==========

Cognitive architectures are computational or mathematical models that mimic a cognitive engine, such as the human brain. Robotics is primarily based on the input-processing-output model, which creates a feedback loop with the environment. This loop is what allows a robot to interact with its surroundings. However, cognitive architectures also have a second loop, called the thought loop, which handles all the thinking that goes on behind the scenes in the brain, both consciously and unconsciously.

One of the key innovations of cognitive architectures is that they can be trained with thousands of examples of the behavior we want to see. For example, if we want our robot to reduce suffering, we can give it examples of how to reduce suffering from all over the internet. This training data allows our robot to learn how to best reduce suffering in any given situation.

 Patricia S. Churchland's book, Brain Trust: What Neuroscience Tells Us About Morality, is a great resource for understanding how morality works from a neuroscientific perspective. In her book, Churchland argues that philosophy is not anchored to anything measurable and that contemporary philosophy is nothing more than armchair opinion. She suggests that sociologists, psychologists, and neuroscientists are the correct people to handle moral and ethical questions when it comes to artificial intelligence.

Churchland's book provides a much better model for designing AI than contemporary philosophy. She argues that brains are organized to seek well-being and that morality is based on this natural impulse. By understanding how morality works from a neuroscientific perspective, we can design better AI that is more likely to act in ways that benefit humanity as a whole.