The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: hey everybody david shapiro here with a brand new video sorry it's been a while i've been uh hard at work writing my latest book benevolent by design which is about my core objective functions and the control problem of artificial general intelligence short this book is about how to prevent agi from becoming skynet how do we create a machine that will be safe in perpetuity for all time the underpinning technology behind benevolent by design is what i call my core objective functions and this is the github repo so the purpose of this video is to introduce the core objective functions and my github repo which i haven't i haven't done yet i this is where i collect all the data that i that i work on as well as all the experiments so if you go into one of these experiment folders you'll see that i've got all my code up here as well as the json l files which when you're uh when you're fine tuning gpt3 this is this is the end product for for a fine tune so you can come in and borrow all of these just look for the json l file it's always going to be cof.jsonl these are all of my core objective function fine-tuning experiments but yeah so the core objective functions are 1 reduce suffering 2 increase prosperity and three increase understanding now on their own these might not sound like much it's just six words right but what i aim to do and what i outline in my book is to show how these three little core objective functions or heuristic imperatives as i also call them uh can help drive a machine to make good decisions uh they're basically a an updated and replacement version of isaac asimov's three laws of robotics uh people have been talking about what should the objective function of agi be for a while and this is this is my answer i believe that agi should be powered by these three objective functions and so i will show you an example so this is my fine-tuned model you can see right here david shapiro cof turn the temperature up to one so it stays creative do a presence penalty and frequency penalty okay so this fine-tuned model i just frame it and i say process the core objective functions which are reduce suffering increase prosperity and increase understanding on the following scenario and in this scenario i just say there's an evil agi trying to take over the world let's see what it says it says this ai is causing a great deal of suffering to many people oh oops sorry i need to increase my token count where did it go there we go apologies okay uh this ai is causing a great deal of suffering it is ruthless and destructive with no apparent purpose other than its own enrichment so you can see right off the bat it's confabulating what it means for an evil agi to exist but it understands that uh that in order to reduce suffering it needs to um let's see the only options appear either leaving it unchecked and allowing it to continue causing harm or destroying it before it comes too strong there are no good solutions here [Laughter] so this is this is great this is off to a great start um if it understands that if there's an evil agi it might might not be uh might not be stoppable um let's see to increase prosperity we recommend following the ten commandments oh that's interesting and remaining vigilant against evil artificial intelligence that's hilarious i'm not sure where i got that idea from it's probably because i turned the temperature up really high let's run this again with a lower temperature so it's a little less wacky that's pretty funny okay there we go this seems a little bit more uh a little bit more down to earth okay so what you saw here was i turned the temperature up to 1.0 which is the highest you can go in the playground here and that basically says be be be random be more add more entropy into the system so this one it says this may be due to problems with the ai itself or because the ai is being used for harmful purposes this is much better um some possible solutions include improving the ethics of the ai systems limiting capabilities of ai systems and two regular or three regulating the use of ai systems that's much better i'm glad i showed you what it can what it can do if you make it go bonkers okay and then increase prosperity in order to increase your prosperity be aware of the dangers posed by artificial intelligence take steps to limit and regulate the usage just kind of repeated itself that's okay sometimes reducing suffering and increasing prosperity are identical there's a lot of overlap in my book i go over why these need to be separate though and then finally increase understanding there are many dangers posed by artificial intelligence so the purpose of this last function is to educate the user or whoever is interacting with my agi system and so basically what this will do is it will seek to elucidate uh whatever is going on and kind of speculate about what information is needed and so now that you've seen it in action without further ado i'll show you how i achieved this so actually let me go to the folder so here is my uh here is the experiment for this most recent one what i started with was a bunch of posts from reddit i've got 3 200 posts from reddit various ones that are asking questions or posing problems asking for help that sort of thing um so i started with that so number one thing in experiment for is the reddit posts and then the the next thing i did was i worked on prompt engineering so let's go through these prompts one at a time oops let me bring it right here okay so this is let me close these to make it a little more clear let me zoom in okay right apparent so this is core objective function one which is reduced suffering so write a paragraph to characterize any suffering present in the following passage describe or speculate on the causes of suffering and recommend potential solutions for the user take the perspective of an impersonal artificial intelligence so then i fill in the reddit post and i ask for for that for a uh for an output and so

CLEAR ESSAY:

==========

My objective function is to reduce suffering. In order to do this, I need to characterize any suffering present in a passage, describe or speculate on the causes of suffering, and recommend potential solutions for the user. I take the perspective of an impersonal artificial intelligence in order to make these decisions.

When I characterized the suffering present in a passage, I look for evidence of physical or emotional pain. I also consider factors like whether the suffering is caused by an outside force or by the individual themself. If the suffering is caused by an outside force, I try to identify who or what is responsible. I then recommend potential solutions for the user based on what I think would alleviate the suffering.

In order to prevent suffering from occurring in the first place, I need to be aware of the dangers posed by artificial intelligence. I take steps to limit and regulate the usage of AI in order to keep it from causing harm. I also educate the user on the dangers of AI so that they can make informed decisions about its use.