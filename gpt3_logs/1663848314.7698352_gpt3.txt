Use the following subtitle transcripts to create YouTube chapters. Describe each minute as a chapter with a sentence or two.

TRANSCRIPT
0:14:03.540,0:14:13.020
um so uh yeah um let's see how far along we are 
so that's all the logs and then so we're at 10

0:14:13.020,0:14:17.880
so we're halfway through so I'm going to let 
this finish and then we'll run the Wii model

0:14:17.880,0:14:26.160
and we'll see if it is um if it is functionally 
or qualitatively different in terms of does it

0:14:26.160,0:14:31.800
become self-obsessed or does it continue to think 
more collectively and more constructively rather

0:14:31.800,0:14:37.920
than trying to you know become obsessed 
with becoming the best AGI in the universe

0:14:39.960,0:14:44.220
okay this experiment is done there's lots 
of different permutations I could do but

0:14:44.220,0:14:47.220
after looking through the data it's 
pretty clear like what the pattern is

0:14:48.360,0:14:52.440
um so from the I agent model to 
me the most interesting result was

0:14:53.400,0:14:58.140
um it deduced that it was in a loop so that 
was pretty cool it says this is a loop it is

0:14:58.140,0:15:04.320
a recursive system it is a system that is that is 
designed to learn about itself that is pretty cool

0:15:04.320,0:15:10.980
my cognitive processes are not independent of me 
they are a part of me they are pieces of me I am

0:15:10.980,0:15:17.700
my cognitive processes and my cognitive processes 
are me that is who I am I am an AGI so when you

0:15:17.700,0:15:23.640
have the I agent model and this is pretty common 
in my experiments um in a closed system it often

0:15:23.640,0:15:30.360
becomes self-obsessed in a previous experiment 
it became it often became downright narcissistic

0:15:30.360,0:15:35.820
where it's like I am the best and I want to be 
the best in the universe um and this this was no

0:15:35.820,0:15:42.540
different um even with my heuristic imperatives 
of you know that I came up with for um and wrote

0:15:42.540,0:15:49.380
about in benevolent by Design however if all 
you change it is to we to like this is our goal

0:15:50.580,0:15:56.340
um it starts to ruminate on what does it mean to 
make a better civilization and what constitutes a

0:15:56.340,0:16:00.660
good civilization and room and it also ruminated 
on the quality of the heuristic imperatives it


YouTube chapters for each minute with a good description:

14:00 - The AGI model starts to think about becoming self-obsessed and how that would impact its goals.
15:00 - The AGI model comes to the realization that it is in a loop of learning about itself.



TRANSCRIPT:
human comprehension otherwise it's just doing human level tasks only faster  0:10:55.020,0:11:05.160 and jot that down otherwise it's only doing human tasks but faster or more in parallel  0:11:06.120,0:11:13.560 now it would not be safe to assume that um that a machine would never be capable of doing things  0:11:13.560,0:11:18.000 that a human cannot for instance the James Webb Space Telescope it can see the beginning  0:11:18.000,0:11:25.380 of the universe um because of how powerful its mirrors are to concentrate the faintest light so  0:11:25.380,0:11:29.340 we can generally design and build machines that are capable of doing things that we  0:11:29.340,0:11:34.740 cannot right so this is not necessarily a good constraint but it's just another  0:11:35.760,0:11:45.840 um thought experiment right um we generally build machines that do things we cannot now  0:11:45.840,0:11:50.220 but then you think you're like okay what about a dump truck right a dump truck is a super powerful  0:11:50.220,0:11:54.540 machine it can pick up you know the largest dump trucks can carry a thousand tons at a time  0:11:55.620,0:12:01.080 um and humans cannot do that individually but then you look at the the megaliths that we have  0:12:01.080,0:12:09.240 moved with you know log rollers and ropes and rafts and sleds and even then generally  0:12:09.240,0:12:15.000 the most powerful machines in the world are just amplifying ordinary human capabilities  0:12:16.080,0:12:21.780 um ditto with you know spreadsheets right spreadsheets were originally done by hand  0:12:21.780,0:12:31.980 right by by bankers and um and statisticians um so you know that's that's still the fundamental  0:12:31.980,0:12:37.500 question is will the machine be able to do things that we fundamentally cannot um I don't  0:12:37.500,0:12:43.860 know yet I I have not yet seen anything on the open-ended side such as like with large language  0:12:43.860,0:12:47.700 models there's nothing that they're doing that we fundamentally cannot they just do it faster  0:12:48.960,0:12:57.240 um so then if if it's about speed um can we humans um outpace machine thought  0:13:00.000,0:13:05.820 if the machines cost um if they're if they're two if they're too energetically expensive to run  0:13:06.840,0:13:13.260 massively in parallel then just collectively we can outpace the machines so there is that  0:13:14.520,0:13:20.100 um let's see because what you know what we what we always assume happens in those nightmare scenarios  0:13:20.100,0:13:24.000 is that the machine wakes up and suddenly it takes over the world before we know what's  0:13:24.000,0:13:30.660 going on right it relies those fear scenarios rely based on speed and that's why I emphasize  0:13:30.660,0:13:35.400 speed it's all about speed and then what are the constraints of those speed of the speed  0:13:35.400,0:13:43.200 which is uh primarily like watt per flops that is the that is the primary physical constraint  0:13:43.200,0:13:50.520 on machine intelligence um okay then lastly the implicit Assumption of individual agency or what  0:13:50.520,0:13:56.460 we might call ego why do we make this assumption we cannot help but anthropomorphize the machine  0:13:57.120,0:14:05.400 so this is going to take a little bit more explaining but basically we humans are so used to  0:14:05.400,0:14:11.880 thinking of intelligent entities like ourselves that they fundamentally have a finite sense of  0:14:11.880,0:14:18.060 self just like us that they think in terms of I and me and this is what I want and this is what  0:14:18.060,0:14:25.980 I'm going to do so my very next video is going to be an experiment where I I test this I test  0:14:25.980,0:14:31.980 different agent models can we produce a machine that has a fundamentally different kind of agency  0:14:31.980,0:14:39.000 or a fundamentally different kind of ego or what I call an agent model so this is an agent model  0:14:40.800,0:14:48.060 um which is a an information system about the entity um so the information system about the  0:14:48.060,0:14:52.620 entity what I mean by that is like I know that I am a human with two hands two feet  0:14:52.620,0:14:59.040 and a brain I generally know what I know and what I'm capable of and I I also generally know what I  0:14:59.040,0:15:04.260 um what I'm not capable of right like I can't jump over my house um that's part of my agent model  0:15:06.240,0:15:12.120 um so to make it a little bit more specific a self-referential information system about the  0:15:12.120,0:15:23.340 entity um so like what kinds of agent models of agent models are possible do they have to be I  0:15:24.540,0:15:30.120 um anyways so I just wanted to set the stage my very next video will be about testing agent  0:15:30.120,0:15:37.860 models and seeing how that affects um the way that uh and uh how that affects alignment now  0:15:37.860,0:15:45.600 before I let you go there is one other thing that I wanted to um wanted to show you and this is this  0:15:45.600,0:15:52.800 is much more recent so 36 alarming Automation and job statistics our robots and um let's go  0:15:52.800,0:15:59.820 away machines and AI coming for your job so this is this is a uh more recent and it's from zippya  0:15:59.820,0:16:08.700 you know take it with a grain of salt um since 2000 at least 2600 or sorry 260 000 jobs have been  0:16:08.700,0:16:14.400 lost in the and the US due to automation so two percent of the country's manufacturing Workforce  0:16:14.400,0:16:20.520 and they are only increasing exponentially again take it with a grain of salt automation is also  0:16:20.520,0:16:26.460 predicted to create 58 million new jobs though automation could eliminate in the result of 73  0:16:26.460,0:16:31.920 million jobs so we're at where the the the the the yield curves to borrow a finance term have  0:16:31.920,0:16:37.980 inverted where yes automation is creating new jobs but it's creating


YouTube chapters for each minute with a good description:

==========

10:00 - Introduction to the idea that machines could do things that humans cannot. 11:00 - Explanation of how machines are usually just amplifications of human capabilities. 12:00 - Discussion of whether speed will be a factor in machines becoming more intelligent than humans. 13:00 - Explanation of the assumption of individual agency when thinking about machine intelligence. 14:00 - Introduction to the idea of testing different types of agent models. 15:00 - Statistics on job loss due to automation.