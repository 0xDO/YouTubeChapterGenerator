The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: hey everyone david shapiro here back with uh auto muse so uh we've done parts one and two this has taken a lot of work um and i've done a little bit of work um in the background just because i didn't wanna bore the audience with um with some nitty-gritty stuff but let me show you what i've done so first i think it'll make sense to let me just show you what the output is and then we'll work backwards just because there's a lot here and without a guide like a orientation it's not going to make a lot of sense okay so there are there's two folders that have the the kind of like final fine-tuned data and it's prompts and completions and so if you're not familiar with gpt3 yet there's a prompt which is the input and then the completion which is the output input output so basically what i've done is all of this goes to creating an input like this outline dorian gray is a young man who is incredibly handsome etc etc story so far so this is chunk number 23 the passage discusses the uselessness of art and how it is seen as a form of autobiography etc etc so this was created by the function called recursive summarization it needs a lot of work this is this is probably not going to work but we're going to see it through to the end and see how it goes so there's the outline which is included in every input so that way gpt3 knows the whole story the story so far so this is a summary of summaries so basically it's it what this is supposed to represent is what has happened in the story so far you can see here that it's a little bit too compacted so we're probably going to need to change that in the training data that's okay um and then the last chunk this is this is the most recent 1500 characters of prose that was written and so then what we ask it to do is write the next chunk and so we're on dorian gray 23 so this is paired with the completion dorian gray 23 so they have the same file name um but so you see here like the painter laughed i don't think there will be any difficulty about that sit down again harry blah blah with the single exception of myself dorian gray stepped up on dot dot dot the dice with the heir of the young greek martyr okay so you can see where it just continues one after the other so this has been formatted into the actual training data called novel.json l and so it's all nice and compacted into json format but this is like a human readable version and i split it up this way rather than just going straight to json l so that because this is so complicated you need to be able to see like okay this is what it's actually going to look like and i've got the fine tune running let's see if it's done still running okay so while that's running in the background we will go over um all the things that i've done and what we can do to test this is we'll put we'll we'll just copy paste this because we know what what we expect the output to be so we'll once we've got the fine-tuned model we'll do this and then put it in and see how close it is it's going to be awful um but you know because like we're basically trying to train da vinci um to write a novel in chunks from you know 280 samples um okay but first let me just give you a little bit of background so in the last video a few of you pointed out that um like dracula and and sherlock were not good examples because they're they're um uh dracula bram stoker's dracula is ephemera right so it's not like a conventional novel i think i'm saying that word right um but basically it's like it's it's transactions right it's like letters and newspaper clippings it's not it's not a conventional novel and neither is sherlock sherlock is a compendium so it's like okay let's get seven actual novels because that's what we're trying to do so let's train it only on novels so i went and got a few different books i went and made their outlines here so that's all the same i split all the books up into chunks so each of these chunks is 1500 characters long and then we did summaries of each of those chunks so each of these well i did i did up to 40 because we're not gonna i didn't want to do like the whole the whole novel um so we did we did 40 uh 40 summarize the first 40 chunks of each of them and then um to do then we got the the prompts and completions um so let me show you the scripts that i wrote to do this so first is so i've got these in order so from the top books to chunks summarize chunks excuse me recursive summaries format the json l and then fine tune and then wow excuse me i always get hiccups when i start talking so then after this we'll write another script to actually use the model but um we can i can it'll probably make more sense to just show you the novel the the model on the playground once it's done so i might not worry about that script just yet um i might just write that offline and do that in a fourth video or pause this one and do it anyways okay so uh from the top to overall review this is what we did so this first one books to chunks what it does is it goes to this folder books it just grabs the books one at a time you can see um files equals os lister books for file and files open the book and then we use text wrap for chunks of 1500 and that generates the uh the chunks here and so each chunk is just going to be it's going to end and begin and end on whole words it's not going to cut the words in half so you see in this case it's um it the the total length is 1500 1559 characters so basically it says like okay we're going to go closest to that console line i think there's other settings you can do to like how how wide is the console width or something along those lines anyways it's a super handy dandy thing to just split up text so use this use this module text wrap i think it's built into python i don't think i needed to install that okay so script one book to chunks just splits it up also let me check the volume okay um yeah because the last one was a little bit saturated so i turned the gain down a little bit i hope this sounds

CLEAR ESSAY:

==========

In the previous video, we looked at how to use the GPT-3 API to generate text in the style of a particular author. In this video, we'll be taking things a step further by training GPT-3 to generate an entire novel, one chunk at a time.

To do this, we first need to split our training data (a collection of novels) into chunks, each around 1500 characters long. We can then generate summaries for each of these chunks, which will be used as input data for GPT-3. Finally, we'll use the GPT-3 API to fine-tune a model to generate new chunks of text, based on the input data.

Once we have a fine-tuned model, we can use it to generate new chunks of text, one after the other, to create a brand new novel.