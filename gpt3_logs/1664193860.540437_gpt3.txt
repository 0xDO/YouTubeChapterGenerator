The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: do have a save file okay so let's do output actually no we'll call this the novel um okay so novel and then each time we run each time we run we get a we get a chunk we'll save it so we'll do save file and then how do i have this formatted it is content and then file path okay so last chunk and then file path will be novel slash chunk zero zero one dot text okay and then the summary chunk so we're going to go ahead and summarize this but i think because um all we need is the last chunk we'll generate the next chunk once it's populated uh and that'll all happen here yeah because all we need all we need to add to the summary chunk is the last chunk right um okay so i think that's probably fine because we've got last chunk we'll summarize um actually no i guess we do need a summary chunk going in okay so now prompt equals open file um we'll say prompt summary text dot replace and then the summary is chunk okay chunk and we'll say last chunk and we'll say actually we'll just say the summary so far summary so far because that's that's that's the variable name that'll be here equals gpthree completion [Music] um prompt okay let's add a little bit so we'll do um prime the summary variable and then we'll do prime the story variable okay so um last chunk and summary so far that is going to be so we've got the outline we loaded that um right so we load the outline here so basically all we're doing is we're priming this input so i probably should have explained that because if you're watching you're probably like what are you doing um story so far and last chunk so we've got those three variables that's what we need to put in and then the fine tune now we're ready for the fine tune model so we're gonna ask it to generate next chunk okay so now that's what we do um okay and we saved it as last chunk um i in range 20. so actually we're going to start at 2 because we're going to be working on chunk 02 and i want to use that variable i um let me just make sure that my python logic works how i think it does range 2 to 20. print um print view as a list damn it there we go okay so if i do print list range 2 to 20 it'll go from it'll go 2 to 19 and i want 20 full chunks so i need to go to 21. okay and that'll give me chunk two three four all the way up through 20. okay okay so we're going to start with this save it out summary so far um that's good now let's look at some of this logic first i guess we're going to need to load the the full prompt so prompt equals open file prompt underscore full dot text so that's similar to right here outline summary and last chunk so let's just copy this so outline is there summary chunk so let's actually replace that with the correct name summary so far and last chunk okay so on the first instance the first time this runs all these variables are populated and then we're going to ask it to do this the fine tune completion which will use my model here now one problem that i'm concerned about is if if the model isn't loaded so we're going to do max retry we're going to do 15 and we're going to wait five seconds between each one so because 5 times 15 5 times 15 that's 75 seconds usually it doesn't take that long for a model to load actually let's go ahead and do 20 retries so basically what happens with fine-tuned models is that sometimes the api will unload the model and you have to wait for it to warm back up and so basically what it's going to do is if it errors out then then i want it to wait and try again so i'll have it sleep five seconds um because if you just try every second you're gonna the api is just gonna get mad at you it'll say too many retries um so i'll wait five seconds and then i'll retry up to 20 times for the fine tune completion okay so let's go ahead and grab fine tune completion and we're going to call this next chunk equals fine tune completion prompt and so that is going to be that next chunk is actually what's going to be saved as o2 and then once it's done next chunk will become last chunk but we also need to summarize it and add it to the summary okay so let's go ahead and save file and the chunk number will be here um oh i guess i need to go look at what i did for the um for the file names yeah get next chunk so i'll need to i'll just write a quick function to generate the file name oops def make file name and so we're gonna have uh n so then we'll just copy this and do number i'll be chunk that um we can get rid of that so it'll just be number and i think we can just return yep return chunk 0 0 s um yeah so let's copy that and then if it's in the hundreds we only need two zeros and if it's less than a thousand um because we're not going to do we're not going to do a thousand um okay but just for completion because you should always so here's here's a rule of thumb if you have a function like this um it's best practice that it always returns something um so if you if it's since it's if else if else if and then else so finally you just return um it'll be uh whoops here we'll just remove all the zeros because that means that it is a four digit number um okay that's fine so make file name um save file uh novel plus make file name i okay so we will this is we're going to want to save the next chunk actually um and you also want to save the next chunk because at the end like we're not going to have another iteration to review the last chunk because the next chunk will ultimately be the last chunk one time okay so now we've got um next chunk is saved out to file so now we need to summarize um so now let's do uh update the summary and so we'll replace that with next chunk and we'll do summary so far equals oops summary so far plus um gpt3 completion prompt so now this is where if the summary is too long we will um we'll use this logic so if summary shorten summary so far if it's too long okay um [Music] actually we'll leave some output shortening the summary and again remember the reason that we want to keep track of the entire summary so far is just

CLEAR ESSAY:

==========

In my previous video, I showed you how to use the GPT-3 API to generate new text based on a prompt. In this video, I'm going to show you how to save the generated text to a file, and how to keep track of the full text so far in a summary variable.

To start, we need to load the full prompt into a variable. We'll call this "prompt_full". Next, we'll create a variable for the summary so far, and another for the last chunk of text generated. These will be called "summary_so_far" and "last_chunk", respectively.

Now we're ready to use the fine-tune completion function. This will take our prompt and generate the next chunk of text. We'll save this chunk as "next_chunk", and then add it to the summary.

Once the next chunk is generated, we'll save it to a file. The file name will be "novel/chunkXXX.txt", where XXX is the chunk number. We'll start at chunk 2 and go up to chunk 20.

After the next chunk is saved, we'll update the summary variable with the new text. If the summary is too long, we'll shorten it.

That's it! By following these steps, you can generate a long, coherent piece of text using the GPT-3 API.