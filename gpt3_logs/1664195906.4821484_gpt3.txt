The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: so basically what this does is it creates what's what's also called a hash table where it's like okay every instance of info is going to have a an item named category and so if we need to get the category we we just call that and then we also can call the vector and so then we'll say class oops class is dot append info and let me show you what this looks like so then we'll do print classes and this is not going to be pretty because it's going to be pretty big um and then we'll just add a quick exit 0 so that we won't even dive into this but this will just show you what we're doing and let me do a quick time check we're already at 17 minutes okay so let's do python embedding whoops what did i do all right okay so what this is going to do is it's going to create a list of dictionaries and each of those dictionaries is going to contain a couple pieces of information so here's what it looks like category mammal and then here's the vector that declares that it's a mammal okay cool all right and we're almost done i promise so let's go ahead and we can get rid of that just comment those out all right so result equals match class vector and then we'll just say classes so this will just be a really quick search so we've already done all the embeddings that we need to um so then let's just match it okay classes all right so what we'll do then is we'll say results equals list and then for c in classes we will want to get the dot product um so we'll say uh brain i need more coffee that's what i need so for c in classes we will do score equals and we'll do similarity because that's the function that we declared right here so we'll do similarity of our vector and then our class our c vector because remember we we added we we have a dictionary that has um the vector for each each category here um so then we get the score and then what we do is we'll do info equals um category equals c category oops and then we'll do oh score equals score and then results dot append info um and there you have it that is pretty much it then we'll do return results and so then we'll print the result at the end and that should be it let me do a quick test python embedding so it's going to get the thing in the background so let's say bald eagle um ooh that doesn't quite look right so let me let me show you another trick from p print import p print so p print is called pretty print um and so we're gonna we're gonna change this to pretty print which will instead of it kind of being all in one line here it'll make it a little bit prettier so let's oops do this again um enter a live form here bald eagle okay so then we can say um the category plant 0.77 not so good category reptile 0.82 category mammal 0.80 category fish 0.80 now you might notice that i did something wrong here i didn't include fish or a bird as a category so um interestingly though the bald eagle which is a descendant of the dinosaurs the common ancestor it's closest to reptile so let's run this again because i'm silly and we'll add bird as a category so let's try that again bald eagle and then we say bird there we go .87 so bald eagle is semantically closest to bird tada all right what else do we have let's do a komodo dragon and as we'd expect that's the highest score so it's a reptile what else do we have let's do a shark so a shark it is a 0.91 similarity to a fish now there was a question i think it was on my discord server we were we were talking about like okay how do you do semantic search for memories and stuff what if we add another category that is not an animal kingdom let's say if we do pet versus wild animal right so let's do that um do a clear screen real quick python embedding okay so let's do a cat so the if we put in a cat it is it's closer to a pet a score of 0.93 than it is to wild animal of 0.85 right and it's also a mammal although this says it's very close to also being a fish that's kind of funny um oh wait nope it says it's closer to bird than it is to mammal interesting okay so i have seen some people complain about these embeddings and i'm beginning to see what they mean um let's do a domestic cat see if that clears it clears it up so domestic cat .83 to mammal okay that's a little bit better so we're a little bit we're a little bit more specific and then it's just slightly more pet than wild animal um let's do uh let's do a golden retriever so that's not even that's not even the name of a species that's a breed of dog okay so golden retriever is a wild animal according to this um uh and it is also a reptile okay so this is pretty funny um i'm wondering what happens let's let's see if this gets better if we uh if we do a different embedding engine okay so we're doing ada and we're doing text similarity but there's also text search um so this one i don't think will apply um but let's let's let's do this let's upgrade to babbage and just see what happens this is actually kind of funny i did not expect this to happen um all right so let's change our engine to text similarity babbage this is kind of funny cls so it works in some cases but not in others it'll take a little bit longer so let's start with bald eagle so bald eagle um let's see reptile mammal fish bird um so bald eagle is just slightly more reptile than anything else that's interesting um it is a wild animal though so we got we got bald eagle is more associated with the term wild animal than it is with pet um let's see uh then what was it golden retriever let's see if we get the right um let's see it is also a reptile interesting um and it's still a wild animal okay so going up to babbage didn't help um i wonder if i'm using this wrong anyways you get the idea you can do this uh for for text search and classification um this is actually kind of funny i will need to do some research and figure out what i've done wrong here i'm sure someone will let me know but anyways thanks for watching

CLEAR ESSAY:

==========

This code creates a hash table where each instance of info has an item named category. If we need to get the category, we can just call that, and then we can also call the vector. So, we'll say class oops class is dot append info, and then we'll print classes. This won't be pretty because it'll be pretty big, but it will show us what we're doing.

Next, we'll create a list of dictionaries, where each dictionary contains information on a category. For example, one dictionary might say "mammal" and then have the vector that declares it as a mammal.

Then, we'll match the vector we're given with the vectors in the dictionaries to see which one is the closest match. We'll do this by getting the dot product and then ranking the results.

Finally, we'll print the results. So, if we enter "bald eagle", it should tell us that it's closest to the "bird" category.