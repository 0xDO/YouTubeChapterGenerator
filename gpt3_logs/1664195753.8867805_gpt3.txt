The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: hey everyone david shapiro here um the reason that we're here recording this video again is because i'm kind of an idiot um actually no i take that back i just really strongly adhere to the agile methodology and really i invented the agile methodology as a small child because i would just improvise until i got stuck and then i would do it again and and learn so really agile is just like the natural way of doing things that's my excuse and i'm sticking to it so without further ado i have revisited the process of of parsing wikipedia so here's what i'm doing i've changed my methodology so now i'm saving every individual article as a json file and this is what it looks like so id so that's the article id that you see or here let me zoom in a little bit there now you can probably see better so you got the article id uh 1575 and then here's the actual text and you see this is lovely plain text it's easy to read there's no links or anything so this this plain text version can be easily used for anything whether you want to just print it out or you want to uh index it in solar which is what i want to do because of my agi research i need an offline encyclopedia so everyone is in this nice little json file this is high fidelity it i've preserved pretty much all information and let me show you how i've done this so let me go ahead and also open the doo doo doo wiki to text so i've right here i've just got a simple uh supervisor function that kicks off or supervisor script that kicks off the main function um as i've shown in past videos i've got it all as the um as the the the stream and i can i can also just show you what this looks like so if you go get if you go download wikipedia it'll look like this the file name is a little bit longer i shortened it up just so that you see that it's the english wikipedia of the year 2021 of april first so this is a month and a half old that's okay you see it's an 80 gigabyte file though so because it's a because it's a monolithic file you need to read it one line at a time it's just too much data to process all at once so that leads to everything else that you see here so here's the new function that i wrote it's based on the old function what i do is i pass it the name of the of this massive xml file and then i give it a document to save the subfiles to and then it's just a real simple process that iterates through this first file line by line it looks for the xml tag page and then it looks for the xml tag end of page and if it's between that it just adds to the line and then i've got all this commented out this is how i used to do it and it was slower and this is how i do it now process save article so what i do there is i just save the article i do the end uh the analysis to get the um the document id title and text and then i change this uh analyze chunk i change this as well so what this does now is uh it call one of the things that it does is it calls the d wiki function and the d wiki function going up a little further i did all this stuff by hand and this is where this is why i say i'm an idiot i did all this stuff by hand because initially i found the wiki text parser which is a let me show you at the top so you can just do a pip install wiki text parser it is exactly what it says on the tin it just parses wiki text to plain text or whatever else you want so yeah so wtp it does everything i wanted it to the reason i didn't do it is because regex was faster but that's where i'm an idiot because i have done so much parallel processing in the past i don't know why it took me so long to figure this out so what i did to make this to make it just as fast to use this really robust function so i just pass in the wiki text and it spits out a plain text version it was really slow if you do just this it takes about like five or six seconds it's two to six seconds per article which is really slow when you've got six million articles right well okay sure but python has this handy dandy built-in stuff where you can just call thread and so what thread does is i just pass it off and i say hey go do this and then don't give me any results i don't care because i this you see this uh the process and save article it doesn't have any return so it just it goes off into oblivion and does its own thing so there you have it uh here's another example this is the uh the entry for ascii so you can see just you should jump right in this is completely legible you could you could use this as is here's another one august william durleth again perfectly legible no html no xml no markdown nothing so and uh let me go back to this let's see if i can zoom in there we go so you can see these articles going by it processes a few dozen articles per second uh overall i estimate uh it does about 1500 articles a minute on average so if it does 1500 articles a minute on average so that's 1 500 zoom in a little bit more um wikipedia once you remove all of the uh all of the the redirects and and disambiguation pages is about five million articles uh so you divide that out and you say okay fifteen hundred articles um per uh per minute divided by or 5 million articles divided by 15 per minute that's 3 300 minutes which is about 55 hours to fully index wikipedia and then it dumps it into a nice easily reusable folder which i've got sorry excuse me uh named wikiplaintext and the file name is the same as the article id i figured i don't need to generate anything fancy i just give it the article id and away it goes and you see the each article they vary in size uh let's see at the top we've got the largest one is a quarter of a megabyte so that's not too big in recreational mathematics of course a math article is the largest one so oh here's a here's a here's a good example of something that it didn't quite get out where it looks like it didn't get a table so i might have to go back and remove some of the table data but really because i'm gonna have this read by gpt3 or similar uh similar what do you

CLEAR ESSAY:

==========

Hey everyone, David Shapiro here. The reason we're recording this video again is because I've revisited the process of parsing Wikipedia. I've changed my methodology so that I'm saving every individual article as a JSON file. This is what it looks like:

So, you've got the article ID (1575), and then here's the actual text. You see that this is lovely plain text - it's easy to read, there's no links or anything. This plain text version can be easily used for anything, whether you want to just print it out or index it in Solar.

I've preserved pretty much all information in this JSON file. Let me show you how I've done this. I've got a supervisor function that kicks off the main function. As I've shown in past videos, I've got it all as the stream. I can also just show you what this looks like. If you go download Wikipedia, it'll look like this:

The file name is a little bit longer, I shortened it up just so that you see that it's the English Wikipedia of the year 2021 of April first. So this is a month and a half old - that's okay. You see it's an 80 gigabyte file though. Because it's a monolithic file, you need to read it one line at a time - it's just too much data to process all at once.

So that leads to everything else that you see here. I've changed this as well. So what I do now is I just save the article. I do the end analysis to get the document ID, title and text. Then I change this uh analyze chunk. I change this as well. So what this does now is it calls the dwiki function. The dwiki function going up a little further does all this stuff by hand. This is where this is why I say I'm an idiot - I did all this stuff by hand because initially I found the wiki text parser, which is a pip install wiki text parser. It is exactly what it says on the tin - it just parses wiki text to plain text or whatever else you want. So yeah, wtp does everything I wanted it to. The reason I didn't do it is because regex was faster, but that's where I'm an idiot because I have done so much parallel processing in the past. I don't know why it took me so long to figure this out.

So what I did to make this just as fast is I just pass in the wiki text and it spits out a plain text version. It was really slow if you just do this - it takes about five or six seconds per article. But Python has this handy dandy built-in stuff where you can just call thread. So what thread does is it just passes off the processing to another function, and then doesn't give me any results. I don't care because in process and save article, it doesn't have any return. So it just goes off into oblivion and does its own thing.

There you have it. Here's another example - this is the entry for ASCII. You can see that it's completely legible. You could use this as is. Here's another one - August William Durleth. Again, perfectly legible, no HTML, no XML, no markdown, nothing.

So overall, I estimate that it does about 1500 articles a minute on average. If it does 1500 articles a minute on average, that's 5 million articles divided by 15 per minute, that's 3,300 minutes, which is about 55 hours to fully index Wikipedia. And then it dumps it into a nice, easily reusable folder which I've named wikiplaintext. The file name is the same as the article ID. I figured I don't need to generate anything fancy - I just give it the article ID and away it goes.

You see that each article varies in size. The largest one is a quarter of a megabyte. So that's not too big. In recreational mathematics, of course a math article is the largest one. Here's a good example of something that it didn't quite get out:

It looks like it didn't get a table. So I might have to go back and remove some of the table data. But really, because I'm going to have this read by GPT3 or similar, I don't need to worry about that.

So there you have it. This is my new methodology for parsing Wikipedia. It's much faster and more efficient than my old one. I hope you find it useful.