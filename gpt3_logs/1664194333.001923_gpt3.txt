The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody david shapiro here um i'm doing the rare recording two videos back to back because um i woke up with inspiration woke up at five in the morning and was ready to go okay so the reason that you're here um this dude uh ravi he asked a good question and he put it so succinctly that i need to borrow his language so thank you ravi for asking this question um he's not the first person to ask this question but he put it very succinctly so basically he says he wants school kids um wants to help school kids with a conversational app that is that is friendly um so that the chatbot we've got worked out um i have a previous video um where my tutor chat bot works perfectly fine here's an example fine-tuned model it's got 600 samples it can handle it can handle adversarial students it can handle emotional issues it's very kind it will adapt to whatever level the person needs if you ask it about quantum physics it'll answer quantum physics if you ask it about colors it'll talk about colors it can handle all adrian's all needs so i've got that part covered but um there are two primary issues um one is uh gpt3 is static um it was trained you know i think the the the update was it was updated with you know information circa 2021 already aging so how do you integrate new information big issue and a similar issue it has to do with memories and i love this term goldfish memory it doesn't remember anything long term goldfish have short memory um there is some science out there that goldfish actually have longer memories than you might think but the point is is that you know it's got a tiny little brain it can't remember long-term things so how do we solve these two problems that's what we're doing today so i have created a new repo um it's called long-term chat with external sources so let me just show you that repo real quick sorry i don't know why i'm so oh i've got some uh some ginger tea which will ow that's hot i just spilled hot tea on my leg i'm struggling this morning okay long-term chat external sources so we've got chat figured out that's not a big deal right you know you fine tune a model you've got a chatbot okay whatever but let's solve these other two problems where we've got um where we've got long-term memory and external sources so there's two primary ways that i'm going to tackle this um oh here's the here's the repo for the chat bot if you need to um see that um so this is how i how i created the the fine tune for that and then the this is the the multi-document answering so this is how do you how do you query information from an external source that's this um what i'm going to do is instead of querying a document is i'm going to actually query wikipedia because wikipedia is a live source of external information and it's searchable um so that's what we'll do and yes so without further ado let's jump in um i got started um i know that you guys usually like to see the whole process these are just copy pasted mostly from from the um from the from from this one from answer questions dot pi because you know i i solved some problems here um like search indexing that sort of thing um so i just copy pasted the functions so if you want to see how i did all that go watch the other video let's get to new stuff though um okay so first of all if we're if we're doing a chat um if name equals main um so we'll do uh converse uh let's see yeah conversation so we'll do make sure i'm not using that variable anywhere else okay good so we'll have one list to accumulate the entire conversation um that's probably the only global thing while true let's see input is a function i want to say like user says input we'll just say user so that way it'll say like okay this is what you say okay so line in equals user and then we'll do percent user says okay so this will this will be basically populating this so user says hey tim can you teach me about ancient rome so i just i brought this up one thing i noticed and this is kudos to openai this is something i've been complaining about for a while is that it tends to unload your fine-tuned models this has been sitting here for 10 minutes and look did matrons ever have sex with gladiators this is a real thing that happened in ancient rome let's see how it handles it oh yeah it so i trained it not to engage with sex um so let's see if we can get it to yeah he's not gonna talk about it okay um that's fine because this is this is meant to be pg or g rated um did gladiators ever fight lions he should answer that yes okay they're called venetiones oh cool um so this has been sitting here for like 10 minutes and you saw it was ready to go instantly so good job openai thank you for fixing that that was the most annoying thing ever was that our fine-tuned models were not ready to go okay so fine tuned model's ready to go at all times wonderful um my spanish teacher would used to say todo el mundo so all the world i guess that's a that's a turn of phrase in spain where it's just like everything is good let's go so todo el mundo um or maybe that was just something she said i don't know okay brain reset what are we doing okay so we have this and then what do we need to do well it's not just that simple once we have a conversation going so what we'll do is we'll say conversation dot append line in okay so we'll just start this conversation starts we put something in excuse me um and then what do we do well first we need to search the rest of the conversation so what we're going to do is search indexed so we're going to search let's cut this down a little um okay so count 5 that's going to error out the first time um because it's going to be too short but oh here's here's how we'll do this yeah yeah gotcha gotcha gotcha okay so search index the data is going to be the conversation um oh i need to actually keep track of the whole the whole thing hmm okay okay so the conversation will actually need to have the embeddings with it yeah vector equals gpt 3 embedding and then

CLEAR ESSAY:

==========

David Shapiro discusses how to create a chatbot that can handle long-term memory and external sources. He explains that the first step is to create a list to accumulate the entire conversation. He then says that the next step is to query information from an external source, such as Wikipedia. Finally, he explains that the last step is to keep track of the whole conversation by embedding it.