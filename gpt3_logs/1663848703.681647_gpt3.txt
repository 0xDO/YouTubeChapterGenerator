Use the following subtitle transcripts to create YouTube chapters. Describe each minute as a chapter with a sentence or two.

TRANSCRIPT:
0:00:00.900,0:00:05.400 morning everybody David Shapiro here with your daily state of the industry update  0:00:06.000,0:00:11.820 as often happens my newsfeed helpfully supplied me with today's topic I think  0:00:11.820,0:00:17.700 it is a very timely topic because I have been diving more into alignment so today's paper  0:00:18.600,0:00:24.300 um it it's actually an older one January 5th 2021 but like I said my newsfeed supplied it to me  0:00:25.320,0:00:29.280 um it's a relatively short paper at least the part that's published is  0:00:29.280,0:00:32.700 uh 12 Pages it's I think it's much longer they just cut some out for  0:00:34.320,0:00:39.900 um for uh for internet publishing but the abstract of this paper is pretty good  0:00:41.100,0:00:46.200 um pretty straightforward super intelligence is a hypothetical agent that possesses intelligence far  0:00:46.200,0:00:52.380 surpassing that of the brightest and most gifted human Minds in light of recent advances in machine  0:00:52.380,0:00:56.220 intelligence a number of scientists philosophers and technologists have revived the discussion  0:00:56.220,0:01:02.160 about the potentially catastrophic risks entailed by such an entity in this article we trace the  0:01:02.160,0:01:06.780 origins and development of the Neo fear of super intelligence and some of the major proposals for  0:01:06.780,0:01:13.320 its containment we argue that total containment is in principle impossible due to fundamental limits  0:01:13.320,0:01:18.960 inherent income in Computing itself assuming that a super intelligence will contain a program that  0:01:18.960,0:01:23.760 includes all the programs that can be executed by a universal turing machine on input potentially  0:01:23.760,0:01:28.380 as complex as the state of the world strict containment requires simulations of such a  0:01:28.380,0:01:34.080 program something theoretically and practically impossible so to put this in other words in order  0:01:34.080,0:01:40.140 to anticipate what the machine is going to do you have to be able to simulate it perfectly including  0:01:40.140,0:01:47.280 the the way that it interacts and the way that the world reacts um basically the whole world  0:01:47.280,0:01:54.300 is too complicated to simulate with any accuracy and so therefore you know if this if this super  0:01:54.300,0:01:58.740 intelligent machine exists it's impossible to calculate what the rest of the world is going to  0:01:58.740,0:02:07.440 do in order to put it in a accurate simulation so we could at best do some you know approximations  0:02:08.940,0:02:18.000 um so skipping down a little bit they have two main classes for can controlling so the  0:02:18.000,0:02:24.240 the first class for controlling the machine is capability control you put it in a box you give  0:02:24.240,0:02:31.800 it uh you create dependence on a reward mechanism that is controlled by us you stunt it you run it  0:02:31.800,0:02:37.620 on inferior hardware and tripwire you you have it automatically shut down if it gets too dangerous  0:02:38.280,0:02:44.280 if you have a machine that can build a copy of itself or that understands programming it could  0:02:44.940,0:02:51.480 you know undo all of those um pretty much motivation selection so uh direct specification  0:02:51.480,0:02:57.480 you say you give it uh hard-coded laws uh Domesticity you teach it to behave within  0:02:57.480,0:03:01.860 certain certain constraints um indirect normativity you endow it with procedures  0:03:01.860,0:03:08.100 of selecting Superior moral rules so this is closest to what I have done in my work benevolent  0:03:08.100,0:03:15.240 by design whereby my proposal is that you give it a set of heuristic imperatives that it will  0:03:15.240,0:03:20.760 um it will ultimately buy into and deliberately choose so that you don't have to control it it  0:03:20.760,0:03:27.480 controls itself because it abides by certain principles that are going to be reliable  0:03:29.400,0:03:35.040 um I believe that this is with the experiments that I've captured in in benevolent by Design I  0:03:35.040,0:03:40.680 believe this is the way to go um augmentation at AI to a benign systems such as the human brain  0:03:40.680,0:03:47.280 so that's you know merging um okay so you can check out the paper if you want to look at their  0:03:47.280,0:03:54.540 discussion on that um but I wanted to go skip down to the um the discussion part uh and then  0:03:54.540,0:04:00.420 I'll I'll share kind of some of my not necessarily criticisms but my own counter thoughts because I  0:04:00.420,0:04:06.120 don't necessarily disagree with anything in this it's a short paper and it's just not quite as uh  0:04:06.120,0:04:10.740 not quite as robust because they're not proposing a solution like I have  0:04:10.740,0:04:16.140 um which is why I'm here okay so today we run billions of computer programs globally on  0:04:16.140,0:04:20.520 connected to machines without any formal guarantee of their absolute safety we have no way of proving  0:04:20.520,0:04:24.540 that when we launch an application on our smartphone our smartphones we would not trigger  0:04:24.540,0:04:29.760 a chain reaction that leads to transmission of missile launch codes that started nuclear war um  0:04:32.700,0:04:34.080 uh  0:04:35.640,0:04:43.320 as a technologist this really hurts um let's talk about firewalls let's talk about security  0:04:43.320,0:04:50.220 protocols you actually can mathematically prove something like that with penetration testing  0:04:51.600,0:04:57.360 um like there's all sorts of controls and constraints that go into every layer of a  0:04:57.360,0:05:03.300 piece of technology such as what that piece of technology can can talk to even the security of  0:05:03.300,0:05:08.820 how it boots up right like we have encrypted boot protocols that ensure that the operating  0:05:08.820,0:05:17.220 system hasn't been tampered with so uh

YouTube chapters for each minute with a good description:

00:00 - David Shapiro gives a short introduction to the topic of the day - super intelligence. He explains that there is a lot of discussion around the topic and that the purpose of the paper is to explore the origins of the Neo fear of super intelligence and some of the major proposals for its containment.
01:00 - Shapiro explains that the paper argues that total containment is in principle impossible due to fundamental limits inherent in computing itself. He goes on to say that in order to anticipate what the machine is going to do, you have to be able to simulate it perfectly, including the way that it interacts and the way that the world reacts - something that is impossible.
02:00 - Shapiro outlines the two main classes for controlling the machine - capability control and motivation selection. He explains that motivation selection is the more promising of the two, as it is less likely to be undone by the machine itself.
03:00 - Shapiro outlines the three types of motivation selection - direct specification, indirect normativity, and benevolent by design. He explains that benevolent by design is the closest to what he has done in his own work and that it is the most promising approach.
04:00 - Shapiro explains that augmentation of AI to a benign system such as the human brain is the most likely way to achieve a benevolent by design machine.
05:00 - Shapiro concludes by saying that we have no way of proving that when we launch an application on our smartphone, our smartphone wouldn't trigger a chain reaction that leads to transmission of missile launch codes that started nuclear war.




TRANSCRIPT:
right there like yes if you if you're not familiar with  0:05:17.220,0:05:22.740 how technology works you could conceivably come to this conclusion so we'll just kind of set  0:05:22.740,0:05:30.420 that on the the B pile of like maybe scientists don't know everything um okay Arthur C Clarke  0:05:30.420,0:05:34.560 wrote a short story dial F from Frankenstein warning that soon all the computers on Earth  0:05:34.560,0:05:40.620 were connected via telephone close enough they could take command of our society um they could  0:05:40.620,0:05:46.740 still use our smartphones and nothing has happened despite the general unsolvability of the program  0:05:46.740,0:05:50.460 prediction problem we are confident for all practical purposes that we are not in one of  0:05:50.460,0:05:57.240 the Troublesome cases okay so practical safety you know can you simulate it can you control it  0:05:58.200,0:06:06.840 um yeah so my point there is uh look up the OSI model and look up security best practices  0:06:08.040,0:06:16.440 um all right so but I jotted down some notes if predictability is the key thing here um why aren't  0:06:16.440,0:06:22.680 humans a bigger problem humans are fundamentally unpredictable so why aren't we a Danger let's  0:06:22.680,0:06:28.560 explore that the reason that human well I mean first humans are dangerous so humans are dangerous  0:06:29.880,0:06:37.920 um but the limit is uh but each individual is limited we only have so much time and energy  0:06:37.920,0:06:44.580 and intelligence that we can apply it per day um so let's just call that um physical limits  0:06:46.380,0:06:52.200 processing energy time those are the primary things is we can only think so fast we can  0:06:52.200,0:06:56.820 only you know punch so many people in the face if we decide to get violent and we only have so  0:06:56.820,0:07:05.340 much time uh per day as well as other constraints like need for food but that falls under energy so  0:07:05.340,0:07:15.540 constraints so law of constraints um so when when we look at the the constraints that humans have  0:07:15.540,0:07:21.900 computers all have the same thing like you can program the most evil smartphone thing but it's  0:07:21.900,0:07:26.640 going to be limited because it's only got its One battery to go on right and it's only got It's one  0:07:26.640,0:07:31.860 4G or 5G connection or Wi-Fi connection and it's also only got a tiny little you know like quad  0:07:31.860,0:07:40.380 core arm processor or whatever um and so when you when you when you talk about a super intelligence  0:07:40.380,0:07:45.360 you have to look at the full stack how much CPU does it have how much RAM does it have  0:07:45.360,0:07:49.740 how much storage does it have how fast are its internet connection so that again I'm thinking  0:07:49.740,0:07:54.180 about this from a technologist perspective what kind of firewalls are around it because you can  0:07:54.180,0:08:00.000 have the smartest thing

YouTube chapters for each minute with a good description:

==========

05:00 - Shapiro discusses Arthur C Clarke's short story "Dial F for Frankenstein" and how it warns that soon all the computers on Earth will be connected and able to take command of our society. He goes on to say that despite the general unsolvability of the program prediction problem, we are confident that we are not in one of the troublesome cases.

06:00 - Shapiro explains that the reason that human beings are not a bigger problem than machines is because each individual is limited by physical constraints such as the amount of time and energy that can be applied per day.

07:00 - Shapiro discusses the law of constraints and how it applies to both human beings and machines. He explains that when you talk about a super intelligence, you have to look at the full stack in order to understand its limitations.