The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody today's episode of state of the industry is going to focus on hardware okay so first um i want to talk about intel because intel is the um you know it's the one of the world leaders in terms of uh uh cpus um they make desktop and server cpus as well as some mobile although arm and a few others dominate the mobile space so sapphire rapids is the code name for their upcoming release which i think is supposed to drop later this year planned for 2022 and so one thing that i saw someone shared this on the discord is that they're going to start supporting amx extension so what is that amx is advanced matrix extensions which basically gives it a little bit more native support for artificial intelligence applications um so this is a step up from avx which is advanced advanced vector extension so it basically turns it from one-dimensional matrices to n-dimensional matrices or two-dimensional matrices which is the bedrock of a.i today because all all deep neural networks use tensor math or matrix math okay so just wanted to shout give a shout out to uh to that to sapphire rapids which means oh so the other thing is um alder lake which is the desktop version is not going to have amx support um or at least rumored not to you see over here on the on the the extensions amx is missing it does still have a vx though so if we go back to sapphire rapids you see it's got avx up to 512 which is even bigger than what they have on the desktop so um basically what i predict that we're going to be seeing is that conventional hardware you know intel chips amd chips are going to start supporting more and more ai related math so that you may or may not need a separate gpu or tpu so right now if you want to do ai locally you need dedicate you need special hardware to accelerate it you can run it on a normal cpu but it's going to be sometimes 100 times slower okay so with that being said i wanted to also introduce the concept of and eight so basically what what this does is most weights are represented as floating point 32 or floating point 16 bit values which means that they are represented by two or four bytes total in order to represent each individual parameter but what happened what you can do what we found is that you can reduce that down to one byte which is uh eight bit and you actually get one it's much much smaller so it takes less memory less storage and the processing is faster and what happens is when you distill or change or quantize a neural network down to 8-bit it is the performance is much better but you don't actually you don't actually lose the the performance of it and so there's a few uh a few papers out here this one came out august 15th so uh almost a month ago um here is a hugging face article about it and i'll have all this in the in the comments um so a gentle introduction to 8-bit matrix multiplication for transformers at scale using hugging face transformers accelerate and bits and bytes um so yeah uh this is so basically i'm showing you the amx because just showing you that matrix math is getting built into cpus and in the meantime we're figuring out more efficiencies in in the transformer space so the hardware is ramping up but then also we're optimizing the neural networks and so what i hope to see before too long is that large language models will be able to run on conventional commodity hardware that'll be ideal uh okay so that's kind of the two the two uh opening things that i wanted to show you so sapphire rapids uh will have amx support which will make it faster to run matrices and then 8-bit matrices or 8-bit parameters are also on the rise okay so i mentioned quantization so what is that so quantization takes a smooth curve and turns it into a stepped curve so this is probably the best example so you take a floating point 32 value and turn it into an 8-bit value so it's actually simpler you represent the same you know waveform or curve or data just with lower resolution and with neural networks because you have so many parameters it kind of doesn't matter so there you have it you could you can represent it very simply so taking that one step further you can also do analog neural networks so i remember many years ago when the mem resistor was announced i'm trying to find it analog computing okay so this is the idea of a mem resistor or a tunable resistor so basically a parameter is just it affects the way information flows through a matrix so what you can do is you can do this at the hardware level where you've got a matrix and then you got a bunch of resistors and the resistors change the current flowing through the circuit but if you can modify those resistors which is a mem resistor or a tunable resistor you basically bring compute and memory together so this is the compute in memory processing element basically you embed a deep neural network or at lea maybe this is just one layer i'm not sure the full architecture of this yet but mythic ai is a company and so they're producing these chips that have a hardware level neural networks and they take a lot less power um and uh let's see i think this is the the layers um yeah here you go layers um so basically you plot out all the layers that you need on hardware and it feeds them one into the next um so there you there you go that is an example of a of a hardware level neural network i was trying to find an article that someone had posted i think on the open ai community it was months ago but where a scientist or a team of researchers actually created a neural network that used i think it was just ambient heat or vibrations of different materials um and so like you could have neural networks that did processing just with the ambient heat passing through like aluminum and glass i could be remembering that wrong but basically the idea is that all materials do some sort of information processing by virtue of the transmit heat and sound and some transmit light and so the idea is that because because the energy

CLEAR ESSAY:

==========

Sapphire Rapids is the code name for Intel's upcoming release, which is planned for 2022. One thing that has been announced is that they will start supporting amx extension.

AMX is advanced matrix extension which gives it more native support for artificial intelligence applications. This is a step up from avx, which is advanced vector extension.

Alder Lake, the desktop version, is not rumored to have amx support. However, it does have avx.

With that being said, I wanted to also introduce the concept of 8-bit matrix multiplication. Basically, what this does is it takes weights that are usually represented as floating point 32 or 16 bit values, and reduces them down to one byte.

The advantage of this is that it takes less memory, less storage, and the processing is faster.

What I hope to see before too long is that large language models will be able to run on conventional commodity hardware. This would be ideal.