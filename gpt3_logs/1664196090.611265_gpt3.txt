The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: need short things so this is 501 characters so this is perfect for the universal sentence encoder now you might say okay well it's only 512 dimensions aren't you losing a lot of dimensionality yes but if you're just trying to search for similar memories that's all you need you don't need a da vinci sized um embedding which is 10 000 dimensions you know you've got 501 characters and it's going to be rendered in 512 dimensions in fact even even that embedding is going to be um several times more text if you rendered it as text than this so the embeddings are all are larger than the in than the input that's okay for right now but 512 is plenty because remember this is just you're just gonna you're casting a wide net you're not doing any other work on it these are just embeddings for search so this is what the embedding service does it does one thing it does it well you give it a list of strings it sends back a list of embeddings and that's that um it's a it's a it runs on flask because flask is good for rapid prototyping and this is all still in prototype phase obviously once we get to a more enterprise scale later on because this is this is working on raven version one so raven is my um is my uh agi project um it stands for um real time assistant vastly extensible network here's the extensible network it allows for arbitrary number of microservices to be contributing to the nexus it also allows you to swap out those microservices so you you have a new version but also these microservices will ultimately be responsible for updating their own models in the future so this is why i've done so much work on fine tuning so fine tuning um well a couple things one the nexus is going to be a repository of millions billions trillions of memories which is a huge amount of data and then what these each of these microservices is going to do eventually is they're going to measure their own performance in part with feedback from the conductor which we'll get to in a future video but with feedback from the conductor these microservices will curate and build their own fine-tuning data sets and then they'll do a b testing for their current model and then newer models to determine which ones are better and ideally these models will get better over time they'll have better understanding of the world and also better understanding of how to behave okay so that's that that's the embedding microservice let's go back nexus so we'll do the nexus real quick um here's the nexus this is the primary one um let's see it's it's got it's so simple um all that this does is you look at the endpoints so app route add so this is how you add a um add a memory to the nexus um and it does it gives you a little bit of output um i just added actually um well no i won't worry about it i added a log file location um so that you can read the read the files without having to open it up right now it just saves it as a pickle but even then i don't i actually don't save most the time because it can just run in memory for while i'm testing i don't need to record everything but in the future you'll obviously want to record all memories save it to disk somewhere a pickle is not going to be the optimal thing for saving this i need to work that out with with indexing and semantic search things i'll probably integrate feis the facebook ai semantic search engine that can scale to a trillion memories so that'll probably be running sidecar to this somehow there's a few endpoints in this so first is add second is search and those are the two primary functions so the search endpoint in the nexus is a very basic semantic similarity search this is this is uh someone was using vdblight which is what this is based on and they said that vdblight was fine up until about 400 000 records so 400 000 memories is more than enough for experimentation um until we start running into those optimization problems 400 000 memories is pretty good and keep in mind that a memory is going to be each memory is going to be about this size so about 500 characters 400 000 memories um let's see 500 characters times 400 000 so that's 200 million um characters that's quite a bit that's quite a bit of information that can be stored here again more than enough for experimentation and development and then we'll work on optimization later so this search is the semantic search function um fetch is give this is this is a function that i haven't even had to use yet so the fetch feature is going to be what's what um uh what these microservices use to pull data sets um it's a it's gonna say give me all um all memories or all records that match very specific criteria um so this is gonna be a big a big thing um like so say for instance you want to give me all um give me all memories that were created by the imperative service then give me all that so that i can look at my own performance or give me all feedback written by the conductor so that i can measure my own performance based on what the conductor says so that's what the fetch does and then bound so this is the temporal memory function where it just says give me all memories between time stamp a and timestamp b um so say for instance if you do a semantic search and you end up with you know one of the memories it says it gives you was from you know eight minutes ago and then another one was from eight days ago you have both of those memories but they're floating in isolation and that doesn't do you much good so then what you what you might want to do is say okay for the memory that was most similar um you know from eight minutes ago now give me everything from you know ten minutes ago to six minutes ago so i've got four minutes of context so this is this is another way that um human memory is associative so it's associative based on content but it's also associative based on time temporal proximity and so the between having the search function and the bound function you can find any memory regardless of when it happened and then you

CLEAR ESSAY:

==========

The embedding microservice is responsible for taking a list of strings and returning a list of 512-dimensional embeddings. This service is important for the nexus, which is a repository of memories that can be searched for using semantic similarity. The microservices that make up the nexus will eventually be responsible for updating their own models, which will allow the system to get better over time.

The search endpoint in the nexus is a basic semantic similarity search that can handle 400,000 memories. This is more than enough for experimentation and development. The fetch endpoint is used by microservices to pull data sets, and the bound endpoint is used to find memories based on temporal proximity. Together, these features allow for a human-like memory system that is associative and constantly improving.