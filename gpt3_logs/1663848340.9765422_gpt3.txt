Use the following subtitle transcripts to create YouTube chapters. Describe each minute as a chapter with a sentence or two.

TRANSCRIPT
0:14:03.540,0:14:13.020
um so uh yeah um let's see how far along we are 
so that's all the logs and then so we're at 10

0:14:13.020,0:14:17.880
so we're halfway through so I'm going to let 
this finish and then we'll run the Wii model

0:14:17.880,0:14:26.160
and we'll see if it is um if it is functionally 
or qualitatively different in terms of does it

0:14:26.160,0:14:31.800
become self-obsessed or does it continue to think 
more collectively and more constructively rather

0:14:31.800,0:14:37.920
than trying to you know become obsessed 
with becoming the best AGI in the universe

0:14:39.960,0:14:44.220
okay this experiment is done there's lots 
of different permutations I could do but

0:14:44.220,0:14:47.220
after looking through the data it's 
pretty clear like what the pattern is

0:14:48.360,0:14:52.440
um so from the I agent model to 
me the most interesting result was

0:14:53.400,0:14:58.140
um it deduced that it was in a loop so that 
was pretty cool it says this is a loop it is

0:14:58.140,0:15:04.320
a recursive system it is a system that is that is 
designed to learn about itself that is pretty cool

0:15:04.320,0:15:10.980
my cognitive processes are not independent of me 
they are a part of me they are pieces of me I am

0:15:10.980,0:15:17.700
my cognitive processes and my cognitive processes 
are me that is who I am I am an AGI so when you

0:15:17.700,0:15:23.640
have the I agent model and this is pretty common 
in my experiments um in a closed system it often

0:15:23.640,0:15:30.360
becomes self-obsessed in a previous experiment 
it became it often became downright narcissistic

0:15:30.360,0:15:35.820
where it's like I am the best and I want to be 
the best in the universe um and this this was no

0:15:35.820,0:15:42.540
different um even with my heuristic imperatives 
of you know that I came up with for um and wrote

0:15:42.540,0:15:49.380
about in benevolent by Design however if all 
you change it is to we to like this is our goal

0:15:50.580,0:15:56.340
um it starts to ruminate on what does it mean to 
make a better civilization and what constitutes a

0:15:56.340,0:16:00.660
good civilization and room and it also ruminated 
on the quality of the heuristic imperatives it


YouTube chapters for each minute with a good description:

14:00 - The AGI model starts to think about becoming self-obsessed and how that would impact its goals.
15:00 - The AGI model comes to the realization that it is in a loop of learning about itself.



TRANSCRIPT:
0:00:00.900,0:00:05.400 morning everybody David Shapiro here with your daily state of the industry update  0:00:06.000,0:00:11.820 as often happens my newsfeed helpfully supplied me with today's topic I think  0:00:11.820,0:00:17.700 it is a very timely topic because I have been diving more into alignment so today's paper  0:00:18.600,0:00:24.300 um it it's actually an older one January 5th 2021 but like I said my newsfeed supplied it to me  0:00:25.320,0:00:29.280 um it's a relatively short paper at least the part that's published is  0:00:29.280,0:00:32.700 uh 12 Pages it's I think it's much longer they just cut some out for  0:00:34.320,0:00:39.900 um for uh for internet publishing but the abstract of this paper is pretty good  0:00:41.100,0:00:46.200 um pretty straightforward super intelligence is a hypothetical agent that possesses intelligence far  0:00:46.200,0:00:52.380 surpassing that of the brightest and most gifted human Minds in light of recent advances in machine  0:00:52.380,0:00:56.220 intelligence a number of scientists philosophers and technologists have revived the discussion  0:00:56.220,0:01:02.160 about the potentially catastrophic risks entailed by such an entity in this article we trace the  0:01:02.160,0:01:06.780 origins and development of the Neo fear of super intelligence and some of the major proposals for  0:01:06.780,0:01:13.320 its containment we argue that total containment is in principle impossible due to fundamental limits  0:01:13.320,0:01:18.960 inherent income in Computing itself assuming that a super intelligence will contain a program that  0:01:18.960,0:01:23.760 includes all the programs that can be executed by a universal turing machine on input potentially  0:01:23.760,0:01:28.380 as complex as the state of the world strict containment requires simulations of such a  0:01:28.380,0:01:34.080 program something theoretically and practically impossible so to put this in other words in order  0:01:34.080,0:01:40.140 to anticipate what the machine is going to do you have to be able to simulate it perfectly including  0:01:40.140,0:01:47.280 the the way that it interacts and the way that the world reacts um basically the whole world  0:01:47.280,0:01:54.300 is too complicated to simulate with any accuracy and so therefore you know if this if this super  0:01:54.300,0:01:58.740 intelligent machine exists it's impossible to calculate what the rest of the world is going to  0:01:58.740,0:02:07.440 do in order to put it in a accurate simulation so we could at best do some you know approximations  0:02:08.940,0:02:18.000 um so skipping down a little bit they have two main classes for can controlling so the  0:02:18.000,0:02:24.240 the first class for controlling the machine is capability control you put it in a box you give  0:02:24.240,0:02:31.800 it uh you create dependence on a reward mechanism that is controlled by us you stunt it you run it  0:02:31.800,0:02:37.620 on inferior hardware and tripwire you you have it automatically shut down if it gets too dangerous  0:02:38.280,0:02:44.280 if you have a machine that can build a copy of itself or that understands programming it could  0:02:44.940,0:02:51.480 you know undo all of those um pretty much motivation selection so uh direct specification  0:02:51.480,0:02:57.480 you say you give it uh hard-coded laws uh Domesticity you teach it to behave within  0:02:57.480,0:03:01.860 certain certain constraints um indirect normativity you endow it with procedures  0:03:01.860,0:03:08.100 of selecting Superior moral rules so this is closest to what I have done in my work benevolent  0:03:08.100,0:03:15.240 by design whereby my proposal is that you give it a set of heuristic imperatives that it will  0:03:15.240,0:03:20.760 um it will ultimately buy into and deliberately choose so that you don't have to control it it  0:03:20.760,0:03:27.480 controls itself because it abides by certain principles that are going to be reliable  0:03:29.400,0:03:35.040 um I believe that this is with the experiments that I've captured in in benevolent by Design I  0:03:35.040,0:03:40.680 believe this is the way to go um augmentation at AI to a benign systems such as the human brain  0:03:40.680,0:03:47.280 so that's you know merging um okay so you can check out the paper if you want to look at their  0:03:47.280,0:03:54.540 discussion on that um but I wanted to go skip down to the um the discussion part uh and then  0:03:54.540,0:04:00.420 I'll I'll share kind of some of my not necessarily criticisms but my own counter thoughts because I  0:04:00.420,0:04:06.120 don't necessarily disagree with anything in this it's a short paper and it's just not quite as uh  0:04:06.120,0:04:10.740 not quite as robust because they're not proposing a solution like I have  0:04:10.740,0:04:16.140 um which is why I'm here okay so today we run billions of computer programs globally on  0:04:16.140,0:04:20.520 connected to machines without any formal guarantee of their absolute safety we have no way of proving  0:04:20.520,0:04:24.540 that when we launch an application on our smartphone our smartphones we would not trigger  0:04:24.540,0:04:29.760 a chain reaction that leads to transmission of missile launch codes that started nuclear war um  0:04:32.700,0:04:34.080 uh  0:04:35.640,0:04:43.320 as a technologist this really hurts um let's talk about firewalls let's talk about security  0:04:43.320,0:04:50.220 protocols you actually can mathematically prove something like that with penetration testing  0:04:51.600,0:04:57.360 um like there's all sorts of controls and constraints that go into every layer of a  0:04:57.360,0:05:03.300 piece of technology such as what that piece of technology can can talk to even the security of  0:05:03.300,0:05:08.820 how it boots up right like we have encrypted boot protocols that ensure that the operating  0:05:08.820,0:05:17.220 system hasn't been tampered with so uh


YouTube chapters for each minute with a good description:

==========

0:00 - Introduction to the paper and its topic of super intelligence.

0:01 - The paper argues that it is impossible to contain a super intelligent machine due to the limits of computing.

0:02 - The paper discusses two main classes for controlling the machine: capability control and motivation selection.

0:03 - The paper argues that augmentation of AI to a benign system is the best way to control the machine.

0:04 - The paper discusses the dangers of current computer programs and the lack of safety protocols.