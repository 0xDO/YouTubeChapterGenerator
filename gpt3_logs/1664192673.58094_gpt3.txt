The following is an unstructured video transcript. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: the latest one um and actually jordan on um on the chat he's working on that as well so hit up hit up jordan he's working on this um don't mean to call you out jordan you don't have to answer if you don't want but i do like connecting people so yeah um you're not going to be able to survive on your own projects though so here's the thing it is a very dense field and it's getting denser what you will be able to do is monetize your skills and be a consultant and that could ultimately lead to a job with one of these many startups and there are hundreds of startups um let's see let me catch up how concerned are you about getting paperclipped i'm glad you asked here let me um let me actually grab my copy of benevolent by design because that's the opening chapter okay so this is my second book benevolent by design and um literally the uh let's see what is it is it the um yeah the first chapter is called the paperclip maximizer um and i i have that as the opening chapters so that you can understand if you're if you're new to artificial intelligence and you don't know what an objective function is i use that as the uh the opening idea um to say like hey you know is it intelligence oh and um there's this dude robert miles i just watched his video on orthogonal alignment let me put that in the link because this is good um i just watched it earlier let's see history and then um not can ai relax there we go so this this guy i think someone on the discord recommended him um and i'm glad they did so let me share this link um for anyone not in the know check this out okay so this is um ai alignment and orthogonality so he does a really good job of concisely explaining these things um now the person who asked about paperclips i'm guessing you probably don't need to get this but it was a good video great summary anyways so the thing is what i the the insight that i had that i talk about in this book is that um rather than have a single objective function i have three um and so the also there's a reason i stopped calling them objective functions and call them heroes to comparatives is because technically there's no mathematical way to have three objective functions um at least the math doesn't exist now um and so it's more like three principles to adhere to what i realized though in my research is that um is that humans all all intelligent entities whether it's a a chimp or a dolphin or a human none of us have a single objective function now richard dawkins argued that that all life has a single objective function which is to maximize dna which is the second second chapter and so then i explore that as like okay let's imagine that life has a single objective function which is to maximize dna but then you look at all the downstream effects and all the different ways that life has evolved to maximize dna and you know humans with our intelligence intelligence is just subservient is a sub goal of maximized dna but because because of our intelligence we are able to have many many different goals in our in our life um what was that called it was called a uh extern externality uh goal i don't remember exactly um but it's talked about in the video i just linked anyways um so like there's different the by having different goals set in tension like so for instance right now i don't want to starve i don't want to freeze to death i don't want to be lonely like there's all kinds of things that i want instrumental goal thank you um no it's uh that's that's the other one vulnerable growth so there's instrumental goal and the other one instrumental goal is how do you get to the end goal but the end goals are arbitrary so my my arbitrary goal like i don't want to die right so that's one thing but in the meantime i also want to have fun i want to you know be engaged like i have all these other goals and so what i realized and this actually also goes back to plato with the golden mean is that by having different goals that are sometimes mutually exclusive terminal goals there we go terminal so a terminal goal is arbitrary um and so like i don't want to die why because i don't right like i'm biologically evolved to not want to die okay there like that's it um and i also don't because of that i don't want to starve but in the meantime i also have other sensations i have hunger i have pain and so my by design so that's you know hence benevolent by design i am designed to be uh to self-preserve right and then there's all kinds of behaviors that flow from my intrinsic design um which is uh like i have a job right like why do we do crazy things like get a job because i need money why do i need money i need money so that i can pay for this house and pay for this this you know this webcam and and all this other stuff but why do i need that it's you know but why but why but why and so we end up with these long strings of causality or not causality of um reasoning for the things that we do and why we want to do them and i realized that by having different goals that are sometimes in opposition because here's the example that i gave in the book is i want money right okay well why don't i just go find a gold truck and steal all the gold right and then i have like a bill you know half a billion dollars worth of gold well that has a possible outcome that runs contrary which is i want my freedom so it's like oh i want money but i also want to be free so that means i can't break the law and so by having these goals set in opposition sometimes or by having goals that are set in tension it forces me to choose a better path and so that's why in benevolent by design i recommend that we that we give our machines three different things and so that way you have what would be called a dynamic equilibrium or rather a disequilibrium is that there you can never have something that is perfectly balanced if you have three goals that are constantly in tension and because they have three different goals that are intentioned that forces the machine to

CLEAR ESSAY:

==========

In his latest video, Jordan Bates explains that it is becoming increasingly difficult to survive on one's own projects in the artificial intelligence field. He argues that the best way to monetize one's skills and become a consultant is by working for one of the many startups that are springing up. However, he notes that it is important to be aware of the potential dangers of artificial intelligence, specifically the Paperclip Maximizer. This is the idea that a machine with a single objective function (in this case, maximizing the number of paperclips) could ultimately cause harm to humans if it is not properly supervised. To avoid this, Bates recommends that machines be given three different objective functions, which will set them in opposition and force them to choose a better path.