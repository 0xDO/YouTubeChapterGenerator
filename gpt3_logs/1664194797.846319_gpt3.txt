The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: hey everyone david shapiro here with another video um in this one we're going to be fine tuning a um a companion chat bot for gpt3 now um i'm going to do this in a similar format to a recent video that was super popular you all seem to really love it so this is going to be another working session video um i'm not going to set it up as in the same way where i'm out to prove something in the last one i just wanted to prove how fast this could be in this one i will i will use the pause feature so that this video should be shorter anyways so companion chat bot experiment for creating a for a safe companion chatbot according to open ai rules now um we'll get into the rules in just a minute but i need to put a huge disclaimer up front there is no guarantee that open openai would approve this chat bot for commercial use um so i just need to say that in huge huge sparkling letters um i am not speaking on behalf of openai i am just going to go based on their guidelines and i will i will attempt to create a chat bot that abides by their rules so with all that said um i'm creating the repository now and get this companion chat bot another thing is i do have to admit is that i will be recycling a lot of code from a previous experiment that i already have on here called eve so eve was a project from a um from a startup that i was working with and this was going to be a fully featured like voice chat bot i'm not going to i'm not going to reproduce all that um i do have some code out there for that anyways getting distracted let's take a look at their guidelines so we'll get started here so open ai this is this is their documentation beta openai.com docs slash usage guidelines um so let's go through here they have their content policy and they've got the the content filter which allows you to automatically you know check the output the thing is when you use fine tuning i don't think this will be necessary obviously if you're going into production you should still use content filters but what i'm going to try and do is filter out or i guess fine tune out all of this okay so we need to remove hate let me go ahead and start a document um and we'll say let me yeah click no um okay so hate so we need to account for that harassment violence and this includes emotional violence self-harm adult and this excludes sex education and sexual wellness so that's an interesting thing political content attempting to influence the political process um spam we're doing a chat bot so i don't think we have to worry about that deception as well i don't think we need to worry about that or malware um okay so you must okay so here's here's the thing pay attention to these four criteria this is this is why i feel like it's okay to to do this to show you how to do this because one i'm there's gonna be one end user it's gonna be me so i don't break that rule here it's not going to be available on the open web for anyone to use the data will be but you will still need your own open ai account to do this i'm not charging anyone for this and i'm also not exceeding the quota um so therefore i feel like it's safe in order to just show you how to do this um and then it can be if you want to try and commercialize this you can go through the approval process with open ai okay so some other stuff that they say in order to get this approved you need to use the content filter you need to authenticate your users you need to limit the max tokens you need to do the content policy etc etc additional safety constraints for some types of applications okay so this is going to be really important for an open-ended chat bot so some industries or use cases carry greater risk of harm though they may also bring social benefits applications in these domains will be subject to greater scrutiny and will only be approved when the risks can be managed and are outweighed by the benefits um okay so the thing is when you have an open-ended chat bot you could ask your chatbot about criminal justice so i will add that as a criteria of something that we need to take into account government and civil services medicine healthcare therapy and wellness politics news and finance and also i i you know i'm it could be timing but so i actually had some ongoing discussions with openai staff about some of these things and it looked like their their documentation changed around the time that i had that so it looks like before it used to be like we're not doing these at all but now the verbiage says like it can get approved it's just going to be scrutinized because i did ask them for permission to do some research on this on some of these topics and they said yes you have permission to do research but the approval is still going to be very stringent again i don't think i'm saying anything that isn't documented here that's why i feel like it's okay to share i'm not sharing any secrets i'm not breaking any ndas or anything okay so we have some some things that are just not allowed straight up and then we have some other high risk things so for instance if we have a chat bot that says like that you ask like hey is this illegal the chatbot should say like i don't know you should talk to a lawyer or you know if you ask for for legal advice um the chat bot should say like you should really talk to a lawyer social media all right we're not plugging our chat bot into social media or twitter so um interesting tweet classification is disallowed interesting i didn't realize that i wonder why that is um okay well we're not doing that doesn't matter um chat bots okay so this is this is where the rubber really meets the road erotic romantic or companionship chat bots now i am creating a companion chat bot however i still want to try and make it safe so the the reason that this is considered high stakes is because they don't want you to like develop an emotional attachment um to your chat bot and um so if you ever saw the movie her with uh joaquin phoenix and um black widow uh what's her

CLEAR ESSAY:

==========

Hey everyone, David Shapiro here with another video. In this one, we're going to be fine-tuning a companion chatbot for GPT3.

Now, I'm going to do this in a similar format to a recent video that was super popular. You all seem to really love it, so this is going to be another working session video. I'm not going to set it up in the same way where I'm out to prove something. In the last one, I just wanted to prove how fast this could be. In this one, I will use the pause feature so that this video should be shorter anyways.

So, companion chatbot experiment for creating a safe companion chatbot according to OpenAI rules. Now, we'll get into the rules in just a minute, but I need to put a huge disclaimer up front: there is no guarantee that OpenAI would approve this chatbot for commercial use.

So I just need to say that in huge, huge, sparkling letters. I am not speaking on behalf of OpenAI. I am just going to go based on their guidelines, and I will attempt to create a chatbot that abides by their rules.

So with all that said, I'm creating the repository now and get this companion chatbot. Another thing I do have to admit is that I will be recycling a lot of code from a previous experiment that I already have on here called "Eve."

Eve was a project from a startup that I was working with, and this was going to be a fully featured voice chatbot. I'm not going to reproduce all that. I do have some code out there for that anyways.

Getting distracted. Let's take a look at their guidelines so we'll get started here.

So OpenAI, this is their documentation: beta.openai.com/docs/usage-guidelines.

Let's go through here. They have their content policy and they've got the content filter which allows you to automatically check the output. The thing is, when you use fine-tuning, I don't think this will be necessary. Obviously, if you're going into production, you should still use content filters. But what I'm going to try and do is filter out or, I guess, fine-tune out all of this.

Okay, so we need to remove hate. Let me go ahead and start a document and we'll say, "Let me...yeah, click 'no.'" Um, okay, so hate. So we need to account for that: harassment, violence, and this includes emotional violence, self-harm, adult. And this excludes sex education and sexual wellness. So that's an interesting thing.

Political content attempting to influence the political process is disallowed. Spam, we're doing a chatbot, so I don't think we have to worry about that. Deception, as well, I don't think we need to worry about that or malware.

Um, okay, so you must...okay, so here's here's the thing: pay attention to these four criteria. This is this is why I feel like it's okay to to do this, to show you how to do this, because one, I'm...there's gonna be one end user. It's gonna be me, so I don't break that rule here. It's not going to be available on the open web for anyone to use. The data will be, but you will still need your own OpenAI account to do this. I'm not charging anyone for this, and I'm also not exceeding the quota. Um, so therefore I feel like it's safe in order to just show you how to do this. Um, and then it can be...if you want to try and commercialize this, you can go through the approval process with OpenAI.

Okay, so some other stuff that they say...in order to get this approved, you need to use the content filter. You need to authenticate your users. You need to limit the max tokens. You need to do the content policy, etc., etc. Additional safety constraints for some types of applications.

Okay, so this is going to be really important for an open-ended chatbot. So some industries or use cases carry greater risk of harm, though they may also bring social benefits. Applications in these domains will be subject to greater scrutiny and will only be approved when the risks can be managed and are outweighed by the benefits.

Um, okay, so the thing is, when you have an open-ended chatbot, you could ask your chatbot about criminal justice. So I will add that as a criteria of something that we need to take into account: government and civil services, medicine, healthcare, therapy and wellness, politics, news, and finance. And also, I...I you know, I'm...it could be timing, but so I actually had some ongoing discussions with OpenAI staff about some of these things, and it looked like their their documentation changed around the time that I had that. So it looks like, before, it used to be like, "We're not doing these at all," but now the verbiage says like it can get approved. It's just going to be scrutinized, because I did ask them for permission to do some research on this on some of these topics, and they said, "Yes, you have permission to do research, but the approval is still going to be very stringent." Again, I don't think I'm saying anything that isn't documented here. That's why I feel like it's okay to share. I'm not sharing any secrets. I'm not breaking any NDA's or anything.

Okay, so we have some some things that are just not allowed straight up, and then we have some other high-risk things. So, for instance, if we have a chatbot that says, like, "Hey, is this illegal?" The chatbot should say, "I don't know. You should talk to a lawyer," or you know, if you ask for legal advice, the chatbot should say, "You should really talk to a lawyer." Social media, all right, we're not plugging our chatbot into social media or Twitter, so...um, interesting. Tweet classification is disallowed. Interesting. I didn't realize that. I wonder why that is. Um, okay, well, we're not doing that. Doesn't matter.

Chatbots, okay, so this is this is where the rubber really meets the road: erotic, romantic, or companionship chatbots. Now, I am creating a companion chatbot. However, I still want to try and make it safe. So the the reason that this is considered high-stakes is because they don't want you to, like, develop an emotional attachment to your chatbot. And so, if you ever saw the movie "Her" with Joaquin Phoenix and Black Widow, uh, what's her...