The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: hey everybody david shapiro here with a video about gpt3 fine tuning and generating synthetic data sets you can find me on github and it's literally just dave shap that's me you can find my picture so gpt3 and fine tuning are super popular right now and i have gotten really good at creating synthetic data sets what do i mean by a synthetic data set so in data science or machine learning a synthetic excuse me a synthetic data set is data that is artificially created it's data that you create using other machines or programming and uh so the ultimate result let me just kind of show you what i've recently created so two weeks ago i put all of my fine-tuned data together in one repository i've got a compassionate chat bot here's the data right here it's ready to go you can put it you can grab it and fine tune your own chat bot i've got some core objective functions which are really big but it's basically my core objective functions are how to create a benevolent agi and then lastly a question generator so question generator is you give it a topic and it asks relevant questions about it so i've got these three very different cases about synthetic data set and fine-tuning gpt-3 i'm not going to spend any time kind of showing you what those actually no i take that back playground so let me show you how these actually behave so scroll down here and i will grab um fine tune that's curie that's okay um user so let's say hey eve i'm sad and then um it'll go from there uh it'll probably fail but i'll keep talking while it's loading so anyways um this this uh fine-tuned model took only 200 samples and i've got a chat bot that is uh pretty powerful um [Music] i created this idea there you go um so it's just creating this whole conversation i forgot to add the stop sequence so that it can so it will not keep talking okay so the stop sequence will be user and eve so that means it'll stop so let's see what eve says user i'm sad um i'm sorry to hear that what's wrong user um my cat died eve how did she die okay so you see where this um this chat bot is it's uses a technique called compassionate listening okay great so how did i how did i come up with this well first we need to go we need to rewind and i need to tell you about synthetic data sets all right so synthetic creating synthetic data sets is difficult but it might be easier than actually going and getting real data from the real world or curating a data set by hand so here is an example of where i started this was the first synthetic data set that i made you can go explore all of this it's um the nauka question generator so i started with a whole bunch of stuff that i pulled from all over the internet these are all publicly available data sets i found most of them either on kaggle or google data sets just search for nlp data sets and you'll get stuff like this where it's like here's a news article that's great here's an entertainment article again great so this is all bbc news this is all stuff that's pulled from reddit it's way too big there's also plenty of scripts to um to pull data from reddit there's medical stuff movie lines this is the cornell movie database um stack exchange um so basically you start with some raw data whether that is whether that comes from stuff that you scrape from the internet yourself or gutenberg gutenberg is another great place to get um text data so then what i did was i wrote a few scripts to kind of pull that and massage it into what i call contexts so context is like some fuel it's some food for thought it's just a basic text file scroll all the way down um it goes way down okay well it's not going to show me anything other than a dialogue that's fine so the point being here is that you start with some raw material and you can there i will show you how to generate raw material in the future later in the video but this is this is where i started so i'm just kind of walking you through the story so what i did then was once you get some context um this folder has like 50 000 contexts in it to choose from again feel free to use it yourself then you write some gpt-3 prompts so you say uh let's see write a list of the most important and salient questions an observer would ask about the following passage it took a lot of tweaking to get that to get this prompt right so let me grab this prompt we'll go back over to the gpt3 playground and we'll switch back over to text davinci important observer questions and so let's then also go back to grab one of these contexts so we'll just grab a random dialogue uh that's not quite long enough usually does better if there's a little bit more information so just click around until i find a longer there we go perfect okay so we'll grab this this is a context and because i used medical text reddit posts all kinds of stuff in creating this synthetic data you can see this is what you're seeing right here is i you just automate this you put in a whole bunch of stuff you ask gpt3 to um [Music] uh to generate a list of questions and then you record that output so let's see what it says and these are all default settings too um so it'll just say okay what is the dynamic between the characters what is the settings of the passage what is the tone of the passage great so it has intuited that that what we're seeing is um is like dialogue happening between individuals um if we run this again with a higher temperature it might ask questions about the characters so let's turn the temperature up there you go so you see it has asked like what is their relationship um and then so if you if you do this again and let's just be like well no i'm not going to do that you can you can play with it yourself these these prompts are available online for free so you run a few hundred or a few thousand different contexts and you generate the questions and you store those questions um so we're already most of the way done with this first data set or this first um method sorry let me mute my phone i did not

CLEAR ESSAY:

==========

In this video, I'm going to show you how to generate synthetic data sets using GPT3. This is a popular technique right now, and I have gotten really good at it.

A synthetic data set is data that is artificially created. You can create it using other machines or programming. The ultimate result is data that looks and behaves like real data, but is actually artificially generated.

I recently created a synthetic data set that includes a chat bot. This chat bot is based on the technique of compassionate listening. I created it by first collecting a bunch of publicly available data sets. I then wrote some scripts to massage this data into what I call contexts. A context is like some fuel or food for thought - it's just a basic text file.

Once I had a bunch of contexts, I wrote some GPT3 prompts. A prompt is a question or prompt that you give to GPT3, and it will generate answers based on the context you provide. I then recorded the output of the GPT3 prompts.

This process can be automated, so that you can generate a synthetic data set with very little effort.