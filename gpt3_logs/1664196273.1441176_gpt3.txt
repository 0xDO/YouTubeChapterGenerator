The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: proliferation which was once the scientific community and the militaries around the world figured out a few problems about making radioactive isotopes and refining them we had nuclear proliferation where we went from a you know a couple dozen nuclear weapons are on the globe to like over a hundred thousand in the space of a decade and so what we're going to see is we're going to see an ai proliferation especially as language models get more more open source and more efficient and so your research really needs to be focused on well here let me just bring it up symphony of thought um let's see we'll bring up the pdf version and yeah so ch ch where did i have it in here ethical paradigms universals okay it'll be down here somewhere more pages um anyways technological trends yep so this this is this is where i make the argument like look this stuff is growing exponentially we need to get a move on um my book is less salty than this video i promise i talk about why model ethics and morality and then i talk about how complex it is okay so here's here's the four conditions for what i call good alignment research or control problem research one universally applicable whatever framework it is it has to be universal now the thing is is that morality and ethics and alignment are so complex that you cannot just choose one framework so the conclusion of this book is you have to you can't just create a moral framework you have to have a thinking machine that can that can think about all moral frameworks um it has to be flexible and adaptable uh so i kept saying interpretable i apologize flexible and adaptable so for it to be universal it has to learn and adapt over time so again talking about it has to be a thinking machine it has to be a learning machine it has to be eternally robust so robustness means that it has to be resilient not only against its own drift but it has to be resilient against external things because you don't want an edge case where an agis operational parameters change because something in the world changes so for instance um you know humans will become vicious if we are scared right or if we're in a um if we're in a war people will do unspeakably horrible things just to you know just to survive but you don't want an agi for its for its um purpose to break just because the conditions change and then finally it has to be implementable if your research is not implementable like i got real bad news for you like you're falling behind so those are the four four conditions for what i call like good like agi research right now um if you just if you're just speculating about how agi will work um and not testing something which again this is this is why i really approved of open ai because one of the things they say is like we are going to be testing things like we're actually going to be testing um let's see what was implement i don't maybe it wasn't on this blog anyways but they did say like very explicitly like we're going to test stuff we're not just going to you know hypothesize um yeah so this is i i said the video was over like five minutes ago okay anyways so this is the state of the industry on alignment outer alignment and control problem um the short version is i'm pretty salty about it but there's a few people that are starting to move in the right direction so thanks for watching

CLEAR ESSAY:

==========

Over the past few decades, we have seen a proliferation of nuclear weapons around the world. This is due to the fact that the scientific community and the militaries have figured out how to create and refine radioactive isotopes. Now, we are seeing a similar proliferation of artificial intelligence (AI). This is because language models are becoming more open source and efficient. As a result, research needs to be focused on creating a framework that is universally applicable, flexible, adaptable, robust, and implementable.