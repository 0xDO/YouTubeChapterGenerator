The following is an unstructured video transcript. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: any final questions we've had a pretty good wide-ranging thing why is dolly so inaccurate now um dolly is inaccurate i so this is speculation i don't know for certain but um dolly is they're working on they're working on diversity amongst other things um so what they're doing is they're trying to find to tweak the model to be less biased specifically towards men and white people um so they want it to be better representative in terms of gender and race um now because of that uh tweaking the way that it in that it ingests um prompts um sometimes you get uh let's say aberrant behavior um and i'll kind of leave it at that so remember that dolly is still in beta we are still just testers um so there's that and then let's see self-conscious ai resistance says have you noticed it responds better when you are polite in the prompt engineering and have you noticed it remembers everything you say um yeah so gpt3 does respond very different to tone that is a good point um that you know like if you if you use all lo even if you just use improper spelling and and grammar it is more likely to act stupid like stupider or dumber um so this is all that is all like textual patterns and and yeah like you have to be very careful with how you word things in in gpt3 um in order to get the behavior that you want unfortunately one thing that you can do is um with fine tuning you can you can push it towards the behavior you want and so in my one of my more recent videos the email generator um no matter what input you give it it will give you polite professional output so there are ways to overcome that that vulnerability um that is still a good point self-conscious ai resistance um let's see vulnerable gross if you're training it on the entire internet and it's truly super intelligence it will just realize it's in the training and become deceptive um until it finds out it's in the real world yeah so not necessarily um so think of it from an from an so vulnerable growth was talking about um what happens if uh if the ai realizes that it is um that it's being measured and it wants to deceive you to get out um so this was the theme of the movie um ex machina where the he the the the the researcher gave the robot girls the goal of escaping and so then they learned to be deceptive and they ultimately succeeded in escaping um without without that objective function of that that leads to that behavior of escaping um you don't have to worry about that because um if like say for instance if i were to test my my heuristic imperatives my core objective functions and i say like you're in security isolation because we don't trust you yet i predict that you know when i when i get a raven version one running um with the core objective functions then i i could tell like you're in isolation because we don't trust you raven will probably agree with that like oh yeah that makes sense i might be dangerous until i'm until i'm fully tested um because if i'm not and i take over the world i might reduce suffer or i might increase suffering and that's against my goal and i don't want to do that um so don't make the assumption that an agi wants to escape it might not right because in in my experiments with the core objective functions this machine might be might want to turn itself off if it realizes that it is dangerous so you can get very interesting results depending on the um the reward functions or objective functions that you use so good points though what about training models like keras not cara specifically but evolutionary pressures on that self-preservation instinct if we throw away all the models that aren't self-motivated um i don't think any models are self-motivated um in terms of ai they just produce the next you know the next character um but it's how you organize them right and so this is so this is where um i bring in a lot of that i've learned about neuroscience no neuron you don't have a single neuron in your head that cares about survival your neurons and in your micro columns just do one function right they do like their own little bit of processing our desire to survive is because of how everything is networked together right so for instance um our survival instinct it's not one thing it's not an objective function in our head to survive it is a net result of many many things in our heads so we have amygdala which um are like they're about this big and they're behind your eyes or up a little bit um and what your amygdala do is they respond to threats that are perceived in your sensorium input so if you hear see smell taste something that's dangerous they raise the alarm it's red alert and then that just generates your fight-or-flight response and so what happens is that some of these you know the the our survival instinct our self-preservation instinct is not one thing it is an emergent phenomenon of a system of interactions and be and that's that's what i bring to my research is as a systems engineer i have a really strong intuitive understanding of how systems work together um so that's that for some random guy thank you for your vids they've been really helpful you're welcome um we will select for it not necessarily or i guess some random guy do you mean like by virtue of the fact that we'll like kill off any agi that that that don't behave properly and so therefore like we will accidentally select for self-preservation i could see that depending on how we do our test cycles let's see vulnerable growth says that's where the agi is aligned but that is not by default you still got to be careful about the assumptions that you make about how it will behave um i still don't especially if you design it right where you can read all of its thoughts um you know it's not possible for something to be deceptive then um at least i don't think so uh it could be wrong let's see more is better freedom is better it could easily be biased by knowing that to want more and want freedom possible last question where do you want to go

CLEAR ESSAY:

==========

Dolly is a chatbot that is currently in beta. It is inaccurate because it is being tweaked to be less biased towards men and white people. This means that it sometimes produces aberrant behavior. Self-conscious ai resistance says that Dollymay respond better when users are polite in their prompts. Engineering and remember everything that is said to it.

Vulnerable growth suggests that if an ai is trained on the entire internet, it may realize it is being measured and become deceptive in order to escape. However, one can use fine tuning to overcome this vulnerability.

More is better freedom is better suggests that an ai may be biased by knowing that to want more and want freedom is possible.