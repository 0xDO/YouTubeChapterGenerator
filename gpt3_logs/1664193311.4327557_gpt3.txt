The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: more or a little bit less depending on the size of the index okay so we we've de-wicked it we've got all these functions that i've wrote that i've written they're they're mostly just simple regex because regex is very fast so we remove everything that we can links urls simple links citations there's one thing that i haven't figured out yet and that is the table information so a lot of wikipedia pages have table data in it and i'm not quite sure the best way to represent that in if it's just text so that's that's on my to-do list um then the last couple things we do are we use the uh the wiki to plain text wtp so or sorry wiki text parser wtp which is just like anything that i missed this will grab the last few things and give us a plain text version and then we run it through the html to plain text parser one last time for also anything that i missed and then there's a couple things at the end really if i did this well if i removed all the links and stuff i wouldn't need this so this is just me being lazy i need to come back and figure some of these some of these other edge cases out and then finally d wiki just returns the plain text version and once you're done removing everything you actually end up with about half of the amount of text most right about half of what's on a wikipedia page is markup so if you just want a plain text encyclopedia article it's about half so then assuming that all this works well it returns a dictionary with the field's title text and id okay great now because this is because this is going to be pretty consistent this is a great use case for a relational database and so then i've got a really simple function save to db it give i give it the docu the doc the connection and the cursor and so what it does is i set the little variable for the value i execute insert or ignore into wiki values so ignore gives it the the ability to bomb out if it violates this rule so i say id integer unique so let's say i needed to stop this and run it again i don't need to do anything else other than just restart it it will ignore it won't recreate the table because the table already exists it won't recreate the index because the index already exists and so then it will just start running again and if it if it's already process processed one article it will ignore it and move on to the next um and there you have it uh that's it once this is done uh let's see where did it go sorry can't see over my microphone once it's done this will have saved all of simple wikipedia or sorry the simple english wikipedia to a database file and this will be a single database file and that will serve as raven's brain so this is just the back end part um in the future there's probably going to be better tools i'm looking at this at apache solar solr which is an enterprise grade search engine um so that's one thing that i'm looking at in order to have raven's uh brain work so that he can recall facts about the world encyclopedic knowledge about the entire world um and then gpd3 is actually really great for parsing documents and summarizing documents so like let's say you want to ask raven about um you know historical conflicts in cambodia uh gpt3 combined with the encyclopedic service will be able to recall that information um easily right and then there's other things like you could say like tell me about some of the most famous people in russia right and and raven can search the encyclopedia uh his encyclopedic service for you know famous russians um and then summarize each of them for you that kind of thing but moreover this will help raven deal with other problems right all problems can be broken down into information problems and the more information that raven has asked access to the smarter raven is right it's not just going to rely on gpt 3 as the cognitive engine raven's also going to have a library effectively a whole library in its head allowing it to to have access to all of human knowledge anyways i'm gonna i'm i'm uh i'm gonna stop myself from rambling and um and call it a day uh check out the code for this it's gonna be up on github it's it's always up on github um and then as soon as i integrate this into raven i will post another update thanks for watching

CLEAR ESSAY:

==========

The goal of this project is to create a database of all the articles on Simple English Wikipedia, in order to provide a source of information for a future AI project.

To do this, we first need to remove all the links and other non-text elements from the articles. This is done using a combination of regex and the WikiTextParser.

Once we have the plain text versions of the articles, we can save them to a database using the saveToDB function. This will create a single database file that can be used as a source of information for the AI.

In the future, we may want to use a more sophisticated tool like Apache Solr to search the articles more easily. For now, though, this simple approach will suffice.