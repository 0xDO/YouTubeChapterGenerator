The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody David Shapiro here for a video today we are going to stress test quadrant so if you're not familiar with quadrant go check it out quadrant.tech that I also have a demo video that I posted just a couple days ago um so without further Ado we'll get into it so here's what I'm doing um so under my quadrant demo uh repo which uh Link in the comments um basically what I'm doing is I've got a folder with 55 000 um uh uh uh different samples of data it's 55 or yeah 55 megabytes so they range in size from one kilobyte to 33 kilobytes each and so what I'm doing right now is I'm going to split them all up into chunks and then I'm also going to use um I'm going to use Universal sentence encoder to generate embeddings for them um so so here's here's how it's going to go um and I'll just go through the whole thing um one one bit at a time so we'll do that and just print files and then we'll go to quadrant demo python prepare data uh oh yeah okay could not load so basically this error that error that it showed just before it barfed out all the files um come here is basically saying you don't have Cuda because I'm not using I'm not using tensorflow Hub with GPU acceleration with uh with this um with this model Universal sentence encoder large five I found that you kind of don't need it it's a hundred times faster than Universal sentence encoder 4. it's basically like sub millisecond per sample so it's not necessarily needed um okay so we'll do that so for file and file so we're just going to iterate through every single file um we'll do text equals open file um oh yeah we're not going to save this out to logs we're going to save this out to um this is just be we'll call it data um encoding yes sort Keys true indent one that's all fine um okay so open file and the file path is contexts dollar and then we'll do file okay so basically that just sends back the content of one of these so here's an example and it's like oh wow that's a lot of text so then what we're going to do let me close some of these Superfluous things so I'm not distracted okay zoom in a little bit so you can see it better okay so text equals open file context so here's an example context and so then we'll do chunk equals text wrap dot rap um and then we'll do text and then a thousand so we'll basically split this into into the number of um into a thousand character chunks so as an example um let me just let's see CLS just show you how this works it'll do CD context oh whoops contexts plural contexts that's difficult to say at least with my uh particular vocabulary of phonemes contexts okay um all right so we'll do python import uh what do I need uh text wrap and then we'll do um with open um Reddit one one eight six eight dot text uh read encoding equals UTF eight um uh um as in file um let's see text equals infile dot read text okay so we got all the text all right so then we'll say chunks equals uh text wrap dot wrap um text a thousand and so then we'll do length of chunks so there's 31 chunks in this so then we'll do chunks zero and so you see basically each chunk is now oh here I'll just do print all of um we'll do um from P print import and then it'll do pre-print chunks Okay so chunks is now a list of chunks or a list of texts so you see how it's got this little bracket here that means that it is a list um and it's it's a little bit difficult to see but you see that one one instance ends here and then the next one um proceeds so basically we're gonna we're gonna split this up into chunks of a thousand characters so that's what that does so um break break the file contents up in into chunks of a thousand characters okay and so then we will take um actually I need to open Nexus again because I don't remember things off the top of my head I figured out once and then I copy my code because I'm lazy um all right so embed where else is this used yep so embeddings equals all right so then we just okay that's that simple all right so let's see embeddings equals embed and then we'll do um chunks so this should actually be chunks plural I think I think that always uh so let's see text 2 equals bacon um text wrap chunks equals text wrap dot wrap um text two I guess I could have just put in the the thing all right thousand chunks okay so text wrap always returns a list even if it's shorter okay so that's good so we can count on this as always a list um always a list and then we'll get a whole bunch of embeddings so irrespective of how long the file is it'll give us a list of of uh text of strings and that's exactly how you use the embedding service all right so then we have a list of text and a list of embeddings what do we do with it then um I'm gonna have to look up how you do the zip thing where you you put things together um because actually no hang on let me pause this for a second because I need to gather my thoughts I didn't think this far ahead okay I didn't want you to have to watch me fussing around with this but I think I've got a simple thing so um we're gonna get a list of embeddings which are numpy objects I believe or it might actually be something else but so we take the those embeddings convert them to numpy convert them to lists so now we've got a list of vectors that is matched to the chunks and so then for I and list of range 0 to the length of the chunks um so we'll just iterate through so rather than zip them together I'll just do it in one step and then we'll save the data by passing a string and an embedding back and then I'll just do one file so let's see print file um yeah and we'll go from there uh just for some debug output so we'll do print um chunks I uh vectors I and let's see what happens it'll probably blow up because it usually does okay uh CD go up a level CLS python prepare data of course um oh right invalid syntax that's supposed to be a comma model does not exist yep if you've watched my videos you've seen that this happens more often than not um really got to figure out a way to fix

CLEAR ESSAY:

==========

I'm stress testing quadrant today. For those of you who are not familiar with quadrant, it's a tool that you can use to split up large files into smaller chunks. I'm using it to split up a folder of 55,000 different samples of data, each ranging in size from one kilobyte to 33 kilobytes.

I'm also using Universal sentence encoder to generate embeddings for each of the chunks. Universal sentence encoder is a hundred times faster than the previous version, so it's not necessarily needed, but I'm using it for this demo.

Basically, what I'm doing is iterating through each file, splitting it into chunks of 1,000 characters, and then generating embeddings for each chunk. I'm then saving the data by passing a string and an embedding back to quadrant.

For some debug output, I'm printing the file, the chunks, and the vectors. So far, everything is working as expected.