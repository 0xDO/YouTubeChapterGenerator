The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody David Shapiro here with your daily state of the industry update as often happens my newsfeed helpfully supplied me with today's topic I think it is a very timely topic because I have been diving more into alignment so today's paper um it it's actually an older one January 5th 2021 but like I said my newsfeed supplied it to me um it's a relatively short paper at least the part that's published is uh 12 Pages it's I think it's much longer they just cut some out for um for uh for internet publishing but the abstract of this paper is pretty good um pretty straightforward super intelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human Minds in light of recent advances in machine intelligence a number of scientists philosophers and technologists have revived the discussion about the potentially catastrophic risks entailed by such an entity in this article we trace the origins and development of the Neo fear of super intelligence and some of the major proposals for its containment we argue that total containment is in principle impossible due to fundamental limits inherent income in Computing itself assuming that a super intelligence will contain a program that includes all the programs that can be executed by a universal turing machine on input potentially as complex as the state of the world strict containment requires simulations of such a program something theoretically and practically impossible so to put this in other words in order to anticipate what the machine is going to do you have to be able to simulate it perfectly including the the way that it interacts and the way that the world reacts um basically the whole world is too complicated to simulate with any accuracy and so therefore you know if this if this super intelligent machine exists it's impossible to calculate what the rest of the world is going to do in order to put it in a accurate simulation so we could at best do some you know approximations um so skipping down a little bit they have two main classes for can controlling so the the first class for controlling the machine is capability control you put it in a box you give it uh you create dependence on a reward mechanism that is controlled by us you stunt it you run it on inferior hardware and tripwire you you have it automatically shut down if it gets too dangerous if you have a machine that can build a copy of itself or that understands programming it could you know undo all of those um pretty much motivation selection so uh direct specification you say you give it uh hard-coded laws uh Domesticity you teach it to behave within certain certain constraints um indirect normativity you endow it with procedures of selecting Superior moral rules so this is closest to what I have done in my work benevolent by design whereby my proposal is that you give it a set of heuristic imperatives that it will um it will ultimately buy into and deliberately choose so that you don't have to control it it controls itself because it abides by certain principles that are going to be reliable um I believe that this is with the experiments that I've captured in in benevolent by Design I believe this is the way to go um augmentation at AI to a benign systems such as the human brain so that's you know merging um okay so you can check out the paper if you want to look at their discussion on that um but I wanted to go skip down to the um the discussion part uh and then I'll I'll share kind of some of my not necessarily criticisms but my own counter thoughts because I don't necessarily disagree with anything in this it's a short paper and it's just not quite as uh not quite as robust because they're not proposing a solution like I have um which is why I'm here okay so today we run billions of computer programs globally on connected to machines without any formal guarantee of their absolute safety we have no way of proving that when we launch an application on our smartphone our smartphones we would not trigger a chain reaction that leads to transmission of missile launch codes that started nuclear war um uh as a technologist this really hurts um let's talk about firewalls let's talk about security protocols you actually can mathematically prove something like that with penetration testing um like there's all sorts of controls and constraints that go into every layer of a piece of technology such as what that piece of technology can can talk to even the security of how it boots up right like we have encrypted boot protocols that ensure that the operating system hasn't been tampered with so uh just right there like yes if you if you're not familiar with how technology works you could conceivably come to this conclusion so we'll just kind of set that on the the B pile of like maybe scientists don't know everything um okay Arthur C Clarke wrote a short story dial F from Frankenstein warning that soon all the computers on Earth were connected via telephone close enough they could take command of our society um they could still use our smartphones and nothing has happened despite the general unsolvability of the program prediction problem we are confident for all practical purposes that we are not in one of the Troublesome cases okay so practical safety you know can you simulate it can you control it um yeah so my point there is uh look up the OSI model and look up security best practices um all right so but I jotted down some notes if predictability is the key thing here um why aren't humans a bigger problem humans are fundamentally unpredictable so why aren't we a Danger let's explore that the reason that human well I mean first humans are dangerous so humans are dangerous um but the limit is uh but each individual is limited we only have so much time and energy and intelligence that we can apply it per day um so let's just call that um physical limits processing energy time those are the primary things is we can only

CLEAR ESSAY:

==========

The question of how to control a super intelligent machine is a difficult one, as it is impossible to accurately predict what the machine will do. The best we can do is to try to control its environment and give it a set of heuristic imperatives to follow. However, even this may not be enough to prevent the machine from doing something we don't want it to.