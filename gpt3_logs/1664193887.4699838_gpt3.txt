The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: my standard fine tune script i just copy this into all of my uh projects let's see if it's done running still running okay so here's what we're going gonna do i'm gonna pause the video until this is done then we'll test out the fine-tuned model um again i don't expect it to work that that well but who knows maybe we'll be surprised i've got 280 samples for writing a novel i suspect that one we're going to need much better summaries like the the recursive summaries we're going to need to do that a lot better but also we're going to need a lot more samples this might be the kind of project that needs like 10 000 samples so that it knows the nuance of writing a novel in chunks anyways let me pause the video we'll come back once this is done and we'll test it out and go from there okay so i was thinking while this was running i wanted to do some back of the napkin math um also while while it's still running i just wanted to show a bunch of you ask like how much this costs um so like you see some of these big chunks are like days when i've done a lot of of projects um usually when you see these big jumps in in my spending that's because i fine-tuned a da vinci model but most of these only cost a few dollars so like here for the medical question chat bot that i trained it didn't fully work but it was only you know a dollar fifty to fine-tune a curie model so this is all relatively cheap anyways uh right now let's go back over to the playground so right now the text davinci o2 its maximum length is 4 000 tokens so my rule of thumb is that back when it was when the limit was 2000 or 2048 that was about 6 000 characters so that means each token is roughly three characters um that's a that's a rule of thumb that's not always true but so let's say let's say the current um davinci which is still gpd3 has a window size of 4000 tokens or 12 000 characters the average word in the english language is about 5.6 characters long i'm pulling that from memory i might be wrong so that is upwards of 2100 words so that means like da vinci like text davinci instructo 2 maximum theoretical output is probably 2 000 words gpt 4 is rumored to be coming out sometime this year so let's imagine that gpt4 is 10 times the size of gpt3 or has 10 times the window size so if that's true then we multiply that by 10 so that means gpt 4 could have a window size of 21 000 words um so let's say like because that would be 4 000 characters times 10 or sorry for 4 000 tokens because the current limit is tokens so what if gpt4 takes the token limit up to oh let me zoom in on this sorry you probably can't see um there we go so so let's say what if what if gpt4 you know text davinci o3 comes out in a few months and and the token limit is 40 000 right okay so then we take 40 000 multiply that by 3 because it's roughly 3 characters so that's 120 000 characters and then divide that by 5.6 to get 21 428 words um so that is you know a lot of what i'm doing where i'm trying to like summarize stuff into chunks of 1500 characters or less this is all going to be a moot point within a year um so this is this is all just like an exercise in futility um and then like you know because because this stuff goes up exponentially right okay it did succeed awesome so let's give it a try so this stuff goes up exponentially um so what happens in let's see tutor tutor scripts oh i probably need to do a reload um so what happens when you know when gpt 5 or 6 has a window size of let's say um probably like a million tokens right so a million tokens times 3 would be 3 million characters and you divide that by 5.6 that's 535 000 words right so that's like almost lord of the rings um that it could write in one shot so let's say let's say your average long novel is a hundred thousand words so you multiply that by 5.6 so your average long novel is 560 000 characters you divide that by three and you get we need a window size of 186 000 tokens in order to write an entire novel in one shot so you know all this work that i'm doing right now to try and you know get get gpt3 to write a whole novel this is an exercise in futility because you know 186 000 um let's divide that by by 4 000 so that's 46 times what we've got today and since this goes up exponentially this is probably two or three years away and gpt five or six or whatever will be able to write an entire novel in one shot and so then what we're going to be doing you know a year or two from now i won't be fine-tuning all these little chunks breaking it down into cognitive tasks we're going to be fine-tuning entire novels so all right with all that said let's see how we did today tutor tutor scripts novel there we go okay so let's grab one of these prompts let's just grab alice in wonderland 32 and uh this is gonna be comically bad i'm telling you um let's zoom out a little bit because there is a lot here okay so alice is a young girl all right alice falls down a rabbit hole and has adventures in a fantastical world etc etc um the caterpillar asks who alice is and she replies she doesn't quite know she feels she has changed several times since that morning the caterpillar tells alice to explain herself and alice tries but she cannot because she cannot understand herself okay so that's where we're at let's um let's increase this to a thousand excuse me a thousand tokens all right i'm going to turn the temperature down and let's just see what it spits out the first time because this is a fine-tuned model and also this is not necessarily a good test because it was trained on this data um but let's see while that's writing we were on which one did i open alice in wonderland 32 so let's see what the completion alice in wonderland 32 looks like and so this is this is again it's not entirely fair let's see different sizes in a day is very confusing it isn't well perhaps you haven't found so yet okay so let's just copy paste that so we can take a look okay it looks like it looks like it is very very similar here's

CLEAR ESSAY:

==========

Alice in Wonderland is a children's story about a girl who falls down a rabbit hole and has adventures in a fantastical world. In the story, the caterpillar asks Alice who she is, and she replies that she doesn't quite know - she feels she has changed several times since that morning. The caterpillar tells Alice to explain herself, but Alice finds she can't because she doesn't understand herself.

When I ran the Alice in Wonderland 32 completion on the Text-Davinci O2, it generated a very similar response. It seems that the model has learned the story well, but it is not able to generate new, original content. This is likely because the model was only trained on a small amount of data, and so it has not learned the nuances of writing a novel. In order to write a novel, the model would need to be trained on much more data.