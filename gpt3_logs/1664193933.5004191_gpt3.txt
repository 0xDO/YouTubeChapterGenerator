The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: called the working set so this is another part of neuroscience when you are working on a task um it's so it's a task set really it's what it's called so if you've ever heard about the term context switching if you switch from one task to another the task set which is all the memories and facts and knowledge that you've accumulated in your brain has to get shuffled off um think of it like like an old-fashioned office where everything is on paper so like let's say you know you're the the the imaginary people in your head they go fetch all the project documents for project a and then it's like the boss comes in and says wait we gotta work on project b so they gotta they gotta take all that all that task set all those papers file them away and then go get project b so what we're doing here is we're basically recreating the equivalent of human memory task set what's called a task set so that's all relevant information knowledge etc has to be in every prompt for gpt 3. in order to have in order to handle larger tasks now i didn't invent this this is how the human brain works i'm just approximating human neuroscience in gpt3 okay so that's why i'm doing this i wanted to sprinkle that in as i go let's see what do i have to do next i got to finish this that's fine i just save that out to file and then we've got to do the summary so the hardest part of this of this data because like you know i just downloaded the books and that's going to be the huge chunk of our data we've got the outlines again that's a huge chunk of the data the hardest part is this because i'm going to have to use gpthree to summarize where we are every step of the way i'm not going to do the whole books i'm only going to do like the first you know few chapters um because one i'm not going to spend like a thousand dollars fine tuning this for five full books um it probably wouldn't cost that much um this should just be a good proof of concept um but yeah so and basically just creating a live a running summary of the story that's gonna be hard it's gonna be expensive okay pause the video again we'll come back when i'm ready to show you more standby all right quick check in as promised um i finished the script there's got to be a better way to do this i don't know how to pad zeros but basically the purpose of this um little bit is to generate um sequential files so you see each of these stories is broken down into sequential files uh so they're just straight up numbered it's in the chunks folder so now each chunk is just a bit of the story also it did this weird thing where there's no vertical white space i think that's okay i'm not going to worry about it this is not going into production i'm just seeing if it even remotely works um yeah uh oh sorry my phone's going off anyways i will uh be back shortly all right i was about to give up on this and take a break but i figured it out well i don't know if i figured out the root cause but i was getting this error let me show you where i kept complaining about um like could not encode where is it i'm just probably go all the way up yeah so you see here error and communicating with open ai charmap character kodak can't encode character blah blah blah all right so there's something funky with these files from gutenberg so the first thing i tried um was to go back whoops we can close this one um sorry this might make a little bit of noise okay so my first right the first thing that i did was i tried to go and change the encoding of the books so in coding it was utf-8 bomb so i just converted it to utf-8 that didn't fix it um so i was like okay um gpt3 should be able to accept all utf-8 but there's some artifact in this that it didn't like so here's what i did was i said okay let's open the prompt and we'll get we'll encode it to ascii which is much simpler than utf-8 and then we'll decode it back to a normal string so it's basically saying okay whatever whatever this is ignore the errors simplify this codec make it ascii standard and then decode it and then gpt3 likes it now so it's uh recording this basically what i'm doing um while this is running so we've got all the chunks uh here and then i've got the summaries so i'm making a summary of the first um you know few uh few pair uh passages of each story um so alice gets bored sitting by her by her sister and sees a white rabbit with a pocket watch she chases it and falls down a rabbit hole so basically these summaries can be used to stack up and fill in this part here let me just save this file we'll save this in the um here actually i'll put this in to the readme do the readme all right general idea and we'll save that um so this is what it just so that it's there it's saved and then i can close this okay so the outlines we've got the outlines i will show you that again where are we in the story that's what their summaries are going to be used for excuse me so each chunk is going to be summarized and this one you know it's down to two sentences so alice in wonderland o2 if we open the summary we can um compare oh that's that is the summary sorry i need the chunk so chunks um alice in wonderland o2 so you see it goes from 1500 characters down here down to 127. so by a fact it's compressed by a factor of 10 and then you could also probably like recursively summarize once it gets too long i'm not going to worry about that this time because this is just a huge experiment in generating something as far as i know no one has ever succeeded in generating a whole actual novel with this kind of structure so this should be done by now excellent all right so if we go into the summaries we've got 11 chunks for each so that's 55 i'll probably need more let's see hmm let me think about this because this is 55 55 summaries which will then be used because if i have five stories i would need um why can't i do math 5 times 40 i would need 40 for each one in order to get to 200 samples okay so i guess i will need to do a little bit more darn okay i will

CLEAR ESSAY:

==========

The "working set" is a term used in neuroscience to describe the memories, facts, and knowledge that are relevant to a particular task. When you switch from one task to another, the working set has to be shuffled off and replaced with the new task set. This process is called "context switching."

In a similar way, the artificial intelligence system known as GPT-3 has to have all the relevant information for a task in order to handle it. This is why the system is designed to break tasks down into smaller chunks, or "sequences." Each sequence is then summarized, so that the system can keep track of the overall task.

This process is not without its challenges. One of the biggest challenges is generating summaries that are accurate and concise. Another challenge is ensuring that all the relevant information is included in each sequence.

Despite these challenges, the system is designed to handle larger tasks. This is because the human brain works in a similar way,Breaking tasks down into smaller chunks and summarizing them.

The system is not perfect, but it is a good proof of concept.