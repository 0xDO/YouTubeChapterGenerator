The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: you are live okay hello um customization [Music] let's see making sure that I did the audio correct if there's anybody on the stream how's the volume foreign does that mean that the volume is good Jordan I think it's on slow mode copy link I'll put a link in here announcements I don't think I can do announcements anymore now oh we got three folks okay um all right Jordan just gave me a uh an emoji but I don't know if that means that the sound is good so if the sound is good someone let me know because the little the little indicator is is not a it's only going up to like one dot sound is good okay cool all right well I guess let's go ahead and get started so today's podcast I think is going to focus on centralization or decentralization rather the opposite okay audio is good um yeah so hopefully we'll get more people streaming in here as we go um yeah so well first uh shortly after my last video today open AI posted their whisper um uh what was it law the uh the what should we call it the um ASR automated speech recognition so open AI let me bring it up open AI whisper introducing whisper so seven hours ago um they did uh We've trained and are open sourcing and neural net called whisper that approaches human level robustness and accuracy on English speech recognition um so yeah basically what this does is it creates you take audio and you create a spectrogram which is an image um of of the of the the registers of all the different uh frequencies and then you decode that into language um this this so ASR has been around for a while um and it's just getting increasingly more sophisticated so for instance YouTube which we're on right now has their own ASR engine in the background and that's how it does automatic captioning um yeah so it's pretty interesting I saw someone on Twitter say that they already used it so um I don't know the code is on is up on GitHub so I think I think you can you can just do it yeah um we used python 3.9.9 Pi torch ffmpeg python hugging face Transformers yeah so if you look at it on um on GitHub it looks like let's see import whisper model equals whisper load base yeah they've got it pretty pretty straightforward ready to go um I'll post a link in the chat and if I remember I sent a collab in the Discord Okay cool so Jordan I didn't see uh but Jordan had already posted a collab notebook for whisper in in our Discord um yeah so it's super straightforward um looks like it's pretty easy to use which is nice um I don't know how much memory it takes um typically eight so for a while ASR was it was uh resource intensive enough that you'd have to run it on a server or you know in the cloud or something but um certainly with uh with recent advances and also just how big gpus are um ASR is probably something that you can run locally um let me see if I can find a uh how many gigabytes you need um required vram okay here it is so the tiny version is 300 or 39 million parameters and it takes one gigabyte um relative speed 32x so it's pretty fast there are five model sizes with four English only versions offering speed and accuracy trade-offs okay so the smaller the faster obviously or maybe not obviously um but so you can do faster than real-time inference so the fact that the smallest one takes about one gigabyte that is getting to the point where you can run it locally and so this is interesting because we just um in my video earlier today I I posted about the Orem um which is the Nvidia like embedded or mobile sorry my ear just started ringing you ever have that happen where like you're in the middle of talking and like your ear just starts ringing and it's like yeah or is that just me I'm weird anyways um it's going away now so that we're in um has what is it I think it's got a a four and an eight gigabyte GPU option so that means that some of these some of these smaller versions of open AI whisper um up to so there's the tiny there's bass there and they're small so I think you can run each of those on a mobile device now so this is different because um up until now things like um Alexa and you know on your phone or whatever um any any voice recognition um almost all of it had to be sent up to the cloud and so it's kind of streaming up to the cloud and the inference is running on Hardware on somebody else's Hardware but with advancements like this that allows you to run it locally so this is going to be really important as we move towards an AI saturated world and we'll talk a little bit more about what I mean by that soon I I've learned a lot today um and there's ideas that are kicking around that I thought were like decades out but they're actually like years away not decades um but yeah so as as Transformers and and neural networks as we find new efficiencies and the hardware gets better so on the one hand like we're getting new capabilities and for a while people are like oh scale is all you need scale is all you need but we're finding that that's not true we're actually finding that more efficient algorithms are all you need um so so between more efficient algorithms and better distillation of neural networks and then better Hardware so you're going to kind of like meet in the middle right like you've got one thing coming up and the other thing going down and so they're going to meet pretty soon this happens with Technologies all the time the same thing happened with solar energy over the last few years so like the cost of solar was plummeting and then the efficiency of solar was going up and so then finally they reach parity and solar is commercially viable and so what I suspect is that we're going to start hitting that point with a lot of AI Technologies um you know because there's all there's all kinds of people on Twitter that say AI hasn't done anything and it's like the fact that you're using Twitter and that you see tweets that are relevant to you that is AI there's actually a really great I think it was on medium it was talking about how NLP is is it's so um

CLEAR ESSAY:

==========

OpenAI recently released Whisper, a neural net that can approach human level robustness and accuracy for English speech recognition. This is significant because it marks a major advancement in ASR technology, which has been around for awhile but is only now becoming increasingly sophisticated.

Whisper is based on the idea of creating a spectrogram (an image of the frequencies of sound) and then decoding it into language. This process is usually resource intensive, but with recent advances in GPU technology, it is becoming increasingly possible to run ASR locally on a personal computer or mobile device.

This is significant because it opens up new possibilities for AI-saturated world. With more efficient algorithms and better hardware, we are quickly reaching a point where AI can be used for a variety of tasks, including voice recognition, that were previously only possible with the help of the cloud.