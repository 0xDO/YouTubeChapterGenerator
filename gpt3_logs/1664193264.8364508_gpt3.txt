The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: hey everybody david shapiro here with a video um i know that i did say that i'm taking a break um from research but apparently i'm getting back into the groove of making youtube videos so there was a discussion on the open ai forum um this guy he saw my video about reducing confabulation and he had some questions um and so first i just wanted to say that like if i have not been as helpful of as an educator as some of you would have liked that's my fault because i was primarily using youtube as a mode of dissemination um but uh and this probably seems obvious to other people um basically what i'm doing is i'm trying to teach people what i have learned and what i have discovered but to me it was like i didn't connect the dots right and i don't even know why because i i do uh training and teaching and mentoring and professionally in my day job so anyways uh i just wanted to say that like i am learning to put on my professor hat you can see like i'm dressing the part better and um i've moved into my own office um in my house i'm getting it all decked out so uh yeah so practicing putting on my professor hat um i don't do teaching full-time uh so i'm getting used to it anyways so he asked this question very salient question can gpt generate its own fine-tuning training data a lot of my videos i do what's called synthetic data uh and so basically what what synthetic data is and i've got a few articles up here i'll put them all in the comments just can gpt3 create synthetic training data for machine learning models the short answer is yes many of my videos i demonstrate how to do this and then here's a forbes article talking about synthetic data is about to revolutionize artificial intelligence here's an nvidia article talking about the same thing that um synthetic data with transformers is gonna you know is good for enterprise um and this was may 9th so this was a month and a half ago and then finally um towards data science article um just over a year uh almost a year and a half old about synthetic data um yeah so i just wanted to say that if you if you have seen all the videos that i have shown where i used gpt3 to generate its own training data for fine tuning models um that is a legitimate practice it does have its own pitfalls and uh and strengths though um and so going through this um there was there so the the first thing is like okay why what's the benefit if you're just using one model to generate output and then fine-tune the model what are you actually getting out of that um and so i wanted to um kind of go over kind of the the the top level reasons even if you don't use much different data um there's a few there's a few benefits so first of all when you generate fine-tuned um or fine-tuning data or synthetic data with something like gpd3 you can filter out the ones that you don't like the samples that you don't like so that you can get a more consistent model and we'll get to that in a minute like what do i mean by consistency um that's the question of precision versus accuracy but then there's also the problem of confabulation or hallucination in these models and we'll get to that as well so then another advantage is that you can incorporate multiple prompts so for instance rather than just have one prompt you can have data that is several different prompts either stacked right where you have meta prompts or prompt chaining to generate the output um or you can have you can go more more lateral scaling which by that i mean you can have multiple different types of problems in your fine tuning data i've demonstrated that in my core objective functions fine tuning videos where i actually train one model to do three different tasks so you can either do different tasks like go sideways where it's like task one task two task three and you have one model that can do all of them and also this chair is squeaking let me let me swatch swap chairs real quick sorry okay sorry about that i got a ten dollar chair used chair just to see like okay if it works it'll be good but it was squeaking so it's no good um ten dollars wasted not a big deal okay sorry so um so yeah when you when you do synthetic data with gpd3 you can incorporate multiple prompts and and that means that you can either do prompt stacking where you generate data that requires multiple prompts to generate the the input and output or you can do multiple tasks or both um in my core objective functions let me actually show you core objective functions in experiment four i actually had to use um maybe it wasn't this one oh yeah so in this one i actually had to use i i did both so this one is trained to do three different functions three different tasks which is reduce suffering increase prosperity and increase understanding but to increase understanding to get that synthetic data it actually took three prompts most of it was just the first two prompts but the point is is that i i basically trained one model to do five different tasks all all in one so that's another advantage of doing um of using uh fine-tuning uh gpt3 with its own synthetic data is you can kind of compress different problems into one model and in their documentation openai said that that doing this actually tends to increase the performance of all so you get some transference effect where um the the bigger your fine-tuning data set is the better performance that you get overall um let's see and then finally you can incorporate um lots of different data so if you don't if you do 100 synthetic data um which this is not so in this one i had um i had some real world data from like reddit um which is so that's not synthetic at all but in my chat bots um that i did those are um those some of those are 100 synthetic um let's see let me swear did they get one um actually the movie script generator because in this one the premises were all generated by gpt3 as well right so this is 100 synthetic data where the input that i generated was synthetic and then the output was also

CLEAR ESSAY:

==========

Hey everybody,

David Shapiro here with a video. I know that I did say that I'm taking a break from research, but apparently I'm getting back into the groove of making YouTube videos.

There was a discussion on the Open AI forum and this guy saw my video about reducing confabulation and he had some questions.

First of all, I just wanted to say that if I have not been as helpful of an educator as some of you would have liked, that's my fault because I was primarily using YouTube as a mode of dissemination. But I'm learning to put on my professor hat and I'm getting it all decked out in my own office in my house.

So anyways, going through this discussion, the first thing is like, okay, why what's the benefit if you're just using one model to generate output and then fine-tune the model? What are you actually getting out of that?

And so I wanted to kind of go over kind of the the the top level reasons even if you don't use much different data. There's a few benefits.

So first of all, when you generate fine-tuning data or synthetic data with something like GPT3, you can filter out the ones that you don't like. So that you can get a more consistent model.

And then there's also the problem of confabulation or hallucination in these models. So then another advantage is that you can incorporate multiple prompts. So for instance, rather than just have one prompt, you can have data that is several different prompts either stacked or you can have prompt chaining to generate the output.

Or you can have more lateral scaling which by that I mean you can have multiple different types of problems in your fine tuning data. I've demonstrated that in my core objective functions fine tuning videos where I actually train one model to do three different tasks.

So you can either do different tasks like go sideways where it's like task one task two task three and you have one model that can do all of them. Or you can do multiple tasks or both in my core objective functions.

In experiment four, I actually had to use maybe it wasn't this one. Oh yeah, so in this one I actually had to use I I did both. So this one is trained to do three different functions three different tasks which is reduce suffering, increase prosperity, and increase understanding.

But to increase understanding, to get that synthetic data, it actually took three prompts. Most of it was just the first two prompts, but the point is that I basically trained one model to do five different tasks all in one.

So that's another advantage of using GPT3 with its own synthetic data is you can kind of compress different problems into one model. And in their documentation, OpenAI said that doing this actually tends to increase the performance of all. So you get some transference effect where the the bigger your fine-tuning data set is, the better performance that you get overall.

And then finally, you can incorporate lots of different data. So if you don't if you do 100 synthetic data, which this is not so in this one, I had I had some real world data from like Reddit, which is so that's not synthetic at all.

But in my chat bots, that I did, those are some of those are 100 synthetic. Let's see. I actually the movie script generator because in this one, the premises were all generated by GPT3 as well, right? So this is 100 synthetic data where the input that I generated was synthetic and then the output was also synthetic.