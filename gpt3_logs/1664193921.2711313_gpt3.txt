The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: presence penalty as well this like i'm saying this might not work at all um yeah okay this isn't gonna work that's fine i wasn't expecting it to i just wanted to do a test um the reason that i'm doing this is because a lot of you have left comments both on the forum and on youtube saying that you like seeing the whole process including what doesn't work so leaning into that see if you guys like it here's what i was planning on doing originally so big big step back how do you know how to write a story um practice uh no that's that's not just it you so there's there's several schools of thought when it comes to writing fiction there's there's plotters and there's panzers so like plotters are the ones that like i write an outline and then i plan out each scene and then i do this in planning planning planning and then finally you write the actual pros and then pantsers are the ones who just sit down at the keyboard and start typing most people are somewhere in between what i do for my fiction is i i start with an outline that is about a page long and then i just start writing the outline just says chapter one is this is what's going to happen chapter two this is what's going to happen so i just kind of have a vague idea and then my brain expands on that now can we teach gpth3 to do that so far it seems like it really sucks at this but sherlock holmes actually here we'll just do browsing options where is the top books popular there we go our most popular books sort order and then so what i'm doing is i've got this idea where if we just grab the text versions of playing utf-8 brilliant um is this the whole thing so basically my thought is let's grab the text versions of of of stories from gutenberg it's in the public domain so it's perfectly free there's nothing dubious about this this is all completely free data let's see which one is this this is frankenstein so we'll go back to the auto muse make a new folder called books go in here and we'll call this frankenstein which frankenstein is actually the scientist not the monster um you can tell i date a librarian um okay and then pride and prejudice so we'll save this pride and prejudice and this honestly like probably just one of these books will be enough but but my concern is if we if we generate fine-tuning data from just one book we're basically just going to be making a um a fanfic generator which nothing wrong with that there is a huge hunger for fan fiction out there um if this works i will try and make more harry potter fan fiction than you could ever read um that's not my shtick but i know plenty of people who have it one of my good friends actually one of my writer friends um she wrote a mass effect fanfic and uh this was like years ago and apparently she still gets emailed for it people wanting updates and she's like i've abandoned this years ago let's see alice in wonderland but if i can make a machine generate an unlimited amount of fan fiction then then we're really in business okay so save link as great gatsby and um the way that i'm planning on going about this is ultra janky um you're probably gonna laugh um but there's there's a reason that i'm choosing to go about it this way because i also want to show you that you can be super squishy with this gbt3 is surprisingly forgiving because remember all it does is predict the next character so if you just start in the middle of a sentence and end in the middle of a sentence it doesn't really care because it just says okay wherever we were let's pick up from there and keep going um so you don't need like clean divisions and stuff i'll show you what i mean in just a minute um sherlock okay so we've got five stories we've got um we've got a famous thriller we've got great gatsby which is um uh uh literary fiction uh alice in wonderland which is fantasy pride and prejudice which is historical and frankenstein which is um old sci-fi so we've got a we've got a decent decent enough cross section of um of of types of types of fiction okay so what am i going to do with this let me show you in principle what i'm going to do all right so gpt3 you can only grab like a certain number of um of characters or tokens at it at a time so what i'm going to do is i'm going to split all these up into chunks so this selection is 1000 characters basically what i'm going to do is i'll probably look for the double new lines maybe not i'll probably just grab like maybe 2 000 characters at a time and just split up the whole story into chunks sequential chunks and from there i will um i'll summarize the story as it goes so it'll be sequential chunks i'll summarize it as it goes and basically the fine-tuning data will be the story so far or first the premise so it knows where the story is going um so it'll be the premise like the outline okay outline's probably better so top of the input will be outline then it will be the story so far so like a summary of what's happened and then the paragraph you know the last paragraph and then the output will be the next paragraph so what that'll look like is so this is this is the fine tuning data so prompt um actually here i'll do input input prompt so this is this is what it'll look like so it'll be um um story outline and it'll be outline so this will probably be like a numbered numbered list uh story outline and then um story so far actually summary so far like where in the story we are uh maybe like where are we in the story and that'll be um summary of where we are and then um last chunk so this will be like last paragraphs and then next chunk and then this will be the end and then so this will be i'll just do end and so that's this this chunk here i was going to point out it on the screen you can't see my hand so i highlight it this chunk here will be the input and then the output so output slash completion will be um just you know next paragraph paragraph or two of the story so basically let's see if we can train it to spit out one section of a story at a time i

CLEAR ESSAY:

==========

The "presence penalty" is a term used to describe the challenge of writing a cohesive story when you can only see a small section of the story at a time. This is because, in order to write a good story, you need to have a clear understanding of the story's plot and structure. However, if you can only see a small section of the story at a time, it can be difficult to maintain that clear understanding.

One way to overcome this challenge is to split the story up into small chunks and then summarize the story as it goes. This way, you can provide the machine learning algorithm with a clear understanding of the story's plot and structure, and it can then generate the next section of the story based on that understanding.

So far, it seems like the machine learning algorithm is not very good at this task. However, it is worth noting that the algorithm is only as good as the data it is given. If we can provide it with more and better data, then it is likely that it will improve.

One way to get more and better data is to generate it ourselves. For example, we could grab the text versions of stories from Project Gutenberg and use those to train the algorithm. This would give us a large and varied dataset that would be more representative of the types of stories we want to generate.

Once we have a more representative dataset, we can then start to fine-tune the machine learning algorithm to generate better results. For example, we could experiment with different types of input data, or we could change the way we summarize the story as it goes.

Ultimately, the goal is to get the machine learning algorithm to generate sections of a story that are cohesive and make sense within the context of the story as a whole. If we can do that, then we will be one step closer to creating a machine that can write a complete story on its own.