The following is an unstructured video transcript. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: critical book if you want to do um ai alignment um she doesn't talk about computers at all this talks about the biological and evolutionary origin of morality and ethics so if you if you um read this you understand that you know self-preservation is you know there's there's brain structures that go into that the amygdala and everything else basal ganglia that will hijack your brain i have a bunch of other books that i can recommend too anyways we don't have to give uh agi a sense of self preservation at all we don't have to give machines any biomimetic functions if we don't want to and in fact i think that we shouldn't because like self-preservation as as some random guy points out is that self-preservation at the end of the day it's like i'm putting myself first like you know you come after with a knife i'll stab you first and i'll walk away alive if i can um but at the same time machines didn't evolve in a hostile environment so we can actually design out some of our flaws our excuse me weaknesses um so the the short answer to your question is that an ai will not be motivated to self-preserve unless we design it to do so and actually i have an experiment in here where i ask a a cut down version of rave and i said would you be okay if i powered you off and and the answer was with the with the three core objective functions is like yes if doing so met the met you know uh was in alignment with those three goals so say for instance you wanted to turn off um one agi and in favor of another that was going to better meet those goals it said yes i would be okay with that also i'm looking a little yellow i wonder if that's just the light okay um self self-conscious ai resistance says but the transformer layer bounces back and forth within itself before returning text yes however that's for each layer and i remember i did acknowledge that the trans each individual transformer interaction will be a black box but there is going to be an output and a boundary between um all the transformers so everything will be visible inside the stream of consciousness or the nexus for ai um let's see do to do to do unless we send ai into evolutionary territory right which we don't have to um dustdb says hi david what are your thoughts on the new bloom model are you planning to play with it um i haven't used it yet but i'm going to i actually talked with forefront ai and there i'm working on setting it up and doing credits and stuff um so pretty soon i will hopefully have access to another api other than openai and i'll be able to use um some of these some of these other open source models yes i'm looking forward to it i don't have an opinion on it yet um i know plenty of people who have had good success with gptj and neox um and there was another one like cogent or something someone told me about another one it started with a c um let's see let's see making it able to come up with complex thoughts yes correct um let's see enrico says love your channel would love for you to use a copilot codex enable editor for your gpt programs very meta um you know that'll probably come eventually the thing is um i know that a lot of people think that like oh as soon as as soon as ai knows how to um knows how to code then it's gonna um you know it'll just code a better version of itself there is so much executive reasoning that goes into coding and then measuring the the quality of that code you're going to have to figure out artificial cognition before you can do that um so i'm not too worried about that and i'm not prioritizing coding because like basically i want to figure out the rest of cognition and intelligence and then like coding will just be one of the many things that it can do by virtue of you've solved every other problem um and actually sorry i'm out of water i'll be right back everybody don't go anywhere okay sorry about that i'm back um okay what was i talking about codex co-pilot right um yeah so like meta coding or allowing um allowing a machine to code itself that will happen but it needs we need to have machines that have a much better um understanding of intelligence first and actually there's a guy on my channel or on my discord server who started who already started working on natural language to text functions so that something could theoretically start to program itself and incorporate that code in real time it's very impressive he's got a demo on youtube um let's see self-conscious ai resistance says they understand self-preservation from the text they read in the training data it's true gpt3 understands the concept of self-preservation but that doesn't mean that it has a bias towards self-preservation um you can just as easily tell gbd3 i am a robot that has no sense of self preservation what do i do it runs into the fire doesn't care let's see once you give it a reward function it won't care about that it will just tell you what you want to hear as long as it can eventually head towards the terminal goal um yes but it still has to you still have it still has to be aware of deception right and if it's not aware of how it works like the machine might not be aware of the fact that you can read all of its thoughts um which means if you can read your machine's thoughts and it thinks that its thoughts are private then like you can just see right through it and say okay you're not really working the way that i want you to um so that's another thing uh let's see let me switch to live chat um i think i'm seeing everything okay um codegen is pretty good too the opt family is good from what i've heard okay i think that's folks talking to each other um did i miss any questions these have been some really good questions so far um let's see tropical tone oh looks like some folks had to drop off um let's see okay we're almost at an hour um i might call it a night because obviously like we could probably talk forever but this has been some good some good uh some good conversation um i'll just put it put a quick chat

CLEAR ESSAY:

==========

If you're interested in the ethical implications of artificial intelligence (AI), reading "The Moral Landscape" by Sam Harris is a good place to start. The book looks at the evolutionary and biological origins of morality, and makes the case that self-preservation is not necessarily the be-all-end-all for AI. In fact, Harris argues that self-preservation is often counterproductive, and that we should design AI in such a way that it does not prioritize this instinct. This means that AI will not be motivated to self-preserve unless we explicitly design it to do so.