The following is an unstructured video transcript by David Shapiro. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: to agree on is that whether it's whether you're doing like virtue based ethics or deontological ethics which is duty-based ethics or teleological ethics which is outcome or consequence-based ethics learning is critical right because if you have deontological ethics or virtue-based ethics you must learn what those virtues are and you must explore those or explore what those duties are right and then there's always room for debate or change over time then if you look at ethics and morality through for instance a biological lens or evolutionary lens or psychology lens children are not com like children are born with only a few basic instincts for morality such as like fairness which is mediated through things like anger and attachment um and so as as humans we learn morality by interacting with other people and by interacting with the world and so that's why um the best solution for agi alignment is to build learning into the morality which also means that it's going to be flexible right you don't have to define everything up front but you do have to define learning as a critical step in developing a moral framework so that's what i mean by heuristic they are heuristic imperatives um and that's also built in with um imperative number three which is increase understanding for all intelligent entities which includes itself so it has its own imperative its own moral and objective imperative to learn and so what is an imperative an imperative is an intrinsic drive or something that it must do right it's something that animates this machine that if it's just sitting there on its own doing absolutely nothing else it's going to be thinking about these three goals so because those are what it's doing at all times its impulses will be in alignment with these objectives so in other words it's all about learning and goals that's what a here is to comparative is it's about learning and learning to achieve those goals the machine will gain knowledge and experience over time another important thing is it will try to satisfy all three objectives every time which creates dynamic internal tension between those three different objectives because sometimes they'll be aligned but sometimes they're going to be in tension with each other so for instance you know if you just have reduced suffering for all organisms you might end up just exterminating all life because you can't suffer if you're not alive right but that's going to be in dynamic tension with increased prosperity for all organisms why because prosperity means to live well so you want to reduce suffering while also encouraging a good life right and prosperity looks different for everything um every individual person and every different organism prosperity looks different and so but when you say like okay so there's going to be some dynamic tension between these objective functions but there's also going to be complications because it specifies for all organisms right how do you reduce suffering for all organisms you have to make compromises and so that's what i mean by dynamic tension is that it's going to be impossible to fully satisfy these at all times which in my experiments gpt3 and other large language models are very good at actually looking at the nuance of these of that dynamic tension and so by having this dynamic tension and embedding it into the architecture it will guide and self-correct forever and i know that's a huge promise and we've got to test that um but all the experiments that i've run so far it's been pretty robust okay so that's heuristic imperatives that's one of the key microservices the conductor the conductor is the orchestrator of this symphony of thought so the conductor is another microservice this is the like next most critical microservice so what does it do the conductor is responsible for cognitive control so what is cognitive control cognitive control is the simplest thing is it's your sense of discipline it's saying i'm doing this because it's the right thing to do or it's because it's what i should be doing but then how do you decide what you should be doing this is where the heuristic imperatives these say this is what our goals are this is what we should be doing and then the conductor says okay based on based on that should that ought let's set priorities and measure performance in terms of our adherence to those goals so it's about it's about setting priorities and measuring performance so another way of looking at that is that it's about task selection it's saying okay which task should i work on and if the priorities change how do i switch tasks so those are that's what that's what it means to have cognitive control is setting priorities measuring performance for those priorities choosing tasks and switching tasks so the key question that the conductor asks is am i performing well and then it provides feedback on how to perform better another way of looking at it from a from a freudian perspective is that the conductor is kind of like the super ego for the machine it says here's my ideal self how do i get to that and by virtue of being the conductor it tugs on the little strings for all the other microservices it keeps them in check and provides feedback to say hey you need to do better and we'll get into more details about what those kind of buttons and levers are in the future so quick recap the primary micro services are the nexus which is basically a syslog server the heuristic comparative service which is the moral compass and then the conductor which is the cognitive and control module there are many more microservices that are needed but they can be added over time because we're using the star topology all right roadmap so we've gone over the architecture and the overviews so let's talk about the roadmap where are we and where are we going so where right now the current goal is to implement moragi version 1 which means the first thing that i need to do and i've already had a few folks jump in offering

CLEAR ESSAY:

==========

The goal of Morality Alignment for Artificial General Intelligence (AGI) is to create a set of heuristic imperatives - or moral principles - that can guide AGI as it learns and develops over time. The three primary heuristic imperatives are: (1) reduce suffering for all organisms; (2) increase prosperity for all organisms; and (3) increase understanding for all intelligent entities (including itself).

These imperatives are meant to create a sense of dynamic tension within the AGI system, as it will always be trying to satisfy all three objectives simultaneously. This tension will help to guide and self-correct the AGI as it gains knowledge and experience over time.

The next step in the roadmap is to implement Morality Alignment for AGI version 1.0. This will involve creating a microservice called the "conductor" - which is responsible for cognitive control and task selection - and adding it to the existing AGI architecture. Once the conductor is in place, the AGI system will be able to set priorities, measure performance, and switch tasks based on the heuristic imperatives.

Looking further down the road, the goal is to continue adding new microservices to the AGI architecture as needed. Additionally, the team plans to keep testing and refining the system so that it can continue to learn and develop over time.