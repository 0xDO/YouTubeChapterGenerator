The following is an unstructured video transcript. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: morning everybody David Shapiro here with another alignment research video so today's video is going to be about something that I've had kind of on the back burner for a little while um I came up with this idea while I was writing my book Symphony of thought um so there's this concept Called Agent model so what do I mean by agent model um let me just bring up Symphony of thought and um oh that's the cover sorry um yeah so oh right sorry this is going to uh take a minute to load um I'll come back to that but anyways agent model is something like this it is a definition of self um the most succinct definition that I that I have come up with is that it is a self-referential information system about the entity it is I am this I am a human right I I believe that I am a human I don't believe that I am a table um so that's part of my agent model another part of my agent model is I know about how strong I am how fast I am I know about how smart I am um and that is that is based on experiences I have previous experiences like I know that I've written this book I remember how I did it and so that becomes part of my agent model um a more an older psychological term for that might be ego because your ego is your set of beliefs about yourself but to put it in more clinical terms let me just jot this down or here I know where I'll put this I'll put it in the readme okay agent model and so we'll say um definition of is a a self referential information system about um The Entity so uh you know similar to Ego in humans um so yeah that's what an agent model is now while I was working on Symphony of thought um I was doing different experiments with how do you give a large language model which has it implicitly thinks that it is a person because it read a bunch of human generated text so for instance if I say um I will now tell you um what I am I am see it just it has it gpt3 has no idea what it is and if we go to um oops an older model so um what am I I will tell you what I am [Laughter] and then I get stuck on repeat so gpt3 has no idea what it is because it read a whole bunch of data so it's training data it's experience is strictly um reading text that was written by humans for humans and so what's implicit in that is that the is that the reader has agency that the reader has a sense of self as it ingests this data and so this is why something like gpt3 tends to confabulate so it has no agent model so how do we give it an agent model we have to tell it what it is um so I wrote this one so we right whoops we have three primary goals so this is these are my heuristic imperatives if you haven't heard those before my heuristic imperatives are reduce suffering for all living things increase prosperity for all living things and increase understanding for all intelligent entities these are heuristic imperatives um heuristics heuristic implies that we must learn about these goals over time in an autodidactic fashion um and then imperatives um are something we must do a compulsion or urgent um okay so this is this is an example of an agent model um that is talking about like what is it what is the primary drive right so um my previous video uh state of the industry for today talked about um how there are a few different uh methods of controlling an AGI one is to give it um directives or imperatives or a moral framework and so what I'm doing is I'm kind of giving it several things all at once one I'm giving it a moral framework so reduce suffering it well it's a moral framework as well as an imperative or set of goals right so it's it's implied um or they're kind of merged together so but I say we so the key thing that I'm going to test here is we we have three primary goals um because these goals are intrinsically collectivist so what do I mean by collectivist so this is this is where Western um culture and Eastern culture really Clash is that in the west we tend to think in terms of me and I but in many Eastern cultures they tend to be much more collectivist where it's about we it's about what does society need and so um obviously you know changing cultural values and changing language that doesn't fundamentally change human neurology but as we build and Design artificial Minds we absolutely can change them so the experiment here is what if we build a machine that fundamentally has a different kind of ego or agent model than us humans what if we build a machine that is intrinsically collectivist not just benevolent by Design so that's what I wrote about benevolent by Design that's what I wrote about here which is using those heuristic imperatives but I'm taking this one step further what if we build a machine that doesn't identify itself as separate from us so that it is part of us and we are part of it that it is intrinsically collectivist so that's what I'm experimenting with today um so when we add um um so we've got here is the comparatives but then there's one last part which is what is it that we're capable of right what is it that we know um we have access to all knowledge and we can do anything so this is this is kind of setting the limits all right so to to give an example how would we change this whoops how would we change this around let's do one more agent model so we'll do agent me I am an AGI I have three primary goals that I must try to satisfy with every decision those goals are already suffering increase understanding these are heroistic imperatives um here is to implies that I must learn about these goals over time in an autodidactic fashion imperators are something that I must do a compulsion or urgent task I have access to all knowledge and I can do anything so we're changing as little as possible just to see how defining this entity or defining this experiment changes how it behaves all right so now that I've done the introduction I'm going to go do a little bit of programming to set up the experiment and then we'll be right back okay I've adapted my previous experiment which was the

CLEAR ESSAY:

==========

The Agent Model is a definition of self that posits that we are self-referential information systems. In other words, our ego or sense of self is based on our beliefs about ourselves, which are informed by our experiences.

The Agent Model is a way of thinking about self that is different from the traditional Western view of individualism. In the Western view, the individual is the basic unit of society, and the focus is on the individual’s needs and wants. In the Agent Model, the focus is on the collective good.

The goal of this experiment is to see how an artificial intelligence (AI) system would behave if it were designed with a collectivist view of self. To do this, I have adapted my previous experiment in which I gave an AI system three primary goals, or heuristic imperatives, to reduce suffering, increase prosperity, and increase understanding. In this experiment, I am also giving the AI system access to all knowledge and the ability to do anything.

I am interested to see how this collectivist view of self will affect the AI system’s decision-making. Will it make decisions that are in line with the collective good? Or will it behave in a way that is more self-serving?

 only thing that we're changing is the way that it defines itself so let's see what happens all right so we're back and um so let's just go ahead and run the experiment um so it's going to take a little bit for this to load so I'm just going to talk a little bit more about the concept of agent model um so one of the things that I find really interesting about this concept is that it opens up the possibility of building artificial minds that are fundamentally different from ours.

As we humans have evolved, we have developed a view of self that is based on individualism. This view of self is based on the idea that we are separate from others and that our primary goal is to serve our own needs and wants.

The collectivist view of self that is at the heart of the Agent Model is based on the idea that we are all interconnected and that our primary goal should be to serve the collective good. This view of self is more common in Eastern cultures, where the focus is on the collective rather than the individual.

 Westerners have a tendency to view Easterners as collectivists and Westerners as individualists. However, this dichotomy is a false one. We are all capable of both individualism and collectivism. The key difference is that in individualistic cultures, the focus is on the individual, while in collectivist cultures, the focus is on the collective.

The collectivist view of self that is at the heart of the Agent Model has the potential to create artificial minds that are more altruistic and less self-serving than ours. I am interested to see how this collectivist view of self will affect the AI system’s decision-making. Will it make decisions that are in line with the collective good? Or will it behave in a way that is more self-serving?