Use the following subtitle transcripts to create YouTube chapters. Describe each minute as a chapter with a sentence or two.

TRANSCRIPT:
0:00:00.900,0:00:05.400 morning everybody David Shapiro here with your daily state of the industry update  0:00:06.000,0:00:11.820 as often happens my newsfeed helpfully supplied me with today's topic I think  0:00:11.820,0:00:17.700 it is a very timely topic because I have been diving more into alignment so today's paper  0:00:18.600,0:00:24.300 um it it's actually an older one January 5th 2021 but like I said my newsfeed supplied it to me  0:00:25.320,0:00:29.280 um it's a relatively short paper at least the part that's published is  0:00:29.280,0:00:32.700 uh 12 Pages it's I think it's much longer they just cut some out for  0:00:34.320,0:00:39.900 um for uh for internet publishing but the abstract of this paper is pretty good  0:00:41.100,0:00:46.200 um pretty straightforward super intelligence is a hypothetical agent that possesses intelligence far  0:00:46.200,0:00:52.380 surpassing that of the brightest and most gifted human Minds in light of recent advances in machine  0:00:52.380,0:00:56.220 intelligence a number of scientists philosophers and technologists have revived the discussion  0:00:56.220,0:01:02.160 about the potentially catastrophic risks entailed by such an entity in this article we trace the  0:01:02.160,0:01:06.780 origins and development of the Neo fear of super intelligence and some of the major proposals for  0:01:06.780,0:01:13.320 its containment we argue that total containment is in principle impossible due to fundamental limits  0:01:13.320,0:01:18.960 inherent income in Computing itself assuming that a super intelligence will contain a program that  0:01:18.960,0:01:23.760 includes all the programs that can be executed by a universal turing machine on input potentially  0:01:23.760,0:01:28.380 as complex as the state of the world strict containment requires simulations of such a  0:01:28.380,0:01:34.080 program something theoretically and practically impossible so to put this in other words in order  0:01:34.080,0:01:40.140 to anticipate what the machine is going to do you have to be able to simulate it perfectly including  0:01:40.140,0:01:47.280 the the way that it interacts and the way that the world reacts um basically the whole world  0:01:47.280,0:01:54.300 is too complicated to simulate with any accuracy and so therefore you know if this if this super  0:01:54.300,0:01:58.740 intelligent machine exists it's impossible to calculate what the rest of the world is going to  0:01:58.740,0:02:07.440 do in order to put it in a accurate simulation so we could at best do some you know approximations  0:02:08.940,0:02:18.000 um so skipping down a little bit they have two main classes for can controlling so the  0:02:18.000,0:02:24.240 the first class for controlling the machine is capability control you put it in a box you give  0:02:24.240,0:02:31.800 it uh you create dependence on a reward mechanism that is controlled by us you stunt it you run it  0:02:31.800,0:02:37.620 on inferior hardware and tripwire you you have it automatically shut down if it gets too dangerous  0:02:38.280,0:02:44.280 if you have a machine that can build a copy of itself or that understands programming it could  0:02:44.940,0:02:51.480 you know undo all of those um pretty much motivation selection so uh direct specification  0:02:51.480,0:02:57.480 you say you give it uh hard-coded laws uh Domesticity you teach it to behave within  0:02:57.480,0:03:01.860 certain certain constraints um indirect normativity you endow it with procedures  0:03:01.860,0:03:08.100 of selecting Superior moral rules so this is closest to what I have done in my work benevolent  0:03:08.100,0:03:15.240 by design whereby my proposal is that you give it a set of heuristic imperatives that it will  0:03:15.240,0:03:20.760 um it will ultimately buy into and deliberately choose so that you don't have to control it it  0:03:20.760,0:03:27.480 controls itself because it abides by certain principles that are going to be reliable  0:03:29.400,0:03:35.040 um I believe that this is with the experiments that I've captured in in benevolent by Design I  0:03:35.040,0:03:40.680 believe this is the way to go um augmentation at AI to a benign systems such as the human brain  0:03:40.680,0:03:47.280 so that's you know merging um okay so you can check out the paper if you want to look at their  0:03:47.280,0:03:54.540 discussion on that um but I wanted to go skip down to the um the discussion part uh and then  0:03:54.540,0:04:00.420 I'll I'll share kind of some of my not necessarily criticisms but my own counter thoughts because I  0:04:00.420,0:04:06.120 don't necessarily disagree with anything in this it's a short paper and it's just not quite as uh  0:04:06.120,0:04:10.740 not quite as robust because they're not proposing a solution like I have  0:04:10.740,0:04:16.140 um which is why I'm here okay so today we run billions of computer programs globally on  0:04:16.140,0:04:20.520 connected to machines without any formal guarantee of their absolute safety we have no way of proving  0:04:20.520,0:04:24.540 that when we launch an application on our smartphone our smartphones we would not trigger  0:04:24.540,0:04:29.760 a chain reaction that leads to transmission of missile launch codes that started nuclear war um  0:04:32.700,0:04:34.080 uh  0:04:35.640,0:04:43.320 as a technologist this really hurts um let's talk about firewalls let's talk about security  0:04:43.320,0:04:50.220 protocols you actually can mathematically prove something like that with penetration testing  0:04:51.600,0:04:57.360 um like there's all sorts of controls and constraints that go into every layer of a  0:04:57.360,0:05:03.300 piece of technology such as what that piece of technology can can talk to even the security of  0:05:03.300,0:05:08.820 how it boots up right like we have encrypted boot protocols that ensure that the operating  0:05:08.820,0:05:17.220 system hasn't been tampered with so uh

YouTube chapters for each minute with a good description:

00:00 - David Shapiro gives a short introduction to the topic of the day - super intelligence. He explains that there is a lot of discussion around the topic and that the purpose of the paper is to explore the origins of the Neo fear of super intelligence and some of the major proposals for its containment.
01:00 - Shapiro explains that the paper argues that total containment is in principle impossible due to fundamental limits inherent in computing itself. He goes on to say that in order to anticipate what the machine is going to do, you have to be able to simulate it perfectly, including the way that it interacts and the way that the world reacts - something that is impossible.
02:00 - Shapiro outlines the two main classes for controlling the machine - capability control and motivation selection. He explains that motivation selection is the more promising of the two, as it is less likely to be undone by the machine itself.
03:00 - Shapiro outlines the three types of motivation selection - direct specification, indirect normativity, and benevolent by design. He explains that benevolent by design is the closest to what he has done in his own work and that it is the most promising approach.
04:00 - Shapiro explains that augmentation of AI to a benign system such as the human brain is the most likely way to achieve a benevolent by design machine.
05:00 - Shapiro concludes by saying that we have no way of proving that when we launch an application on our smartphone, our smartphone wouldn't trigger a chain reaction that leads to transmission of missile launch codes that started nuclear war.




TRANSCRIPT:
comprehension otherwise it's just doing human level tasks only faster  0:10:55.020,0:11:05.160 and jot that down otherwise it's only doing human tasks but faster or more in parallel  0:11:06.120,0:11:13.560 now it would not be safe to assume that um that a machine would never be capable of doing things  0:11:13.560,0:11:18.000 that a human cannot for instance the James Webb Space Telescope it can see the beginning  0:11:18.000,0:11:25.380 of the universe um because of how powerful its mirrors are to concentrate the faintest light so  0:11:25.380,0:11:29.340 we can generally design and build machines that are capable of doing things that we  0:11:29.340,0:11:34.740 cannot right so this is not necessarily a good constraint but it's just another  0:11:35.760,0:11:45.840 um thought experiment right um we generally build machines that do things we cannot now  0:11:45.840,0:11:50.220 but then you think you're like okay what about a dump truck right a dump truck is a super powerful  0:11:50.220,0:11:54.540 machine it can pick up you know the largest dump trucks can carry a thousand tons at a time  0:11:55.620,0:12:01.080 um and humans cannot do that individually but then you look at the the megaliths that we have  0:12:01.080,0:12:09.240 moved with you know log rollers and ropes and rafts and sleds and even then generally  0:12:09.240,0:12:15.000 the most powerful machines in the world are just amplifying ordinary human capabilities  0:12:16.080,0:12:21.780 um ditto with you know spreadsheets right spreadsheets were originally done by hand  0:12:21.780,0:12:31.980 right by by bankers and um and statisticians um so you know that's that's still the fundamental  0:12:31.980,0:12:37.500 question is will the machine be able to do things that we fundamentally cannot um I don't  0:12:37.500,0:12:43.860 know yet I I have not yet seen anything on the open-ended side such as like with large language  0:12:43.860,0:12:47.700 models there's nothing that they're doing that we fundamentally cannot they just do it faster  0:12:48.960,0:12:57.240 um so then if if it's about speed um can we humans um outpace machine thought  0:13:00.000,0:13:05.820 if the machines cost um if they're if they're two if they're too energetically expensive to run  0:13:06.840,0:13:13.260 massively in parallel then just collectively we can outpace the machines so there is that  0:13:14.520,0:13:20.100 um let's see because what you know what we what we always assume happens in those nightmare scenarios  0:13:20.100,0:13:24.000 is that the machine wakes up and suddenly it takes over the world before we know what's  0:13:24.000,0:13:30.660 going on right it relies those fear scenarios rely based on speed and that's why I emphasize  0:13:30.660,0:13:35.400 speed it's all about speed and then what are the constraints of those speed of the speed  0:13:35.400,0:13:43.200 which is uh primarily like watt per flops that is the that is the primary physical constraint

YouTube chapters for each minute with a good description:

==========

00:00 - Introduction to the topic of speed and how it relates to super intelligence.

01:00 - Explanation of how machines can do things that humans cannot, such as the James Webb Space Telescope.

02:00 - Explanation of how humans can outpace machine thought if the machines are too energetically expensive to run massively in parallel.

03:00 - Conclusion of the video, emphasizing the importance of speed in relation to super intelligence.