The following is an unstructured video transcript. Please rewrite this as a more structured, clear essay.

TRANSCRIPT: remembered it right not quite but almost um we'll do we'll leave it indent too that's fine okay because nothing is worse than oh and not right binary this is just text um yeah and actually i think maybe maybe um let's say i don't think i need to do the separators that's fine and it's always as right and not encoded as utf-8 okay so let me remove the encoding as utf-8 okay we'll leave it at that that should work um okay so alt text we open it we get the embedding we append this to the result so the result will be a list of dictionaries with some text and then a vector and so that'll be our database um yeah and we don't even need to build the index because we're building it now so let me just go ahead and delete this function because that's noise search index we can delete that because that's noise we can delete similarity because we don't even use that we don't use completion do i use save file i don't use save file anywhere all i use is open file okay um we don't need that i think that's about it i don't need re i think we just use text wrap json i don't even use os anymore clean this up okay that should be fine um let's go to multi-document answering and we'll do we'll just call this build the index there we go keep it in editor no we'll just reopen it okay so this will generate a json file that will have it'll be a list of chunks of text that are 3000 characters long each of them with an embedding we can probably do more do it a little bit longer um let's do 4 000 characters because it's roughly three or four characters per token so this will be roughly a thousand tokens which will be a quarter of well hmm i wonder if we can do longer no this this would be fine because we we still need some room to work around it if we have one of these chunks this is this will be our knowledge base um so let me make sure this will work i'll let it run cd multi-document answering python build index did i get it right the first time no save gpt3 log is not defined ah see that's what i did wrong i'm not gonna worry about saving every single log um this is a normal list that's fine okay let's try again and then oh i don't have any output do i yeah i need i need i need some kind of output so that i can see what it's doing because otherwise i'm gonna be confused um so what i often do for things like this just to make just to do a sanity check is i'll set it to a variable so then i can just print the variable and then we'll do comma and then newline newline newline um so i can so i can see what it's what it's actually generating because sometimes if you do it wrong you know your brain just is not working with you today well my mind is working with me today sometimes it isn't um but sometimes your brain just isn't working and the variable isn't what you thought it was but in this case i think i think it's all right because this is relatively straightforward okay python build index oh wow that's fast that's really fast the embeddings endpoint is quick dang oh i wonder if it's because i'm using the ada endpoint yeah i bet that's why it's so fast gum is it already done index.json okay so we've taken an input from 440 kilobytes and made it into um more than three and a half megabytes oh perfect so there we go look it worked it worked it worked okay so this is what it looks like now um let me zoom out a little bit um because you don't need to see it in detail so you've got content which is just the tongue the chunk of text there we go i can speak and then a semantic vector so the semantic vector is the mathematical the numerical representation of this meaning so it's basically this is this is a pair right it's this is the human readable text and then this is the machine readable representation and so then we've got a whole bunch of those and this file could be compressed but what i did was um when i put the the indent here um to say indent equals two so that makes it more more human friendly right so you can see that you know the data is structured so like for every layer of embedding there's two spaces so the um the the root list is it index zero and then we've got um two spaces for the first um dictionary and then we've got another two spaces for the for the the nested list so you can clearly see the levels of nesting um okay so we've got our index that was much faster than i thought it would be let's zoom back in wow i was hoping that i'd have like a mental break to be able to keep thinking okay so i'm gonna actually pause this for a second just because you don't need to see me like gathering my thoughts typically when i run these loops i have like a few minutes to gather my thoughts so i'm gonna pause the video for just a second and we'll be right back as i mentally plan the next step okay and we're back um as with all things data prep is the biggest uh thing biggest problem so we're closer to being done than than you might have guessed um i started on the next part so we've we've built the index it was way faster than i thought now we're going to answer questions and so what i did was i just created wrote a quick thing to open the open the index that we just created and then we'll do an infinite loop um where we will just ask questions so um this is based on the artificial cognitive entity thing where it's uh basically just searching for um for a particular uh set of memories right um this same paradigm should work anywhere so what we're going to do is we're going to um we're going to take whatever our question is and we're going to get the vector from it um and so we'll just get a a vector and then we'll match whichever all the thing all the parts that are closest and actually here let's go ahead and just do a separate function so def and then um we will call let's just copy this function because it's it's pretty close to what we need so instead what we'll do is we will do let's see results equals search index and then we'll do query um so count we'll say 10 we'll say top 10. we don't have time stamp so

CLEAR ESSAY:

==========

We've built an index of textual data, and now we need to answer questions with it. To do this, we'll take the question, convert it into a vector representation, and then find the 10 closest matches in our index. This should work for any question, as long as the question can be represented as a vector.